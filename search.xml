<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Thesis_Reading</title>
      <link href="/2024/07/15/Time-Series/"/>
      <url>/2024/07/15/Time-Series/</url>
      
        <content type="html"><![CDATA[<h2 id="Time-Series-Contrastive-Learning-with-Information-Aware-Augmentations"><a href="#Time-Series-Contrastive-Learning-with-Information-Aware-Augmentations" class="headerlink" title="Time Series Contrastive Learning with Information-Aware Augmentations"></a><strong>Time Series Contrastive Learning with Information-Aware Augmentations</strong></h2><h3 id="æ‘˜è¦"><a href="#æ‘˜è¦" class="headerlink" title="æ‘˜è¦"></a>æ‘˜è¦</h3><p>èƒŒæ™¯ï¼šåœ¨è¿‘å¹´æ¥ï¼Œå·²ç»æœ‰è®¸å¤šå¯¹æ¯”å­¦ä¹ æ–¹æ³•è¢«æå‡ºï¼Œå¹¶åœ¨å®è¯ä¸Šå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚</p><p>å°½ç®¡å¯¹æ¯”å­¦ä¹ åœ¨å›¾åƒå’Œè¯­è¨€é¢†åŸŸéå¸¸æœ‰æ•ˆå’Œæ™®éï¼Œä½†åœ¨æ—¶é—´åºåˆ—æ•°æ®ä¸Šçš„åº”ç”¨ç›¸å¯¹è¾ƒå°‘ã€‚<br>å¯¹æ¯”å­¦ä¹ çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼š</p><p>å¯¹æ¯”å­¦ä¹ çš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ˜¯é€‰æ‹©é€‚å½“çš„æ•°æ®å¢å¼ºï¼ˆaugmentationï¼‰æ–¹å¼ï¼Œé€šè¿‡æ–½åŠ ä¸€äº›å…ˆéªŒæ¡ä»¶æ„å»ºå¯è¡Œçš„æ­£æ ·æœ¬ã€‚è¿™æ ·ï¼Œç¼–ç å™¨å¯ä»¥é€šè¿‡è®­ç»ƒæ¥å­¦ä¹ ç¨³å¥å’Œå…·æœ‰åŒºåˆ†æ€§çš„è¡¨ç¤ºã€‚<br>é—®é¢˜é™ˆè¿°ï¼š</p><p>ä¸å›¾åƒå’Œè¯­è¨€é¢†åŸŸä¸åŒï¼Œæ—¶é—´åºåˆ—æ•°æ®çš„â€œæœŸæœ›â€å¢å¼ºæ ·æœ¬å¾ˆéš¾é€šè¿‡äººä¸ºçš„å…ˆéªŒæ¡ä»¶æ¥ç”Ÿæˆï¼Œå› ä¸ºæ—¶é—´åºåˆ—æ•°æ®å…·æœ‰å¤šæ ·ä¸”äººç±»éš¾ä»¥è¯†åˆ«çš„æ—¶é—´ç»“æ„ã€‚å¦‚ä½•ä¸ºç»™å®šçš„å¯¹æ¯”å­¦ä¹ ä»»åŠ¡å’Œæ•°æ®é›†æ‰¾åˆ°å¯¹æ—¶é—´åºåˆ—æ•°æ®æœ‰æ„ä¹‰çš„å¢å¼ºä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£çš„é—®é¢˜ã€‚</p><p>è§£å†³æ–¹æ³•ï¼šä½œè€…é€šè¿‡ä¿¡æ¯ç†è®ºæå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œæ—¨åœ¨åŒæ—¶é¼“åŠ±é«˜ä¿çœŸåº¦å’Œå¤šæ ·æ€§ï¼Œä»è€Œäº§ç”Ÿæœ‰æ„ä¹‰çš„æ—¶é—´åºåˆ—æ•°æ®å¢å¼ºã€‚é€šè¿‡ç†è®ºåˆ†æï¼Œæå‡ºäº†é€‰æ‹©å¯è¡Œæ•°æ®å¢å¼ºçš„æ ‡å‡†ã€‚</p><p>æ–°æ–¹æ³•æå‡ºï¼šä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œåä¸ºInfoTSï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä¿¡æ¯æ„ŸçŸ¥çš„å¢å¼ºï¼ˆinformation-aware augmentationsï¼‰ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©æœ€ä¼˜çš„å¢å¼ºæ–¹å¼ç”¨äºæ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ ã€‚</p><p>å®éªŒè¯æ˜ï¼šåœ¨å„ç§æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„æµ‹ä»»åŠ¡ä¸Šèƒ½å¤Ÿå®ç°é«˜ç«äº‰æ€§çš„æ€§èƒ½ï¼Œå‡å°‘äº†å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰é«˜è¾¾12.0%ã€‚åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šç›¸å¯¹äºä¸»æµåŸºçº¿æ–¹æ³•ï¼Œå‡†ç¡®åº¦æé«˜äº†æœ€é«˜è¾¾3.7%ã€‚</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>è”é‚¦å­¦ä¹ </title>
      <link href="/2024/07/08/README/"/>
      <url>/2024/07/08/README/</url>
      
        <content type="html"><![CDATA[<h1 id="Federated-Learning"><a href="#Federated-Learning" class="headerlink" title="Federated Learning"></a>Federated Learning</h1><h2 id="Part-1-Introduction"><a href="#Part-1-Introduction" class="headerlink" title="Part 1: Introduction"></a>Part 1: Introduction</h2><ul><li><a href="https://federated.withgoogle.com/">Federated Learning Comic</a></li><li><a href="http://ai.googleblog.com/2017/04/federated-learning-collaborative.html">Federated Learning: Collaborative Machine Learning without Centralized Training Data</a></li><li><a href="https://aaai.org/Conferences/AAAI-19/invited-speakers/#yang">GDPR, Data Shotrage and AI (AAAI-19)</a></li><li><a href="https://www.youtube.com/watch?v=89BGjQYA0uE">Federated Learning: Machine Learning on Decentralized Data (Google I&#x2F;Oâ€™19)</a></li><li><a href="https://www.fedai.org/static/flwp-en.pdf">Federated Learning White Paper V1.0</a></li><li><a href="https://blog.fastforwardlabs.com/2018/11/14/federated-learning.html">Federated learning: distributed machine learning with data locality and privacy</a></li></ul><h2 id="Part-2-Survey"><a href="#Part-2-Survey" class="headerlink" title="Part 2: Survey"></a>Part 2: Survey</h2><ul><li><a href="https://arxiv.org/abs/1908.07873">Federated Learning: Challenges, Methods, and Future Directions</a></li><li><a href="https://arxiv.org/abs/1907.09693">Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection</a></li><li><a href="https://arxiv.org/abs/1909.11875">Federated Learning in Mobile Edge Networks: A Comprehensive Survey</a></li><li><a href="https://arxiv.org/abs/1908.06847">Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges</a></li><li><a href="https://arxiv.org/pdf/1907.08349.pdf">Convergence of Edge Computing and Deep Learning: A Comprehensive Survey</a></li><li><a href="https://arxiv.org/pdf/1912.04977.pdf">Advances and Open Problems in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1902.04885.pdf">Federated Machine Learning: Concept and Applications</a></li><li><a href="https://arxiv.org/pdf/2003.02133.pdf">Threats to Federated Learning: A Survey</a></li><li><a href="https://arxiv.org/pdf/2003.08673.pdf">Survey of Personalization Techniques for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2006.06217.pdf">SECure: A Social and Environmental Certificate for AI Systems</a></li><li><a href="https://arxiv.org/pdf/2006.03594.pdf">From Federated Learning to Fog Learning: Towards Large-Scale Distributed Machine Learning in Heterogeneous Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/2006.02931.pdf">Federated Learning for 6G Communications: Challenges, Methods, and Future Directions</a></li><li><a href="https://arxiv.org/pdf/2004.11794.pdf">A Review of Privacy Preserving Federated Learning for Private IoT Analytics</a></li><li><a href="https://arxiv.org/pdf/2002.11545.pdf">Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective</a></li><li><a href="https://arxiv.org/pdf/2002.10610.pdf">Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art</a></li><li><a href="https://arxiv.org/pdf/1912.04859.pdf">Privacy-Preserving Blockchain Based Federated Learning with Differential Data Sharing</a></li><li><a href="https://arxiv.org/pdf/1912.01554.pdf">An Introduction to Communication Efficient Edge Machine Learning</a></li><li><a href="https://arxiv.org/pdf/1911.06270.pdf">Federated Learning for Healthcare Informatics</a></li><li><a href="https://arxiv.org/pdf/1910.06799.pdf">Federated Learning for Coalition Operations</a></li><li><a href="https://arxiv.org/pdf/1812.03288.pdf">No Peek: A Survey of private distributed deep learning</a></li><li><a href="http://arxiv.org/pdf/2002.09668.pdf">Communication-Efficient Edge AI: Algorithms and Systems</a></li></ul><h2 id="Part-3-Benchmarks"><a href="#Part-3-Benchmarks" class="headerlink" title="Part 3: Benchmarks"></a>Part 3: Benchmarks</h2><ul><li><a href="https://arxiv.org/abs/1812.01097">LEAF: A Benchmark for Federated Settings</a>(<a href="https://github.com/TalwalkarLab/leaf">https://github.com/TalwalkarLab/leaf</a>) [Recommend]</li><li><a href="https://www.researchgate.net/profile/Gregor_Ulm/publication/329106719_A_Performance_Evaluation_of_Federated_Learning_Algorithms/links/5c0fabcfa6fdcc494febf907/A-Performance-Evaluation-of-Federated-Learning-Algorithms.pdf">A Performance Evaluation of Federated Learning Algorithms</a></li><li><a href="https://arxiv.org/abs/1908.01924">Edge AIBench: Towards Comprehensive End-to-end Edge Computing Benchmarking</a></li></ul><h2 id="Part-4-Converge"><a href="#Part-4-Converge" class="headerlink" title="Part 4: Converge"></a>Part 4: Converge</h2><h3 id="4-1-Model-Aggregation"><a href="#4-1-Model-Aggregation" class="headerlink" title="4.1 Model Aggregation"></a>4.1 Model Aggregation</h3><ul><li><a href="https://arxiv.org/abs/1902.11175">One-Shot Federated Learning</a></li><li><a href="https://arxiv.org/abs/1910.08234">Federated Learning with Unbiased Gradient Aggregation and Controllable Meta Updating</a> (NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1905.12022">Bayesian Nonparametric Federated Learning of Neural Networks</a> (ICML 2019)</li><li><a href="https://openreview.net/forum?id=dgtpE6gKjHn">FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning</a> (ICLR 2021)</li><li><a href="https://arxiv.org/abs/1902.00146">Agnostic Federated Learning</a> (ICML 2019)</li><li><a href="https://openreview.net/forum?id=BkluqlSFDS">Federated Learning with Matched Averaging</a> (ICLR 2020)</li><li><a href="https://arxiv.org/abs/1907.01132">Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications</a></li></ul><h3 id="4-2-Convergence-Research"><a href="#4-2-Convergence-Research" class="headerlink" title="4.2 Convergence Research"></a>4.2 Convergence Research</h3><ul><li><a href="https://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication">A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication</a> ï¼ˆNIPS 2018ï¼‰</li><li><a href="https://openreview.net/forum?id=jDdzh5ul-d">Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning</a> (ICLR 2021)</li><li><a href="https://arxiv.org/pdf/2007.07682.pdf">FetchSGD: Communication-Efficient Federated Learning with Sketching</a></li><li><a href="https://arxiv.org/abs/2105.05001">FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning Convergence Analysis</a> (ICML 2021)</li><li><a href="http://proceedings.mlr.press/v130/shi21c.html">Federated Multi-armed Bandits with Personalization</a> (AISTATS 2021)</li><li><a href="http://proceedings.mlr.press/v130/haddadpour21a.html">Federated Learning with Compression: Unified Analysis and Sharp Guarantees</a> (AISTATS 2021)</li><li><a href="http://proceedings.mlr.press/v130/charles21a.html">Convergence and Accuracy Trade-Offs in Federated Learning and Meta-Learning</a> (AISTATS 2021)</li><li><a href="">Towards Flexible Device Participation in Federated Learning</a> (AISTATS 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467309">Fed2: Feature-Aligned Federated Learning</a> (KDD 2021)</li><li><a href="https://arxiv.org/pdf/1812.06127">Federated Optimization for Heterogeneous Networks</a></li><li><a href="https://arxiv.org/abs/1907.02189">On the Convergence of FedAvg on Non-IID Data</a> <a href="https://openreview.net/forum?id=HJxNAnVtDS">[OpenReview]</a></li><li><a href="https://arxiv.org/abs/1910.09126">Communication Efficient Decentralized Training with Multiple Local Updates</a></li><li><a href="https://arxiv.org/abs/1805.09767">Local SGD Converges Fast and Communicates Little</a></li><li><a href="https://arxiv.org/abs/1910.00643">SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum</a></li><li><a href="https://arxiv.org/abs/1807.06629">Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning</a> (AAAI 2018ï¼‰</li><li><a href="https://arxiv.org/abs/1905.03817">On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization</a> (ICML 2019ï¼‰</li><li><a href="https://arxiv.org/abs/1811.11479">Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data</a></li><li><a href="https://arxiv.org/abs/1905.12648">Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data</a> ï¼ˆNIPS 2019 Workshop)</li></ul><h3 id="4-3-Statistical-Heterogeneity"><a href="#4-3-Statistical-Heterogeneity" class="headerlink" title="4.3 Statistical Heterogeneity"></a>4.3 Statistical Heterogeneity</h3><ul><li><a href="https://arxiv.org/pdf/2005.11418.pdf">FedPD: A Federated Learning Framework with Optimal Rates andAdaptivity to Non-IID Data</a></li><li><a href="https://openreview.net/forum?id=6YEQUn0QICG">FedBN: Federated Learning on Non-IID Features via Local Batch Normalization</a> (ICLR 2021)</li><li><a href="https://openreview.net/forum?id=Ogga20D2HO-">FedMix: Approximation of Mixup under Mean Augmented Federated Learning</a> (ICLR 2021)</li><li><a href="https://openreview.net/forum?id=TNkPBBYFkXg">HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients</a> (ICLR 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467254">FedRS: Federated Learning with Restricted Softmax for Label Distribution Non-IID Data</a> (KDD 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3459637.3482345">FedMatch: Federated Learning Over Heterogeneous Question Answering Data</a> (CIKM 2021)</li><li><a href="https://arxiv.org/pdf/1905.09684.pdf">Decentralized Learning of Generative Adversarial Networks from Non-iid Data</a></li><li><a href="https://arxiv.org/pdf/2008.06217.pdf">Towards Class Imbalance in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1811.11479v1.pdf">Communication-Efficient On-Device Machine Learning:Federated Distillation and Augmentationunder Non-IID Private Data</a></li><li><a href="https://arxiv.org/pdf/2007.07481.pdf">Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization</a></li><li><a href="https://arxiv.org/abs/1911.02054">Federated Adversarial Domain Adaptation</a></li><li><a href="https://arxiv.org/pdf/2004.10342.pdf">Federated Learning with Only Positive Labels</a></li><li><a href="https://arxiv.org/abs/1806.00582">Federated Learning with Non-IID Data</a> </li><li><a href="https://arxiv.org/abs/1910.00189">The Non-IID Data Quagmire of Decentralized Machine Learning</a></li><li><a href="https://arxiv.org/pdf/1903.02891">Robust and Communication-Efficient Federated Learning from Non-IID Data</a> (IEEE transactions on neural networks and learning systems)</li><li><a href="https://arxiv.org/abs/1910.03581">FedMD: Heterogenous Federated Learning via Model Distillation</a> (NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1909.04715">First Analysis of Local GD on Heterogeneous Data</a></li><li><a href="https://arxiv.org/abs/1910.06378">SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning</a></li><li><a href="https://arxiv.org/abs/1909.12488">Improving Federated Learning Personalization via Model Agnostic Meta Learning</a> (NIPS 2019 Workshop)</li><li><a href="https://openreview.net/forum?id=ehJqJQk9cw">Personalized Federated Learning with First Order Model Optimization</a> (ICLR 2021)</li><li><a href="https://arxiv.org/pdf/1811.12629">LoAdaBoost: Loss-Based AdaBoost Federated Machine Learning on Medical Data</a></li><li><a href="https://openreview.net/forum?id=SJeOAJStwB">On Federated Learning of Deep Networks from Non-IID Data: Parameter Divergence and the Effects of Hyperparametric Methods</a></li><li><a href="https://arxiv.org/abs/1910.07796">Overcoming Forgetting in Federated Learning on Non-IID Data</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="#workshop">FedMAX: Activation Entropy Maximization Targeting Effective Non-IID Federated Learning</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/pdf/2003.00295.pdf">Adaptive Federated Optimization.</a>(ICLR 2021 (Under Review))</li><li><a href="https://arxiv.org/pdf/1707.01155.pdf">Stochastic, Distributed and Federated Optimization for Machine Learning. FL PhD Thesis. By Jakub</a></li><li><a href="https://arxiv.org/pdf/1706.07880.pdf">Collaborative Deep Learning in Fixed Topology Networks</a></li><li><a href="https://arxiv.org/pdf/2006.09637.pdf">FedCD: Improving Performance in non-IID Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.10937.pdf">Life Long Learning: FedFMC: Sequential Efficient Federated Learning on Non-iid Data.</a></li><li><a href="https://arxiv.org/pdf/2006.08907.pdf">Robust Federated Learning: The Case of Affine Distribution Shifts.</a></li><li><a href="https://arxiv.org/abs/2102.07078">Exploiting Shared Representations for Personalized Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2103.04628">Personalized Federated Learning using Hypernetworks</a> (ICML 2021)</li><li><a href="https://onikle.com/articles/359482">Ditto: Fair and Robust Federated Learning Through Personalization</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2105.10056">Data-Free Knowledge Distillation for Heterogeneous Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2102.03198">Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2103.00697">Heterogeneity for the Win: One-Shot Federated Clustering</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2105.05883">Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2102.04635">Federated Deep AUC Maximization for Hetergeneous Data with a Constant Communication Complexity</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2104.08776">Federated Learning of User Verification Models Without Sharing Embeddings</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2103.03228">One for One, or All for All: Equilibria and Optimality of Collaboration in Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/pdf/2006.07242.pdf">Ensemble Distillation for Robust Model Fusion in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.05148.pdf">XOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.04088.pdf">An Efficient Framework for Clustered Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2005.12657.pdf">Continual Local Training for Better Initialization of Federated Models.</a></li><li><a href="https://arxiv.org/pdf/2005.11418.pdf">FedPD: A Federated Learning Framework with Optimal Rates and Adaptivity to Non-IID Data.</a></li><li><a href="https://arxiv.org/pdf/2005.10848.pdf">Global Multiclass Classification from Heterogeneous Local Models.</a></li><li><a href="https://arxiv.org/pdf/2005.01026.pdf">Multi-Center Federated Learning.</a></li><li><a href="https://openreview.net/forum?id=ce6CFXBh30h">Federated Semi-Supervised Learning with Inter-Client Consistency &amp; Disjoint Learning</a> (ICLR 2021)</li><li><a href="https://arxiv.org/pdf/2004.03657.pdf">(*) FedMAX: Mitigating Activation Divergence for Accurate and Communication-Efficient Federated Learning. CMU ECE.</a></li><li><a href="https://arxiv.org/pdf/2003.13461.pdf">(*) Adaptive Personalized Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2003.12795.pdf">Semi-Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.11223.pdf">Device Heterogeneity in Federated Learning: A Superquantile Approach.</a></li><li><a href="https://arxiv.org/pdf/2002.10671.pdf">Personalized Federated Learning for Intelligent IoT Applications: A Cloud-Edge based Framework</a></li><li><a href="https://arxiv.org/pdf/2002.10619.pdf">Three Approaches for Personalization with Applications to Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.07948.pdf">Personalized Federated Learning: A Meta-Learning Approach</a></li><li><a href="https://arxiv.org/pdf/2002.05038.pdf">Towards Federated Learning: Robustness Analytics to Data Heterogeneity</a></li><li><a href="https://arxiv.org/pdf/2002.04758.pdf">Salvaging Federated Learning by Local Adaptation</a></li><li><a href="https://arxiv.org/pdf/2001.11359.pdf">FOCUS: Dealing with Label Quality Disparity in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2001.08300.pdf">Overcoming Noisy and Irrelevant Data in Federated Learning.</a>(ICPR 2020)</li><li><a href="https://arxiv.org/pdf/2001.03229.pdf">Real-Time Edge Intelligence in the Making: A Collaborative Learning Framework via Federated Meta-Learning.</a></li><li><a href="https://arxiv.org/pdf/2001.01523.pdf">(*) Think Locally, Act Globally: Federated Learning with Local and Global Representations. NeurIPS 2019 Workshop on Federated Learning distinguished student paper award</a></li><li><a href="https://arxiv.org/pdf/1912.00818.pdf">Federated Learning with Personalization Layers</a></li><li><a href="https://arxiv.org/pdf/1910.10252.pdf">Federated Evaluation of On-device Personalization</a></li><li><a href="https://arxiv.org/pdf/1909.08525.pdf">Measure Contribution of Participants in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1909.06335.pdf">(*) Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification</a></li><li><a href="https://arxiv.org/pdf/1907.06426.pdf">Multi-hop Federated Private Data Augmentation with Sample Compression</a></li><li><a href="https://arxiv.org/pdf/1906.01736.pdf">Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms</a></li><li><a href="https://arxiv.org/pdf/1902.08999.pdf">High Dimensional Restrictive Federated Model Selection with multi-objective Bayesian Optimization over shifted distributions</a></li><li><a href="https://arxiv.org/pdf/1912.13075.pdf">Robust Federated Learning Through Representation Matching and Adaptive Hyper-parameters</a></li><li><a href="https://arxiv.org/pdf/2005.12326.pdf">Towards Efficient Scheduling of Federated Mobile Devices under Computational and Statistical Heterogeneity</a></li><li><a href="https://arxiv.org/pdf/2007.04806.pdf">Client Adaptation improves Federated Learning with Simulated Non-IID Clients</a></li></ul><h3 id="4-4-Adaptive-Aggregation"><a href="#4-4-Adaptive-Aggregation" class="headerlink" title="4.4 Adaptive Aggregation"></a>4.4 Adaptive Aggregation</h3><ul><li><a href="https://link.springer.com.remotexs.ntu.edu.sg/chapter/10.1007/978-3-030-14880-5_2">Asynchronous Federated Learning for Geospatial Applications</a> (ECML PKDD Workshop 2018ï¼‰ </li><li><a href="https://arxiv.org/abs/1903.03934">Asynchronous Federated Optimization</a></li><li><a href="https://arxiv.org/abs/1804.05271">Adaptive Federated Learning in Resource Constrained Edge Computing Systems</a> (IEEE Journal on Selected Areas in Communications, 2019ï¼‰</li><li><a href="https://arxiv.org/abs/2102.06387">The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation</a> (ICML 2021)</li></ul><h2 id="Part-5-Security"><a href="#Part-5-Security" class="headerlink" title="Part 5: Security"></a>Part 5: Security</h2><h3 id="5-1-Adversarial-Attacks"><a href="#5-1-Adversarial-Attacks" class="headerlink" title="5.1 Adversarial Attacks"></a>5.1 Adversarial Attacks</h3><ul><li><a href="https://arxiv.org/abs/1911.07963">Can You Really Backdoor Federated Learning? </a>(NeruIPS 2019)</li><li><a href="https://dais-ita.org/sites/default/files/main_secml_model_poison.pdf">Model Poisoning Attacks in Federated Learning</a> (NIPS workshop 2018ï¼‰</li><li><a href="https://arxiv.org/pdf/2004.04676.pdf">An Overview of Federated Deep Learning Privacy Attacks and Defensive Strategies.</a></li><li><a href="https://arxiv.org/pdf/1807.00459.pdf">How To Backdoor Federated Learning.</a>(AISTATS 2020)</li><li><a href="https://arxiv.org/pdf/1702.07464.pdf">Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning.</a>(ACM CCS 2017)</li><li><a href="https://arxiv.org/pdf/1803.01498.pdf">Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates</a></li><li><a href="https://papers.nips.cc/paper/9617-deep-leakage-from-gradients.pdf">Deep Leakage from Gradients.</a>(NIPS 2019)</li><li><a href="https://arxiv.org/pdf/1812.00910.pdf">Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1812.00535.pdf">Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning.</a>(INFOCOM 2019)</li><li><a href="https://arxiv.org/pdf/1811.12470.pdf">Analyzing Federated Learning through an Adversarial Lens.</a>(ICML 2019ï¼‰</li><li><a href="https://arxiv.org/pdf/1808.04866.pdf">Mitigating Sybils in Federated Learning Poisoning.</a>(RAID 2020)</li><li><a href="https://arxiv.org/abs/1811.03761">RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets.</a>(AAAI 2019)</li><li><a href="https://arxiv.org/pdf/2004.10397.pdf">A Framework for Evaluating Gradient Leakage Attacks in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1911.11815.pdf">Local Model Poisoning Attacks to Byzantine-Robust Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.07026.pdf">Backdoor Attacks on Federated Meta-Learning</a></li><li><a href="https://arxiv.org/pdf/2004.04986.pdf">Towards Realistic Byzantine-Robust Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2004.10020.pdf">Data Poisoning Attacks on Federated Machine Learning.</a></li><li><a href="https://arxiv.org/pdf/2004.12571.pdf">Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.13041.pdf">Byzantine-Resilient High-Dimensional SGD with Local Iterations on Heterogeneous Data.</a></li><li><a href="https://arxiv.org/pdf/2006.11489.pdf">FedMGDA+: Federated Learning meets Multi-objective Optimization.</a></li><li><a href="http://proceedings.mlr.press/v130/fraboni21a.html">Free-rider Attacks on Model Aggregation in Federated Learning</a> (AISTATS 2021)</li><li><a href="https://arxiv.org/pdf/2006.15632.pdf">FDA3 : Federated Defense Against Adversarial Attacks for Cloud-Based IIoT Applications.</a></li><li><a href="https://arxiv.org/pdf/2003.07630.pdf">Privacy-preserving Weighted Federated Learning within Oracle-Aided MPC Framework.</a></li><li><a href="https://arxiv.org/pdf/2003.00937.pdf">BASGD: Buffered Asynchronous SGD for Byzantine Learning.</a></li><li><a href="https://arxiv.org/pdf/2002.10940.pdf">Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees.</a></li><li><a href="https://arxiv.org/pdf/2002.00211.pdf">Learning to Detect Malicious Clients for Robust Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1912.13445.pdf">Robust Aggregation for Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1912.12370.pdf">Towards Deep Federated Defenses Against Malware in Cloud Ecosystems.</a></li><li><a href="https://arxiv.org/pdf/1912.11464.pdf">Attack-Resistant Federated Learning with Residual-based Reweighting.</a></li><li><a href="https://arxiv.org/pdf/1911.12560.pdf">Free-riders in Federated Learning: Attacks and Defenses.</a></li><li><a href="https://arxiv.org/pdf/1911.00251.pdf">Robust Federated Learning with Noisy Communication.</a></li><li><a href="https://arxiv.org/pdf/1910.09933.pdf">Abnormal Client Behavior Detection in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1910.06044.pdf">Eavesdrop the Composition Proportion of Training Labels in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1909.05125.pdf">Byzantine-Robust Federated Machine Learning through Adaptive Model Averaging.</a></li><li><a href="https://arxiv.org/pdf/1908.08340.pdf">An End-to-End Encrypted Neural Network for Gradient Updates Transmission in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1906.00887.pdf">Secure Distributed On-Device Learning Networks With Byzantine Adversaries.</a></li><li><a href="https://arxiv.org/pdf/1905.02941.pdf">Robust Federated Training via Collaborative Machine Teaching using Trusted Instances.</a></li><li><a href="https://arxiv.org/pdf/1811.09712.pdf">Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting.</a></li><li><a href="https://arxiv.org/pdf/2003.14053.pdf">Inverting Gradients - How easy is it to break privacy in federated learning?</a></li></ul><h3 id="5-2-Data-Privacy-and-Confidentiality"><a href="#5-2-Data-Privacy-and-Confidentiality" class="headerlink" title="5.2 Data Privacy and Confidentiality"></a>5.2 Data Privacy and Confidentiality</h3><ul><li><a href="https://arxiv.org/abs/1805.05838">Gradient-Leaks: Understanding and Controlling Deanonymization in Federated Learning</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/pdf/1910.05467.pdf">Quantification of the Leakage in Federated Learning</a></li></ul><h2 id="Part-6-Communication-Efficiency"><a href="#Part-6-Communication-Efficiency" class="headerlink" title="Part 6: Communication Efficiency"></a>Part 6: Communication Efficiency</h2><ul><li><a href="https://arxiv.org/abs/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a>](<a href="https://github.com/roxanneluo/Federated-Learning">https://github.com/roxanneluo/Federated-Learning</a>) [Google] <strong>[Must Read]</strong></li><li><a href="https://ieeexplore.ieee.org/document/8698609">Two-Stream Federated Learning: Reduce the Communication Costs</a> (2018 IEEE VCIP)</li><li><a href="https://openreview.net/forum?id=B7v4QMR6Z9w">Federated Learning Based on Dynamic Regularization</a> (ICLR 2021)</li><li><a href="https://openreview.net/forum?id=GFsU8a0sGB">Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms</a> (ICLR 2021)</li><li><a href="https://openreview.net/forum?id=LkFG3lB13U5">Adaptive Federated Optimization</a> (ICLR 2021)</li><li><a href="https://arxiv.org/abs/1905.13727">PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization</a> ï¼ˆNIPS 2019ï¼‰</li><li><a href="https://arxiv.org/abs/1712.01887">Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training</a> (ICLR 2018)</li><li><a href="https://arxiv.org/abs/1909.05350">The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication</a></li><li><a href="https://arxiv.org/abs/1912.11187">A Communication Efficient Collaborative Learning Framework for Distributed Features</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1909.12641">Active Federated Learning</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1909.05844">Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1909.04716">Gradient Descent with Compressed Iterates</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1805.09965">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a></li><li><a href="https://arxiv.org/pdf/2006.12583.pdf">Exact Support Recovery in Federated Regression with One-shot Communication</a></li><li><a href="https://arxiv.org/pdf/2006.11401.pdf">DEED: A General Quantization Scheme for Communication Efficiency in Bits</a></li><li><a href="https://arxiv.org/pdf/2006.08848.pdf">Personalized Federated Learning with Moreau Envelopes</a></li><li><a href="https://arxiv.org/pdf/2006.06954.pdf">Towards Flexible Device Participation in Federated Learning for Non-IID Data.</a></li><li><a href="https://arxiv.org/pdf/2006.03474.pdf">A Primal-Dual SGD Algorithm for Distributed Nonconvex Optimization</a></li><li><a href="https://arxiv.org/pdf/2005.05238.pdf">FedSplit: An algorithmic framework for fast federated optimization</a></li><li><a href="https://arxiv.org/pdf/2005.00224.pdf">Distributed Stochastic Non-Convex Optimization: Momentum-Based Variance Reduction</a></li><li><a href="https://arxiv.org/pdf/2007.00878.pdf">On the Outsized Importance of Learning Rates in Local Update Methods.</a></li><li><a href="https://arxiv.org/pdf/2007.01154.pdf">Federated Learning with Compression: Unified Analysis and Sharp Guarantees.</a></li><li><a href="https://arxiv.org/pdf/2004.01442.pdf">From Local SGD to Local Fixed-Point Methods for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2003.12880.pdf">Federated Residual Learning.</a></li><li><a href="https://arxiv.org/pdf/2002.11364.pdf">Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization.</a>[ICML 2020]</li><li><a href="https://arxiv.org/abs/1804.08333">Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge</a> (FedCS)</li><li><a href="https://arxiv.org/abs/1905.07210">Hybrid-FL for Wireless Networks: Cooperative Learning Mechanism Using Non-IID Data</a></li><li><a href="https://arxiv.org/pdf/2002.11360.pdf">LASG: Lazily Aggregated Stochastic Gradients for Communication-Efficient Distributed Learning</a></li><li><a href="https://arxiv.org/pdf/2002.08958.pdf">Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor</a></li><li><a href="https://arxiv.org/pdf/2002.08782.pdf">Dynamic Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.07454.pdf">Distributed Optimization over Block-Cyclic Data</a></li><li><a href="https://arxiv.org/abs/2011.08474">Federated Composite Optimization</a> (ICML 2021)</li><li><a href="https://arxiv.org/pdf/2002.07399.pdf">Distributed Non-Convex Optimization with Sublinear Speedup under Intermittent Client Availability</a></li><li><a href="https://arxiv.org/pdf/2002.05516.pdf">Federated Learning of a Mixture of Global and Local Models</a></li><li><a href="https://arxiv.org/pdf/2002.02090.pdf">Faster On-Device Training Using New Federated Momentum Algorithm</a></li><li><a href="https://arxiv.org/pdf/2001.01920.pdf">FedDANE: A Federated Newton-Type Method</a></li><li><a href="https://arxiv.org/pdf/1912.09925.pdf">Distributed Fixed Point Methods with Compressed Iterates</a></li><li><a href="https://arxiv.org/pdf/1912.08546.pdf">Primal-dual methods for large-scale and distributed convex optimization and data analytics</a></li><li><a href="https://arxiv.org/pdf/1912.06036.pdf">Parallel Restarted SPIDER - Communication Efficient Distributed Nonconvex Optimization with Optimal Computation Complexity</a></li><li><a href="https://arxiv.org/pdf/1912.05571.pdf">Representation of Federated Learning via Worst-Case Robust Optimization Theory</a></li><li><a href="https://arxiv.org/pdf/1910.14425.pdf">On the Convergence of Local Descent Methods in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1910.06378.pdf">SCAFFOLD: Stochastic Controlled Averaging for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1910.03197.pdf">Accelerating Federated Learning via Momentum Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/1906.06629.pdf">Robust Federated Learning in a Heterogeneous Environment</a></li><li><a href="https://arxiv.org/pdf/1906.08320.pdf">Scalable and Differentially Private Distributed Aggregation in the Shuffled Model</a></li><li><a href="https://arxiv.org/pdf/1905.03871.pdf">Differentially Private Learning with Adaptive Clipping</a></li><li><a href="https://arxiv.org/pdf/1904.10120.pdf">Semi-Cyclic Stochastic Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/1812.06127.pdf">Federated Optimization in Heterogeneous Networks</a></li><li><a href="https://arxiv.org/pdf/1811.11206.pdf">Partitioned Variational Inference: A unified framework encompassing federated and continual learning</a></li><li><a href="https://arxiv.org/pdf/1809.03832.pdf">Learning Rate Adaptation for Federated and Differentially Private Learning</a></li><li><a href="https://arxiv.org/pdf/2006.09992.pdf">Communication-Efficient Robust Federated Learning Over Heterogeneous Datasets</a></li><li><a href="https://arxiv.org/pdf/1808.07217.pdf">Donâ€™t Use Large Mini-Batches, Use Local SGD</a></li><li><a href="https://arxiv.org/pdf/2002.09539.pdf">Overlap Local-SGD: An Algorithmic Approach to Hide Communication Delays in Distributed SGD</a></li><li><a href="https://arxiv.org/pdf/2006.02582.pdf">Local SGD With a Communication Overhead Depending Only on the Number of Workers</a></li><li><a href="https://arxiv.org/pdf/2006.08950.pdf">Federated Accelerated Stochastic Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/1909.04746.pdf">Tighter Theory for Local SGD on Identical and Heterogeneous Data</a></li><li><a href="https://arxiv.org/pdf/2006.06377.pdf">STL-SGD: Speeding Up Local SGD with Stagewise Communication Period</a></li><li><a href="https://arxiv.org/pdf/1808.07576.pdf">Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms</a></li><li><a href="http://arxiv.org/pdf/2006.07490.pdf">Understanding Unintended Memorization in Federated Learning</a></li><li><a href="https://www.usenix.org/conference/hotedge18/presentation/tao">eSGD: Communication Efficient Distributed Deep Learning on the Edge</a> (USENIX 2018 Workshop)</li><li><a href="http://home.cse.ust.hk/~lwangbm/CMFL.pdf">CMFL: Mitigating Communication Overhead for Federated Learning</a></li></ul><h3 id="6-1-Compression"><a href="#6-1-Compression" class="headerlink" title="6.1 Compression"></a>6.1 Compression</h3><ul><li><a href="https://arxiv.org/abs/1812.07210">Expanding the Reach of Federated Learning by Reducing Client Resource Requirements</a></li><li><a href="https://arxiv.org/abs/1610.05492">Federated Learning: Strategies for Improving Communication Efficiency</a> ï¼ˆNIPS2016 Workshop) [Google]</li><li><a href="https://arxiv.org/abs/1905.10988">Natural Compression for Distributed Deep Learning</a></li><li><a href="https://arxiv.org/abs/1909.13014">FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization</a></li><li><a href="https://arxiv.org/abs/1806.04090">ATOMO: Communication-efficient Learning via Atomic Sparsification</a>(NIPS 2018ï¼‰</li><li><a href="https://arxiv.org/abs/1911.07971">vqSGD: Vector Quantized Stochastic Gradient Descent</a></li><li><a href="https://arxiv.org/abs/1610.02132">QSGD: Communication-efficient SGD via gradient quantization and encoding</a> ï¼ˆNIPS 2017)</li><li><a href="https://arxiv.org/abs/1610.02527">Federated Optimization: Distributed Machine Learning for On-Device Intelligence</a> [Google]</li><li><a href="https://arxiv.org/abs/1611.00429">Distributed Mean Estimation with Limited Communication</a> (ICML 2017)</li><li><a href="https://arxiv.org/abs/1611.07555">Randomized Distributed Mean Estimation: Accuracy vs Communication</a></li><li><a href="https://arxiv.org/abs/1901.09847">Error Feedback Fixes SignSGD and other Gradient Compression Schemes</a> (ICML 2019ï¼‰</li><li><a href="http://proceedings.mlr.press/v70/zhang17e.html">ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning</a> (ICML 2017)</li></ul><h2 id="Part-7-Personalized-Federated-Learning"><a href="#Part-7-Personalized-Federated-Learning" class="headerlink" title="Part 7: Personalized Federated Learning"></a>Part 7: Personalized Federated Learning</h2><h3 id="7-1-Meta-Learning"><a href="#7-1-Meta-Learning" class="headerlink" title="7.1 Meta Learning"></a>7.1 Meta Learning</h3><ul><li><a href="https://arxiv.org/abs/1802.07876">Federated Meta-Learning with Fast Convergence and Efficient Communication</a></li><li><a href="https://www.semanticscholar.org/paper/Federated-Meta-Learning-for-Recommendation-Chen-Dong/8e21d353ba283bee8fd18285558e5e8df39d46e8#paper-header">Federated Meta-Learning for Recommendation</a></li><li><a href="https://arxiv.org/abs/1906.02717">Adaptive Gradient-Based Meta-Learning Methods</a></li></ul><h3 id="7-2-Multi-task-Learning"><a href="#7-2-Multi-task-Learning" class="headerlink" title="7.2 Multi-task Learning"></a>7.2 Multi-task Learning</h3><ul><li><a href="https://arxiv.org/abs/1705.10467">MOCHA: Federated Multi-Task Learning</a> (NIPS 2017)</li><li><a href="https://arxiv.org/abs/1906.06268">Variational Federated Multi-Task Learning</a></li><li><a href="https://mlsys.org/Conferences/2019/doc/2018/30.pdf">Federated Kernelized Multi-Task Learning</a></li><li><a href="https://arxiv.org/abs/1910.01991">Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/pdf/2006.13460.pdf">Local Stochastic Approximation: A Unified View of Federated Learning and Distributed Multi-Task Reinforcement Learning Algorithms</a></li></ul><h3 id="7-3-Hierarchical-FL"><a href="#7-3-Hierarchical-FL" class="headerlink" title="7.3 Hierarchical FL"></a>7.3 Hierarchical FL</h3><ul><li><a href="https://arxiv.org/pdf/1905.06641.pdf">Client-Edge-Cloud Hierarchical Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.01647.pdf">(FL startup: Tongdun, HangZhou, China) Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework.</a></li><li><a href="https://arxiv.org/pdf/2002.11343.pdf">HFEL: Joint Edge Association and Resource Allocation for Cost-Efficient Hierarchical Federated Edge Learning</a></li><li><a href="https://arxiv.org/pdf/1909.02362.pdf">Hierarchical Federated Learning Across Heterogeneous Cellular Networks</a></li><li><a href="https://arxiv.org/pdf/2004.11361.pdf">Enhancing Privacy via Hierarchical Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2004.11791.pdf">Federated learning with hierarchical clustering of local updates to improve training on non-IID data.</a></li><li><a href="https://arxiv.org/pdf/1906.00638.pdf">Federated Hierarchical Hybrid Networks for Clickbait Detection</a></li></ul><h3 id="7-4-Transfer-Learning"><a href="#7-4-Transfer-Learning" class="headerlink" title="7.4 Transfer Learning"></a>7.4 Transfer Learning</h3><ul><li><a href="https://arxiv.org/pdf/1812.03337.pdf">Secure Federated Transfer Learning. IEEE Intelligent Systems 2018.</a></li><li><a href="https://arxiv.org/pdf/1910.13271.pdf">Secure and Efficient Federated Transfer Learning</a></li><li><a href="https://arxiv.org/pdf/1907.02745.pdf">Wireless Federated Distillation for Distributed Edge Learning with Heterogeneous Data</a></li><li><a href="https://arxiv.org/pdf/2005.06105.pdf">Proxy Experience Replay: Federated Distillation for Distributed Reinforcement Learning.</a></li><li><a href="https://arxiv.org/pdf/2002.01337.pdf">Cooperative Learning via Federated Distillation over Fading Channels</a></li><li><a href="https://arxiv.org/pdf/1912.11279.pdf">(*) Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer</a></li><li><a href="https://arxiv.org/pdf/1907.06536.pdf">Federated Reinforcement Distillation with Proxy Experience Memory</a></li><li><a href="https://openreview.net/forum?id=xWr8qQCJU3m">Federated Continual Learning with Weighted Inter-client Transfer</a> (ICML 2021)</li></ul><h2 id="Part-8-Decentralization-Incentive-Mechanism"><a href="#Part-8-Decentralization-Incentive-Mechanism" class="headerlink" title="Part 8 Decentralization &amp; Incentive Mechanism"></a>Part 8 Decentralization &amp; Incentive Mechanism</h2><h3 id="8-1-Decentralized"><a href="#8-1-Decentralized" class="headerlink" title="8.1 Decentralized"></a>8.1 Decentralized</h3><ul><li><a href="https://arxiv.org/abs/1803.06443">Communication Compression for Decentralized Training</a> ï¼ˆNIPS 2018ï¼‰</li><li><a href="https://arxiv.org/abs/1907.07346">ğ™³ğšğšğš™ğš‚ğššğšğšğšğš£ğš: Decentralization Meets Error-Compensated Compression</a></li><li><a href="https://arxiv.org/pdf/1910.04956.pdf">Central Server Free Federated Learning over Single-sided Trust Social Networks</a></li><li><a href="https://arxiv.org/pdf/1705.09056.pdf">Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/2005.00797.pdf">Multi-consensus Decentralized Accelerated Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/1905.10466.pdf">Decentralized Bayesian Learning over Graphs.</a></li><li><a href="https://arxiv.org/pdf/1905.06731.pdf">BrainTorrent: A Peer-to-Peer Environment for Decentralized Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1811.09904.pdf">Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning</a></li><li><a href="https://arxiv.org/pdf/1905.09435.pdf">Matcha: Speeding Up Decentralized SGD via Matching Decomposition Sampling</a></li></ul><h3 id="8-2-Incentive-Mechanism"><a href="#8-2-Incentive-Mechanism" class="headerlink" title="8.2 Incentive Mechanism"></a>8.2 Incentive Mechanism</h3><ul><li><a href="https://ieeexplore.ieee.org/document/8832210">Incentive Mechanism for Reliable Federated Learning: A Joint Optimization Approach to Combining Reputation and Contract Theory</a></li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3470814">Towards Fair Federated Learning</a> (KDD 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467281">Federated Adversarial Debiasing for Fair and Transferable Representations</a> (KDD 2021)</li><li><a href="https://arxiv.org/abs/1908.03092">Motivating Workers in Federated Learning: A Stackelberg Game Perspective</a></li><li><a href="https://arxiv.org/abs/1905.07479">Incentive Design for Efficient Federated Learning in Mobile Networks: A Contract Theory Approach</a></li><li><a href="https://arxiv.org/pdf/1905.10497v1.pdf">Fair Resource Allocation in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.09699.pdf">FMore: An Incentive Scheme of Multi-dimensional Auction for Federated Learning in MEC.</a>(ICDCS 2020)</li><li><a href="https://arxiv.org/pdf/1912.06370.pdf">Toward an Automated Auction Framework for Wireless Federated Learning Services Market</a></li><li><a href="https://arxiv.org/pdf/1911.05642.pdf">Federated Learning for Edge Networks: Resource Optimization and Incentive Mechanism</a></li><li><a href="https://www.u-aizu.ac.jp/~pengli/files/fl_incentive_iot.pdf">A Learning-based Incentive Mechanism forFederated Learning</a></li><li><a href="https://arxiv.org/pdf/1911.01046.pdf">A Crowdsourcing Framework for On-Device Federated Learning</a></li><li><a href="https://arxiv.org/abs/1908.11598">Rewarding High-Quality Data via Influence Functions</a></li><li><a href="https://arxiv.org/abs/1811.12082">Joint Service Pricing and Cooperative Relay Communication for Federated Learning</a></li><li><a href="https://arxiv.org/abs/1909.08525">Measure Contribution of Participants in Federated Learning</a></li><li><a href="https://eprint.iacr.org/2018/679.pdf">DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-based Incentive</a></li></ul><h2 id="Part-9-Vertical-Federated-Learning"><a href="#Part-9-Vertical-Federated-Learning" class="headerlink" title="Part 9: Vertical Federated Learning"></a>Part 9: Vertical Federated Learning</h2><ul><li><a href="https://arxiv.org/abs/1912.00513">A Quasi-Newton Method Based Vertical Federated Learning Framework for Logistic Regression</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/pdf/1901.08755.pdf">SecureBoost: A Lossless Federated Learning Framework</a></li><li><a href="https://arxiv.org/pdf/1911.09824.pdf">Parallel Distributed Logistic Regression for Vertical Federated Learning without Third-Party Coordinator</a></li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467169">AsySQN: Faster Vertical Federated Learning Algorithms with Better Computation Resource Utilization</a> (KDD 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3459637.3482361">Large-scale Secure XGB for Vertical Federated Learning</a> (CIKM 2021)</li><li><a href="https://arxiv.org/pdf/1711.10677.pdf">Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption</a></li><li><a href="https://arxiv.org/pdf/1803.04035.pdf">Entity Resolution and Federated Learning get a Federated Resolution.</a></li><li><a href="https://arxiv.org/pdf/2001.11154.pdf">Multi-Participant Multi-Class Vertical Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1912.11187.pdf">A Communication-Efficient Collaborative Learning Framework for Distributed Features</a></li><li><a href="https://arxiv.org/pdf/2004.07427.pdf">Asymmetrical Vertical Federated Learning</a></li><li><a href="https://arxiv.org/abs/2007.06081">VAFL: a Method of Vertical Asynchronous Federated Learning</a> (ICML workshop on FL, 2020)</li><li><a href="https://arxiv.org/abs/2004.12088v2">SplitFed: When Federated Learning Meets Split Learning</a></li><li><a href="https://arxiv.org/abs/1910.13212">Privacy Enhanced Multimodal Neural Representations for Emotion Recognition</a></li><li><a href="https://arxiv.org/abs/1709.06161">PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training</a></li><li><a href="https://arxiv.org/abs/1911.01682">One Pixel Image and RF Signal Based Split Learning for mmWave Received Power Prediction</a></li><li><a href="https://arxiv.org/abs/1812.06415">Stochastic Distributed Optimization for Machine Learning from Decentralized Features</a></li></ul><h3 id="Part-10-Wireless-Communication-and-Cloud-Computing"><a href="#Part-10-Wireless-Communication-and-Cloud-Computing" class="headerlink" title="Part 10: Wireless Communication and Cloud Computing"></a>Part 10: Wireless Communication and Cloud Computing</h3><ul><li><a href="https://arxiv.org/pdf/2006.09801.pdf">Mix2FLD: Downlink Federated Learning After Uplink Federated Distillation With Two-Way Mixup</a></li><li><a href="https://arxiv.org/pdf/2006.02499.pdf">Wireless Communications for Collaborative Federated Learning in the Internet of Things</a></li><li><a href="https://arxiv.org/pdf/2007.00641.pdf">Democratizing the Edge: A Pervasive Edge Computing Framework</a></li><li><a href="https://arxiv.org/pdf/2006.03262.pdf">UVeQFed: Universal Vector Quantization for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2005.09969.pdf">Federated Deep Learning Framework For Hybrid Beamforming in mm-Wave Massive MIMO</a></li><li><a href="https://arxiv.org/pdf/2005.07776.pdf">Efficient Federated Learning over Multiple Access Channel with Differential Privacy Constraints</a></li><li><a href="https://arxiv.org/pdf/2005.05752.pdf">A Secure Federated Learning Framework for 5G Networks</a></li><li><a href="https://arxiv.org/pdf/2005.05265.pdf">Federated Learning and Wireless Communications</a></li><li><a href="https://arxiv.org/pdf/2005.03977.pdf">Lightwave Power Transfer for Federated Learning-based Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/2004.13563.pdf">Towards Ubiquitous AI in 6G with Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2004.09168.pdf">Optimizing Over-the-Air Computation in IRS-Aided C-RAN Systems</a></li><li><a href="https://arxiv.org/pdf/2004.08488.pdf">Network-Aware Optimization of Distributed Learning for Fog Computing</a></li><li><a href="https://arxiv.org/pdf/2004.07351.pdf">On the Design of Communication Efficient Federated Learning over Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/2004.05843.pdf">Federated Machine Learning for Intelligent IoT via Reconfigurable Intelligent Surface</a></li><li><a href="https://arxiv.org/pdf/2004.04314.pdf">Client Selection and Bandwidth Allocation in Wireless Federated Learning Networks: A Long-Term Perspective</a></li><li><a href="https://arxiv.org/pdf/2004.04104.pdf">Resource Management for Blockchain-enabled Federated Learning: A Deep Reinforcement Learning Approach</a></li><li><a href="https://arxiv.org/pdf/2004.00773.pdf">A Blockchain-based Decentralized Federated Learning Framework with Committee Consensus</a></li><li><a href="https://arxiv.org/pdf/2004.00490.pdf">Scheduling for Cellular Federated Edge Learning with Importance and Channel.</a></li><li><a href="https://arxiv.org/pdf/2003.12705.pdf">Differentially Private Federated Learning for Resource-Constrained Internet of Things.</a></li><li><a href="https://arxiv.org/pdf/2003.09375.pdf">Federated Learning for Task and Resource Allocation in Wireless High Altitude Balloon Networks.</a></li><li><a href="https://arxiv.org/pdf/2003.08059.pdf">Gradient Estimation for Federated Learning over Massive MIMO Communication Systems</a></li><li><a href="https://arxiv.org/pdf/2003.01344.pdf">Adaptive Federated Learning With Gradient Compression in Uplink NOMA</a></li><li><a href="https://arxiv.org/pdf/2003.00229.pdf">Performance Analysis and Optimization in Privacy-Preserving Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2003.00199.pdf">Energy-Efficient Federated Edge Learning with Joint Communication and Computation Design</a></li><li><a href="https://arxiv.org/pdf/2002.12873.pdf">Federated Over-the-Air Subspace Learning and Tracking from Incomplete Data</a></li><li><a href="https://arxiv.org/pdf/2002.12507.pdf">Decentralized Federated Learning via SGD over Wireless D2D Networks</a></li><li><a href="https://arxiv.org/pdf/2002.08196.pdf">Federated Learning in the Sky: Joint Power Allocation and Scheduling with UAV Swarms</a></li><li><a href="https://arxiv.org/pdf/2002.05151.pdf">Wireless Federated Learning with Local Differential Privacy</a></li><li><a href="https://arxiv.org/pdf/2002.01337.pdf">Federated Learning under Channel Uncertainty: Joint Client Scheduling and Resource Allocation.</a></li><li><a href="https://arxiv.org/pdf/2001.11567.pdf">Learning from Peers at the Wireless Edge</a></li><li><a href="https://arxiv.org/pdf/2001.10402.pdf">Convergence of Update Aware Device Scheduling for Federated Learning at the Wireless Edge</a></li><li><a href="https://arxiv.org/pdf/2001.08737.pdf">Communication Efficient Federated Learning over Multiple Access Channels</a></li><li><a href="https://arxiv.org/pdf/2001.07845.pdf">Convergence Time Optimization for Federated Learning over Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/2001.05713.pdf">One-Bit Over-the-Air Aggregation for Communication-Efficient Federated Edge Learning: Design and Convergence Analysis</a></li><li><a href="https://arxiv.org/pdf/1912.13163.pdf">Federated Learning with Cooperating Devices: A Consensus Approach for Massive IoT Networks.</a>(IEEE Internet of Things Journal. 2020)</li><li><a href="https://arxiv.org/pdf/1912.07902.pdf">Asynchronous Federated Learning with Differential Privacy for Edge Intelligence</a></li><li><a href="https://arxiv.org/pdf/1912.06273.pdf">Federated learning with multichannel ALOHA</a></li><li><a href="https://arxiv.org/pdf/1912.00131.pdf">Federated Learning with Autotuned Communication-Efficient Secure Aggregation</a></li><li><a href="https://arxiv.org/pdf/1911.07615.pdf">Bandwidth Slicing to Boost Federated Learning in Edge Computing</a></li><li><a href="https://arxiv.org/pdf/1911.02417.pdf">Energy Efficient Federated Learning Over Wireless Communication Networks</a></li><li><a href="https://arxiv.org/pdf/1911.00856.pdf">Device Scheduling with Fast Convergence for Wireless Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1911.00188.pdf">Energy-Aware Analog Aggregation for Federated Learning with Redundant Data</a></li><li><a href="https://arxiv.org/pdf/1910.14648.pdf">Age-Based Scheduling Policy for Federated Learning in Mobile Edge Networks</a></li><li><a href="https://arxiv.org/pdf/1910.13067.pdf">Federated Learning over Wireless Networks: Convergence Analysis and Resource Allocation</a></li><li><a href="http://networking.khu.ac.kr/layouts/net/publications/data/2019)Federated%20Learning%20over%20Wireless%20Network.pdf">Federated Learning over Wireless Networks: Optimization Model Design and Analysis</a></li><li><a href="https://arxiv.org/pdf/1910.09172.pdf">Resource Allocation in Mobility-Aware Federated Learning Networks: A Deep Reinforcement Learning Approach</a></li><li><a href="https://arxiv.org/pdf/1910.06837.pdf">Reliable Federated Learning for Mobile Networks</a></li><li><a href="https://arxiv.org/pdf/1909.12567.pdf">Cell-Free Massive MIMO for Wireless Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1909.07972.pdf">A Joint Learning and Communications Framework for Federated Learning over Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/1909.06512.pdf">On Safeguarding Privacy and Security in the Framework of Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1908.06287.pdf">Scheduling Policies for Federated Learning in Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/1908.05891.pdf">Federated Learning with Additional Mechanisms on Clients to Reduce Communication Costs</a></li><li><a href="https://arxiv.org/pdf/1907.06040.pdf">Energy-Efficient Radio Resource Allocation for Federated Edge Learning</a></li><li><a href="https://arxiv.org/pdf/1906.10893.pdf">Mobile Edge Computing, Blockchain and Reputation-based Crowdsourcing IoT Federated Learning: A Secure, Decentralized and Privacy-preserving System</a></li><li><a href="https://arxiv.org/pdf/1906.10718.pdf">Active Learning Solution on Distributed Edge Computing</a></li><li><a href="https://arxiv.org/pdf/1905.04519.pdf">Fast Uplink Grant for NOMA: a Federated Learning based Approach</a></li><li><a href="https://arxiv.org/pdf/1901.00844.pdf">Machine Learning at the Wireless Edge: Distributed Stochastic Gradient Descent Over-the-Air</a></li><li><a href="https://arxiv.org/pdf/1812.11494.pdf">Broadband Analog Aggregation for Low-Latency Federated Edge Learning</a></li><li><a href="https://arxiv.org/pdf/1812.01202.pdf">Federated Echo State Learning for Minimizing Breaks in Presence in Wireless Virtual Reality Networks</a></li><li><a href="https://arxiv.org/pdf/1811.12082.pdf">Joint Service Pricing and Cooperative Relay Communication for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1809.07857.pdf">In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1905.01656.pdf">Asynchronous Task Allocation for Federated and Parallelized Mobile Edge Learning</a></li><li><a href="https://arxiv.org/abs/1812.03633">Ask to upload some data from client to server Efficient Training Management for Mobile Crowd-Machine Learning: A Deep Reinforcement Learning Approach</a></li><li><a href="https://arxiv.org/abs/1812.11494">Low-latency Broadband Analog Aggregation For Federated Edge Learning</a></li><li><a href="https://arxiv.org/pdf/1907.09769.pdf">Federated Learning over Wireless Fading Channels</a></li><li><a href="https://arxiv.org/abs/1812.11750">Federated Learning via Over-the-Air Computation</a></li></ul><h2 id="Part-11-Federated-with-Deep-learning"><a href="#Part-11-Federated-with-Deep-learning" class="headerlink" title="Part 11: Federated with Deep learning"></a>Part 11: Federated with Deep learning</h2><h3 id="11-1-Neural-Architecture-Search-NAS"><a href="#11-1-Neural-Architecture-Search-NAS" class="headerlink" title="11.1 Neural Architecture Search(NAS)"></a>11.1 Neural Architecture Search(NAS)</h3><ul><li><a href="https://arxiv.org/pdf/2004.08546.pdf">FedNAS: Federated Deep Learning via Neural Architecture Search.</a>(CVPR 2020)</li><li><a href="https://arxiv.org/pdf/2003.02793.pdf">Real-time Federated Evolutionary Neural Architecture Search.</a></li><li><a href="https://arxiv.org/pdf/2002.06352.pdf">Federated Neural Architecture Search.</a></li><li><a href="https://arxiv.org/pdf/2006.10559.pdf">Differentially-private Federated Neural Architecture Search.</a></li></ul><h3 id="11-2-Graph-Neural-Network-GNN"><a href="#11-2-Graph-Neural-Network-GNN" class="headerlink" title="11.2 Graph Neural Network(GNN)"></a>11.2 Graph Neural Network(GNN)</h3><ul><li><a href="https://ieeexplore.ieee.org/document/9005983">SGNN: A Graph Neural Network Based Federated Learning Approach by Hiding Structure</a> (Big Data)</li><li><a href="https://arxiv.org/abs/2008.11989">GraphFederator: Federated Visual Analysis for Multi-party Graphs.</a></li><li><a href="https://arxiv.org/abs/2010.12882">FedE: Embedding Knowledge Graphs in Federated Setting</a></li><li><a href="https://arxiv.org/abs/2011.03248">ASFGNN: Automated Separated-Federated Graph Neural Network</a></li><li><a href="https://arxiv.org/abs/2012.04187">GraphFL: A Federated Learning Framework for Semi-Supervised Node Classification on Graphs</a></li><li><a href="https://arxiv.org/abs/1901.11173">Peer-to-peer Federated Learning on Graphs</a></li><li><a href="https://arxiv.org/abs/1909.12946">Towards Federated Graph Learning for Collaborative Financial Crimes Detection</a></li><li><a href="https://arxiv.org/abs/2005.00455v3">Secure Deep Graph Generation with Link Differential Privacy</a> (IJCAI 2021)</li><li><a href="https://arxiv.org/pdf/2006.05535.pdf">Locally Private Graph Neural Networks</a> (CCS 2021)</li><li><a href="https://arxiv.org/pdf/2006.05535v1.pdf">When Differential Privacy Meets Graph Neural Networks</a></li><li><a href="https://arxiv.org/abs/2109.08907">Releasing Graph Neural Networks with Differential Privacy</a></li><li><a href="https://arxiv.org/abs/2005.11903">Vertically Federated Graph Neural Network for Privacy-Preserving Node Classification</a></li><li><a href="https://arxiv.org/abs/2102.04925">FedGNN: Federated Graph Neural Network for Privacy-Preserving Recommendation</a> (ICML 2021)</li><li><a href="https://federated-learning.org/fl-ijcai-2021/FTL-IJCAI21_paper_20.pdf">Decentralized Federated Graph Neural Networks</a> (IJCAI 2021)</li><li><a href="https://arxiv.org/abs/2106.13423">Federated Graph Classification over Non-IID Graphs</a> (NeurIPS 2021)</li><li><a href="https://arxiv.org/abs/2106.02743">SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2104.07145">FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks</a> (ICLR 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467371">Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling</a> (KDD 2021)</li></ul><h2 id="Part-12-FL-system-Library-Courses"><a href="#Part-12-FL-system-Library-Courses" class="headerlink" title="Part 12: FL system &amp; Library &amp; Courses"></a>Part 12: FL system &amp; Library &amp; Courses</h2><h3 id="12-1-System"><a href="#12-1-System" class="headerlink" title="12.1 System"></a>12.1 System</h3><ul><li><a href="https://arxiv.org/abs/1902.01046">Towards Federated Learning at Scale: System Design</a> <strong>[Must Read]</strong></li><li><a href="https://www.cs.cmu.edu/~muli/file/mu-thesis.pdf">Scaling Distributed Machine Learning with System and Algorithm Co-design</a></li><li><a href="https://ieeexplore.ieee.org/document/8784064">Demonstration of Federated Learning in a Resource-Constrained Networked Environment</a></li><li><a href="https://arxiv.org/abs/1812.02903">Applied Federated Learning: Improving Google Keyboard Query Suggestions</a></li><li><a href="https://arxiv.org/abs/2007.00914">Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy</a></li><li><a href="https://arxiv.org/pdf/2007.13518.pdf">FedML: A Research Library and Benchmark for Federated Machine Learning</a></li><li><a href="https://arxiv.org/pdf/2006.07273.pdf">FLeet: Online Federated Learning via Staleness Awareness and Performance Prediction.</a></li><li><a href="https://arxiv.org/pdf/2006.06983.pdf">Heterogeneity-Aware Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2006.04150.pdf">Decentralised Learning from Independent Multi-Domain Labels for Person Re-Identification</a></li><li><a href="https://arxiv.org/pdf/2005.06850.pdf">[startup] Industrial Federated Learning â€“ Requirements and System Design</a></li><li><a href="https://arxiv.org/pdf/2001.09249.pdf">(*) TiFL: A Tier-based Federated Learning System.</a>(HPDC 2020)</li><li><a href="https://arxiv.org/pdf/2001.04756.pdf">Adaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach</a>(ICDCS 2020)</li><li><a href="https://arxiv.org/pdf/1912.12795.pdf">Quantifying the Performance of Federated Transfer Learning</a></li><li><a href="https://arxiv.org/pdf/1912.01684.pdf">ELFISH: Resource-Aware Federated Learning on Heterogeneous Edge Devices</a></li><li><a href="https://arxiv.org/pdf/1911.04559.pdf">Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices</a></li><li><a href="https://arxiv.org/pdf/1910.11567.pdf">Substra: a framework for privacy-preserving, traceable and collaborative Machine Learning</a></li><li><a href="https://arxiv.org/pdf/1909.07452.pdf">BAFFLE : Blockchain Based Aggregator Free Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1808.08143.pdf">Functional Federated Learning in Erlang (ffl-erl)</a></li><li><a href="https://arxiv.org/pdf/2003.09876.pdf">HierTrain: Fast Hierarchical Edge AI Learning With Hybrid Parallelism in Mobile-Edge-Cloud Computing</a></li><li><a href="https://petuum.com/wp-content/uploads/2019/01/Orpheus.pdf">Orpheus: Efficient Distributed Machine Learning via System and Algorithm Co-design</a></li><li><a href="https://arxiv.org/abs/1810.11112">Scalable Distributed DNN Training using TensorFlow and CUDA-Aware MPI: Characterization, Designs, and Performance Evaluation</a></li><li><a href="https://arxiv.org/abs/1707.09414">Optimized Broadcast for Deep Learning Workloads on Dense-GPU InfiniBand Clusters: MPI or NCCL?</a></li><li><a href="https://arxiv.org/abs/1902.06855">Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet&#x2F;AlexNet Training in 1.5 Minutes</a></li></ul><h3 id="12-2-Courses"><a href="#12-2-Courses" class="headerlink" title="12.2 Courses"></a>12.2 Courses</h3><ul><li><p><a href="https://www.udacity.com/course/applied-cryptography--cs387">Applied Cryptography</a></p></li><li><p><a href="https://medium.com/georgian-impact-blog/a-brief-introduction-to-differential-privacy-eacf8722283b">A Brief Introduction to Differential Privacy</a></p></li><li><p><a href="http://doi.acm.org/10.1145/2976749.2978318">Deep Learning with Differential Privacy.</a></p></li><li><p><a href="http://iamtrask.github.io/2017/03/17/safe-ai/">Building Safe A.I.</a></p><ul><li>A Tutorial for Encrypted Deep Learning</li><li>Use Homomorphic Encryption (HE)</li></ul></li><li><p><a href="https://mortendahl.github.io/2017/04/17/private-deep-learning-with-mpc/">Private Deep Learning with MPC</a></p><ul><li>A Simple Tutorial from Scratch</li><li>Use Multiparty Compuation (MPC)</li></ul></li><li><p><a href="https://mortendahl.github.io/2017/09/19/private-image-analysis-with-mpc/">Private Image Analysis with MPC</a></p><ul><li>Training CNNs on Sensitive Data</li><li>Use SPDZ as MPC protocol</li></ul></li></ul><h3 id="13-2-Secret-Sharing"><a href="#13-2-Secret-Sharing" class="headerlink" title="13.2 Secret Sharing"></a>13.2 Secret Sharing</h3><ul><li><a href="https://www.youtube.com/watch?v=kkMps3X_tEE">Simple Introduction to Sharmirâ€™s Secret Sharing and Lagrange Interpolation</a></li><li><a href="https://mortendahl.github.io/2017/06/04/secret-sharing-part1/">Secret Sharing, Part 1</a>: Shamirâ€™s Secret Sharing &amp; Packed Variant</li><li><a href="https://mortendahl.github.io/2017/06/24/secret-sharing-part2/">Secret Sharing, Part 2</a>: Improve efficiency</li><li><a href="https://mortendahl.github.io/2017/08/13/secret-sharing-part3/">Secret Sharing, Part 3</a></li></ul><h2 id="Part-13-Secure-Multi-party-Computation-MPC"><a href="#Part-13-Secure-Multi-party-Computation-MPC" class="headerlink" title="Part 13: Secure Multi-party Computation(MPC)"></a>Part 13: Secure Multi-party Computation(MPC)</h2><h3 id="13-1-Differential-Privacy"><a href="#13-1-Differential-Privacy" class="headerlink" title="13.1 Differential Privacy"></a>13.1 Differential Privacy</h3><ul><li><a href="https://arxiv.org/abs/1710.06963">Learning Differentially Private Recurrent Language Models</a></li><li><a href="https://arxiv.org/abs/1911.10071">Federated Learning with Bayesian Differential Privacy</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1912.06733">Private Federated Learning with Domain Adaptation</a> ï¼ˆNIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1805.10559">cpSGD: Communication-efficient and differentially-private distributed SGD</a></li><li><a href="https://arxiv.org/pdf/1611.04482.pdf">Practical Secure Aggregation for Federated Learning on User-Held Data.</a>ï¼ˆNIPS 2016 Workshop)</li><li><a href="https://arxiv.org/pdf/1712.07557.pdf">Differentially Private Federated Learning: A Client Level Perspective.</a>ï¼ˆNIPS 2017 Workshop)</li><li><a href="https://arxiv.org/pdf/1805.04049.pdf">Exploiting Unintended Feature Leakage in Collaborative Learning.</a>(S&amp;P 2019)</li><li><a href="https://arxiv.org/pdf/1812.03224.pdf">A Hybrid Approach to Privacy-Preserving Federated Learning.</a> (AISec 2019)</li><li><a href="https://arxiv.org/pdf/1811.04017.pdf">A generic framework for privacy preserving deep learning.</a> (PPML 2018)</li><li><a href="https://arxiv.org/pdf/1910.08385.pdf">Federated Generative Privacy.</a>ï¼ˆIJCAI 2019 FL Workshop)</li><li><a href="https://arxiv.org/pdf/1911.01812.pdf">Enhancing the Privacy of Federated Learning with Sketching.</a></li><li><a href="https://arxiv.org/pdf/1912.05897.pdf">https://aisec.cc/</a></li><li><a href="http://proceedings.mlr.press/v130/zheng21a.html">Federated f-Differential Privacy</a> (AISTATS 2021)</li><li><a href="http://proceedings.mlr.press/v130/girgis21a.html">Shuffled Model of Differential Privacy in Federated Learning</a> (AISTATS 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3459637.3482252">Differentially Private Federated Knowledge Graphs Embedding</a> (CIKM 2021)</li><li><a href="https://arxiv.org/pdf/2002.09096.pdf">Anonymizing Data for Privacy-Preserving Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2002.09843.pdf">Practical and Bilateral Privacy-preserving Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2003.06612.pdf">Decentralized Policy-Based Private Analytics.</a></li><li><a href="https://arxiv.org/pdf/2003.10637.pdf">FedSel: Federated SGD under Local Differential Privacy with Top-k Dimension Selection.</a> (DASFAA 2020)</li><li><a href="https://arxiv.org/pdf/2003.10933.pdf">Learn to Forget: User-Level Memorization Elimination in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.03637.pdf">LDP-Fed: Federated Learning with Local Differential Privacy.</a>ï¼ˆEdgeSys 2020)</li><li><a href="https://arxiv.org/pdf/2004.02264.pdf">PrivFL: Practical Privacy-preserving Federated Regressions on High-dimensional Data over Mobile Networks.</a></li><li><a href="https://arxiv.org/pdf/2004.08856.pdf">Local Differential Privacy based Federated Learning for Internet of Things.</a></li><li><a href="https://arxiv.org/pdf/2004.06337.pdf">Differentially Private AirComp Federated Learning with Power Adaptation Harnessing Receiver Noise.</a></li><li><a href="https://arxiv.org/pdf/2004.06567.pdf">Decentralized Differentially Private Segmentation with PATE.</a>ï¼ˆMICCAI 2020 Under Review)</li><li><a href="https://arxiv.org/pdf/2004.12108.pdf">Privacy Preserving Distributed Machine Learning with Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2005.00218.pdf">Exploring Private Federated Learning with Laplacian Smoothing.</a></li><li><a href="https://arxiv.org/pdf/2005.02503.pdf">Information-Theoretic Bounds on the Generalization Error and Privacy Leakage in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2005.04563.pdf">Efficient Privacy Preserving Edge Computing Framework for Image Classification.</a></li><li><a href="https://arxiv.org/pdf/2006.02456.pdf">A Distributed Trust Framework for Privacy-Preserving Machine Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.04747.pdf">Secure Byzantine-Robust Machine Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.04593.pdf">ARIANN: Low-Interaction Privacy-Preserving Deep Learning via Function Secret Sharing.</a></li><li><a href="https://arxiv.org/pdf/2006.05459.pdf">Privacy For Free: Wireless Federated Learning Via Uncoded Transmission With Adaptive Power Control.</a></li><li><a href="https://arxiv.org/pdf/2006.07218.pdf">(*) Distributed Differentially Private Averaging with Improved Utility and Robustness to Malicious Parties.</a></li><li><a href="https://arxiv.org/pdf/2006.08848.pdf">GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators.</a></li><li><a href="https://arxiv.org/pdf/1911.00222.pdf">Federated Learning with Differential Privacy:Algorithms and Performance Analysis</a></li></ul><h3 id="13-2-Secret-Sharing-1"><a href="#13-2-Secret-Sharing-1" class="headerlink" title="13.2 Secret Sharing"></a>13.2 Secret Sharing</h3><ul><li><a href="https://www.youtube.com/watch?v=kkMps3X_tEE">Simple Introduction to Sharmirâ€™s Secret Sharing and Lagrange Interpolation</a></li><li><a href="https://mortendahl.github.io/2017/06/04/secret-sharing-part1/">Secret Sharing, Part 1</a>: Shamirâ€™s Secret Sharing &amp; Packed Variant</li><li><a href="https://mortendahl.github.io/2017/06/24/secret-sharing-part2/">Secret Sharing, Part 2</a>: Improve efficiency</li><li><a href="https://mortendahl.github.io/2017/08/13/secret-sharing-part3/">Secret Sharing, Part 3</a></li></ul><h2 id="Part-14-Applications"><a href="#Part-14-Applications" class="headerlink" title="Part 14: Applications"></a>Part 14: Applications</h2><ul><li><a href="https://arxiv.org/abs/1907.13113">Federated Learning Approach for Mobile Packet Classification</a></li><li><a href="https://arxiv.org/abs/1911.11807">Federated Learning for Ranking Browser History Suggestions</a> (NIPS 2019 Workshop)</li></ul><h3 id="14-1-Healthcare"><a href="#14-1-Healthcare" class="headerlink" title="14.1 Healthcare"></a>14.1 Healthcare</h3><ul><li><a href="https://arxiv.org/abs/1909.05784">HHHFL: Hierarchical Heterogeneous Horizontal Federated Learning for Electroencephalography</a> (NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1912.01792">Learn Electronic Health Records by Fully Decentralized Federated Learning</a> (NIPS 2019 Workshop)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467185">FLOP: Federated Learning on Medical Datasets using Partial Networks</a> (KDD 2021)</li><li><a href="https://arxiv.org/ftp/arxiv/papers/1903/1903.09296.pdf">Patient Clustering Improves Efficiency of Federated Machine Learning to predict mortality and hospital stay time using distributed Electronic Medical Records</a> <a href="https://venturebeat.com/2019/03/25/federated-learning-technique-predicts-hospital-stay-and-patient-mortality/">[News]</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pubmed/29500022">Federated learning of predictive models from federated Electronic Health Records.</a></li><li><a href="https://arxiv.org/pdf/1907.09173.pdf">FedHealth: A Federated Transfer Learning Framework for Wearable Healthcare</a></li><li><a href="https://arxiv.org/pdf/1810.04304.pdf">Multi-Institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation</a></li><li><a href="https://blogs.nvidia.com/blog/2019/12/01/clara-federated-learning/">NVIDIA Clara Federated Learning to Deliver AI to Hospitals While Protecting Patient Data</a></li><li><a href="https://blogs.nvidia.com/blog/2019/10/13/what-is-federated-learning/">What is Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1812.00564">Split learning for health: Distributed deep learning without sharing raw patient data</a></li><li><a href="https://www.aclweb.org/anthology/W19-5030.pdf">Two-stage Federated Phenotyping and Patient Representation Learning</a> (ACL 2019)</li><li><a href="https://dl.acm.org/doi/10.1145/3097983.3098118">Federated Tensor Factorization for Computational Phenotyping</a> (SIGKDD 2017)</li><li><a href="https://arxiv.org/abs/1907.09173">FedHealth- A Federated Transfer Learning Framework for Wearable Healthcare</a> (ICJAI 2019 workshop)</li><li><a href="https://arxiv.org/abs/1810.04304">Multi-Institutional Deep Learning Modeling Without Sharing Patient Data- A Feasibility Study on Brain Tumor Segmentation</a> (MICCAIâ€™18 Workshop)</li><li><a href="https://aaai.org/ojs/index.php/AAAI/article/view/6121">Federated Patient Hashing</a> (AAAI 2020)</li></ul><h3 id="14-2-Natual-Language-Processing"><a href="#14-2-Natual-Language-Processing" class="headerlink" title="14.2 Natual Language Processing"></a>14.2 Natual Language Processing</h3><p>Google</p><ul><li><a href="https://arxiv.org/abs/1811.03604">Federated Learning for Mobile Keyboard Prediction</a></li><li><a href="https://arxiv.org/abs/1812.02903">Applied Federated Learning: Improving Google Keyboard Query Suggestions</a></li><li><a href="https://arxiv.org/abs/1903.10635">Federated Learning Of Out-Of-Vocabulary Words</a></li><li><a href="https://arxiv.org/abs/1906.04329">Federated Learning for Emoji Prediction in a Mobile Keyboard</a></li></ul><p>Snips</p><ul><li><a href="https://arxiv.org/pdf/1810.05512.pdf">Federated Learning for Wake Keyword Spotting</a> <a href="https://medium.com/snips-ai/federated-learning-for-wake-word-detection-c8b8c5cdd2c5">[Blog]</a> <a href="https://github.com/snipsco/keyword-spotting-research-datasets">[Github]</a></li></ul><h3 id="14-3-Computer-Vision"><a href="#14-3-Computer-Vision" class="headerlink" title="14.3 Computer Vision"></a>14.3 Computer Vision</h3><ul><li><a href="https://arxiv.org/abs/2008.11560">Performance Optimization for Federated Person Re-identification via Benchmark Analysis</a> (ACMMM 2020) <a href="https://github.com/cap-ntu/FedReID">[Github]</a></li><li><a href="https://arxiv.org/abs/1910.11089">Real-World Image Datasets for Federated Learning</a></li><li><a href="https://arxiv.org/abs/2001.06202">FedVision- An Online Visual Object Detection Platform Powered by Federated Learning</a> (IAAI20)</li><li><a href="http://web.pkusz.edu.cn/adsp/files/2019/11/AAAI-FenglinL.1027.pdf">Federated Learning for Vision-and-Language Grounding Problems</a> (AAAI20)</li></ul><h3 id="14-4-Recommendation"><a href="#14-4-Recommendation" class="headerlink" title="14.4 Recommendation"></a>14.4 Recommendation</h3><ul><li><a href="https://arxiv.org/abs/1901.09888">Federated Collaborative Filtering for Privacy-Preserving Personalized Recommendation System</a></li><li><a href="https://arxiv.org/abs/1802.07876">Federated Meta-Learning with Fast Convergence and Efficient Communication</a></li><li><a href="https://arxiv.org/abs/1906.05108">Secure Federated Matrix Factorization</a></li><li><a href="https://www.cs.cmu.edu/~muli/file/difacto.pdf">DiFacto: Distributed Factorization Machines</a></li></ul><h3 id="14-5-Industrial"><a href="#14-5-Industrial" class="headerlink" title="14.5 Industrial"></a>14.5 Industrial</h3><ul><li><a href="https://github.com/matthiaslau/Turbofan-Federated-Learning-POC">Turbofan POC: Predictive Maintenance of Turbofan Engines using Federated Learning</a></li><li><a href="https://turbofan.fastforwardlabs.com/">Turbofan Tycoon Simulation by Cloudera&#x2F;FastForwardLabs</a></li><li><a href="https://florian.github.io/federated-learning/">Firefox Search Bar</a></li></ul><h2 id="Part-15-Organizations-and-Companies"><a href="#Part-15-Organizations-and-Companies" class="headerlink" title="Part 15: Organizations and Companies"></a>Part 15: Organizations and Companies</h2><h3 id="15-1-å›½å†…ç¯‡"><a href="#15-1-å›½å†…ç¯‡" class="headerlink" title="15.1 å›½å†…ç¯‡"></a>15.1 å›½å†…ç¯‡</h3><h5 id="å¾®ä¼—é“¶è¡Œå¼€æº-FATE-æ¡†æ¶"><a href="#å¾®ä¼—é“¶è¡Œå¼€æº-FATE-æ¡†æ¶" class="headerlink" title="å¾®ä¼—é“¶è¡Œå¼€æº FATE æ¡†æ¶."></a>å¾®ä¼—é“¶è¡Œå¼€æº <a href="https://github.com/FederatedAI/FATE">FATE</a> æ¡†æ¶.</h5><p>Qiang Yang, Tianjian Chen, Yang Liu, Yongxin Tong.</p><ul><li><a href="https://dl.acm.org/doi/abs/10.1145/3298981">ã€ŠFederated machine learning: Concept and applicationsã€‹</a></li><li><a href="https://ieeexplore.ieee.org/abstract/document/9440789">ã€ŠSecureboost: A lossless federated learning frameworkã€‹</a></li></ul><h5 id="å­—èŠ‚è·³åŠ¨å¼€æº-FedLearner-æ¡†æ¶"><a href="#å­—èŠ‚è·³åŠ¨å¼€æº-FedLearner-æ¡†æ¶" class="headerlink" title="å­—èŠ‚è·³åŠ¨å¼€æº FedLearner æ¡†æ¶."></a>å­—èŠ‚è·³åŠ¨å¼€æº <a href="https://github.com/bytedance/fedlearner">FedLearner</a> æ¡†æ¶.</h5><p>Jiankai Sun, Weihao Gao, Hongyi Zhang, Junyuan Xie.<a href="https://arxiv.org/pdf/2102.08504.pdf">ã€ŠLabel Leakage and Protection in Two-party Split learningã€‹</a></p><h5 id="åæ§æ¸…äº¤-PrivPy-å¤šæ–¹è®¡ç®—å¹³å°"><a href="#åæ§æ¸…äº¤-PrivPy-å¤šæ–¹è®¡ç®—å¹³å°" class="headerlink" title="åæ§æ¸…äº¤ PrivPy å¤šæ–¹è®¡ç®—å¹³å°"></a>åæ§æ¸…äº¤ PrivPy å¤šæ–¹è®¡ç®—å¹³å°</h5><p>Yi Li, Wei Xu.<a href="https://dl.acm.org/doi/pdf/10.1145/3292500.3330920">ã€ŠPrivPy: General and Scalable Privacy-Preserving Data Miningã€‹</a></p><h5 id="åŒç›¾ç§‘æŠ€-åŒç›¾å¿—é‚¦çŸ¥è¯†è”é‚¦å¹³å°"><a href="#åŒç›¾ç§‘æŠ€-åŒç›¾å¿—é‚¦çŸ¥è¯†è”é‚¦å¹³å°" class="headerlink" title="åŒç›¾ç§‘æŠ€ åŒç›¾å¿—é‚¦çŸ¥è¯†è”é‚¦å¹³å°"></a>åŒç›¾ç§‘æŠ€ åŒç›¾å¿—é‚¦çŸ¥è¯†è”é‚¦å¹³å°</h5><p>Hongyu Li, Dan Meng, Hong Wang, Xiaolin Li.</p><ul><li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9194566">ã€ŠKnowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Frameworkã€‹</a></li><li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9408024">ã€ŠFedMONN: Meta Operation Neural Network for Secure Federated Aggregationã€‹</a></li></ul><h5 id="ç™¾åº¦-MesaTEE-å®‰å…¨è®¡ç®—å¹³å°"><a href="#ç™¾åº¦-MesaTEE-å®‰å…¨è®¡ç®—å¹³å°" class="headerlink" title="ç™¾åº¦ MesaTEE å®‰å…¨è®¡ç®—å¹³å°"></a>ç™¾åº¦ <a href="https://anquan.baidu.com/product/mesatee">MesaTEE</a> å®‰å…¨è®¡ç®—å¹³å°</h5><p>Tongxin Li, Yu Ding, Yulong Zhang, Tao Wei.<a href="https://www.ieee-security.org/TC/SP2019/posters/hotcrp_sp19posters-final11.pdf">ã€Šgbdt-rs: Fast and Trustworthy Gradient Boosting Decision Treeã€‹</a></p><h5 id="çŸ©é˜µå…ƒ-Rosetta-éšç§å¼€æºæ¡†æ¶"><a href="#çŸ©é˜µå…ƒ-Rosetta-éšç§å¼€æºæ¡†æ¶" class="headerlink" title="çŸ©é˜µå…ƒ Rosetta éšç§å¼€æºæ¡†æ¶"></a>çŸ©é˜µå…ƒ <a href="https://github.com/LatticeX-Foundation/Rosetta">Rosetta</a> éšç§å¼€æºæ¡†æ¶</h5><h5 id="ç™¾åº¦-PaddlePaddle-å¼€æºè”é‚¦å­¦ä¹ æ¡†æ¶"><a href="#ç™¾åº¦-PaddlePaddle-å¼€æºè”é‚¦å­¦ä¹ æ¡†æ¶" class="headerlink" title="ç™¾åº¦ PaddlePaddle å¼€æºè”é‚¦å­¦ä¹ æ¡†æ¶"></a>ç™¾åº¦ <a href="https://github.com/PaddlePaddle/PaddleFL">PaddlePaddle</a> å¼€æºè”é‚¦å­¦ä¹ æ¡†æ¶</h5><h5 id="èš‚èšåŒºå—é“¾ç§‘æŠ€-èš‚èšé“¾æ‘©æ–¯å®‰å…¨è®¡ç®—å¹³å°"><a href="#èš‚èšåŒºå—é“¾ç§‘æŠ€-èš‚èšé“¾æ‘©æ–¯å®‰å…¨è®¡ç®—å¹³å°" class="headerlink" title="èš‚èšåŒºå—é“¾ç§‘æŠ€ èš‚èšé“¾æ‘©æ–¯å®‰å…¨è®¡ç®—å¹³å°"></a>èš‚èšåŒºå—é“¾ç§‘æŠ€ <a href="https://antchain.antgroup.com/products/morse">èš‚èšé“¾æ‘©æ–¯å®‰å…¨è®¡ç®—å¹³å°</a></h5><h5 id="é˜¿é‡Œäº‘-DataTrust-éšç§å¢å¼ºè®¡ç®—å¹³å°"><a href="#é˜¿é‡Œäº‘-DataTrust-éšç§å¢å¼ºè®¡ç®—å¹³å°" class="headerlink" title="é˜¿é‡Œäº‘ DataTrust éšç§å¢å¼ºè®¡ç®—å¹³å°"></a>é˜¿é‡Œäº‘ <a href="https://dp.alibaba.com/index">DataTrust</a> éšç§å¢å¼ºè®¡ç®—å¹³å°</h5><h5 id="ç™¾åº¦ç™¾åº¦ç‚¹çŸ³è”é‚¦å­¦ä¹ å¹³å°"><a href="#ç™¾åº¦ç™¾åº¦ç‚¹çŸ³è”é‚¦å­¦ä¹ å¹³å°" class="headerlink" title="ç™¾åº¦ç™¾åº¦ç‚¹çŸ³è”é‚¦å­¦ä¹ å¹³å°"></a>ç™¾åº¦ç™¾åº¦ç‚¹çŸ³è”é‚¦å­¦ä¹ å¹³å°</h5><h5 id="å¯Œæ•°ç§‘æŠ€-é˜¿å‡¡è¾¾å®‰å…¨è®¡ç®—å¹³å°"><a href="#å¯Œæ•°ç§‘æŠ€-é˜¿å‡¡è¾¾å®‰å…¨è®¡ç®—å¹³å°" class="headerlink" title="å¯Œæ•°ç§‘æŠ€ é˜¿å‡¡è¾¾å®‰å…¨è®¡ç®—å¹³å°"></a>å¯Œæ•°ç§‘æŠ€ é˜¿å‡¡è¾¾å®‰å…¨è®¡ç®—å¹³å°</h5><h5 id="é¦™æ¸¯ç†å·¥å¤§å­¦"><a href="#é¦™æ¸¯ç†å·¥å¤§å­¦" class="headerlink" title="é¦™æ¸¯ç†å·¥å¤§å­¦"></a>é¦™æ¸¯ç†å·¥å¤§å­¦</h5><p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/7021">ã€ŠFedVision: An Online Visual Object Detection Platform Powered by Federated Learningã€‹</a></p><p><a href="https://www.usenix.org/system/files/atc20-zhang-chengliang.pdf">ã€ŠBatchCrypt: Efficient Homomorphic Encryption for Cross-Silo Federated Learningã€‹</a></p><p><a href="https://arxiv.org/pdf/1910.09933.pdf">ã€ŠAbnormal Client Behavior Detection in Federated Learningã€‹</a></p><h5 id="åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦"><a href="#åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦" class="headerlink" title="åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦"></a>åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦</h5><p><a href="https://dl.acm.org/doi/abs/10.1145/3298981">ã€ŠFederated machine learning: Concept and applicationsã€‹</a></p><p><a href="https://arxiv.org/pdf/2101.11715.pdf">ã€ŠFailure Prediction in Production Line Based on Federated Learning: An Empirical Studyã€‹</a></p><h3 id="15-2-å›½é™…ç¯‡"><a href="#15-2-å›½é™…ç¯‡" class="headerlink" title="15.2 å›½é™…ç¯‡"></a>15.2 å›½é™…ç¯‡</h3><p>Google æå‡º Federated Learning.<br>H. Brendan McMahan. Daniel Ramage. Jakub KoneÄnÃ½. Kallista A. Bonawitz. Hubert Eichner.</p><p><a href="https://arxiv.org/abs/1602.05629">ã€ŠCommunication-efficient learning of deep networks from decentralized dataã€‹</a></p><p><a href="https://arxiv.org/abs/1610.05492">ã€ŠFederated Learning: Strategies for Improving Communication Efficiencyã€‹</a></p><p><a href="https://arxiv.org/pdf/1912.04977.pdf">ã€ŠAdvances and Open Problems in Federated Learningã€‹</a></p><p><a href="https://arxiv.org/abs/1902.01046">ã€ŠTowards Federated Learning at Scale: System Designã€‹</a></p><p><a href="https://arxiv.org/pdf/1905.03871.pdf">ã€ŠDifferentially Private Learning with Adaptive Clippingã€‹</a></p><p>â€¦â€¦ï¼ˆæ›´å¤šè”é‚¦å­¦ä¹ ç›¸å…³æ–‡ç« è¯·è‡ªè¡Œæœç´¢ Google Scholarï¼‰</p><h4 id="Cornell-University"><a href="#Cornell-University" class="headerlink" title="Cornell University."></a>Cornell University.</h4><p>Antonio Marcedone.</p><p><a href="https://arxiv.org/pdf/1611.04482.pdf">ã€ŠPractical Secure Aggregation for Federated Learning on User-Held Dataã€‹</a></p><p><a href="https://academic.microsoft.com/paper/2949130532/citedby/search?q=Practical%20Secure%20Aggregation%20for%20Privacy%20Preserving%20Machine%20Learning.&qe=RId%253D2949130532&f=&orderBy=0">ã€ŠPractical Secure Aggregation for Privacy-Preserving Machine Learningã€‹</a></p><p>Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov.</p><p><a href="https://arxiv.org/pdf/1807.00459.pdf">ã€ŠHow To Backdoor Federated Learningã€‹</a></p><p><a href="https://proceedings.neurips.cc/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html">ã€ŠDifferential privacy has disparate impact on model accuracyã€‹</a></p><p>Ziteng Sun.</p><p><a href="https://arxiv.org/abs/1911.07963">ã€ŠCan you really backdoor federated learning?ã€‹</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>liberary</title>
      <link href="/2024/07/08/index/"/>
      <url>/2024/07/08/index/</url>
      
        <content type="html"><![CDATA[<h2 id="é©¬åŸå¤ä¹ æ•´ç†"><a href="#é©¬åŸå¤ä¹ æ•´ç†" class="headerlink" title="é©¬åŸå¤ä¹ æ•´ç†"></a>é©¬åŸå¤ä¹ æ•´ç†</h2><h3 id="1-å¯¼è®º"><a href="#1-å¯¼è®º" class="headerlink" title="1.å¯¼è®º"></a>1.å¯¼è®º</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/07/07/hello-world/"/>
      <url>/2024/07/07/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
