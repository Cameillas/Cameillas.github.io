<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>lxt亲启</title>
      <link href="/2024/07/26/lxt%E4%BA%B2%E5%90%AF/"/>
      <url>/2024/07/26/lxt%E4%BA%B2%E5%90%AF/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="b9a6780be6b2acb270e057701633a507d34d726f23732260baf4579dc2af0513">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
        <tags>
            
            <tag> lxt亲启 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习</title>
      <link href="/2024/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/07/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/m0_65121454/article/details/128178708?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522172100375216800182158652%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=172100375216800182158652&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-128178708-null-null.142%5Ev100%5Epc_search_result_base8&utm_term=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&spm=1018.2226.3001.4187">机器学习入门基础（万字总结）（建议收藏！！！）-CSDN博客</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Thesis_Reading</title>
      <link href="/2024/07/15/Time-Series/"/>
      <url>/2024/07/15/Time-Series/</url>
      
        <content type="html"><![CDATA[<h2 id="Time-Series-Contrastive-Learning-with-Information-Aware-Augmentations"><a href="#Time-Series-Contrastive-Learning-with-Information-Aware-Augmentations" class="headerlink" title="Time Series Contrastive Learning with Information-Aware Augmentations"></a><strong>Time Series Contrastive Learning with Information-Aware Augmentations</strong></h2><h2 id="论文阅读"><a href="#论文阅读" class="headerlink" title="论文阅读"></a>论文阅读</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>背景：在近年来，已经有许多对比学习方法被提出，并在实证上取得了显著的成功。</p><p>尽管对比学习在图像和语言领域非常有效和普遍，但在时间序列数据上的应用相对较少。<br>对比学习的关键组成部分：</p><p>对比学习的一个关键组成部分是选择适当的数据增强（augmentation）方式，通过施加一些先验条件构建可行的正样本。这样，编码器可以通过训练来学习稳健和具有区分性的表示。<br>问题陈述：</p><p>与图像和语言领域不同，时间序列数据的“期望”增强样本很难通过人为的先验条件来生成，因为时间序列数据具有多样且人类难以识别的时间结构。如何为给定的对比学习任务和数据集找到对时间序列数据有意义的增强仍然是一个未解的问题。</p><p>解决方法：作者通过信息理论提出了一种方法，旨在同时鼓励高保真度和多样性，从而产生有意义的时间序列数据增强。通过理论分析，提出了选择可行数据增强的标准。</p><p>新方法提出：作者提出了一种新的对比学习方法，名为InfoTS，该方法使用信息感知的增强（information-aware augmentations），能够自适应地选择最优的增强方式用于时间序列表示学习。</p><p>实验证明：在各种数据集上的实验表明，该方法在预测任务上能够实现高竞争性的性能，减少了均方误差（MSE）高达12.0%。在分类任务上相对于主流基线方法，准确度提高了最高达3.7%。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong>Introduction</strong></h3><p>时间序列数据的特性：时间序列数据在现实世界中通常是高维的、非结构化的，具有独特的属性。</p><p>其复杂性和缺乏结构性使得在数据建模方面面临挑战（引用了 Yang 和 Wu 在 2006 年的工作）。</p><p>标注难题：相比于图像和语言数据，时间序列数据通常没有人类能够轻松识别的模式。<br>在实际应用中，由于这些标注难以实现，很难为时间序列数据添加准确的标签。<br>这种标注的困难性限制了深度学习方法的应用，因为这些方法通常需要大量标记数据进行训练，而在时间序列数据上很难获得足够的标记数据（引用了 Eldele 等人在 2021 年的工作）。</p><p>表示学习的重要性：为了克服标注的限制，提到了表示学习的概念。表示学习通过从原始时间序列数据中学习到的固定维度嵌入，保留其固有特征。<br>相对于原始时间序列数据，这些表示具有更好的可传递性和泛化能力。</p><h5 id="补充（对比学习：对比学习是一种学习方法，侧重于通过对比正反两方面的实例来提取有意义的表征。它利用的假设是，在学习到的嵌入空间中，相似的实例应靠得更近，而不相似的实例应离得更远。通过将学习作为一项辨别任务，对比学习允许模型捕捉数据中的相关特征和相似性。）"><a href="#补充（对比学习：对比学习是一种学习方法，侧重于通过对比正反两方面的实例来提取有意义的表征。它利用的假设是，在学习到的嵌入空间中，相似的实例应靠得更近，而不相似的实例应离得更远。通过将学习作为一项辨别任务，对比学习允许模型捕捉数据中的相关特征和相似性。）" class="headerlink" title="补充（对比学习：对比学习是一种学习方法，侧重于通过对比正反两方面的实例来提取有意义的表征。它利用的假设是，在学习到的嵌入空间中，相似的实例应靠得更近，而不相似的实例应离得更远。通过将学习作为一项辨别任务，对比学习允许模型捕捉数据中的相关特征和相似性。）"></a>补充（对比学习：对比学习是一种学习方法，侧重于通过对比正反两方面的实例来提取有意义的表征。它利用的假设是，在学习到的嵌入空间中，相似的实例应靠得更近，而不相似的实例应离得更远。通过将学习作为一项辨别任务，对比学习允许模型捕捉数据中的相关特征和相似性。）</h5><p>对比学习方法的应用：为了应对标注的限制，该文提到在各个领域广泛采用了对比学习方法。<br>对比学习在视觉、语言和图结构数据等领域表现出色，因为它在表示学习方面具有出色的性能。<br>对比学习方法通常通过训练一个编码器（encoder）将实例映射到一个嵌入空间，其中不相似（负面）实例容易与相似（正面）实例区分开来。</p><p>对比学习在时间序列领域的不足：尽管对比学习在其他领域取得了成功，但在时间序列领域却受到相对较少的探索。引用了一些相关的研究工作（Eldele等人在2021年，Franceschi等人在2019年，Fan等人在2020年，Tonekaboni等人在2021年）。</p><p>现有对比学习方法的局限性：现有的对比学习方法通常包括特定的数据增强策略，这些策略通过创建新的、看起来真实的训练数据，但不改变其标签，为任何输入样本构建正样本的替代。<br>这些方法的成功依赖于由领域专业知识指导的精心设计的经验法则。对比学习中通常使用的数据增强主要是为图像和语言数据设计的，如颜色扭曲、翻转、词替换和回译等，这些技术通常不适用于时间序列数据。</p><p>时间序列数据的特殊性：与图像不同，时间序列数据通常关联着难以解释的潜在模式。<br>强大的数据增强方法（如置换）可能破坏这些模式，导致模型将负手工制作的样本误认为正样本。<br>弱的数据增强方法（如抖动）可能生成的增强实例与原始输入过于相似，难以为对比学习提供足够的信息。</p><p>现有方法的局限性和挑战：现有方法存在两个主要限制。首先，与具有人类可识别特征的图像不同，时间序列数据通常具有难以解释的潜在模式。<br>其次，来自不同领域的时间序列数据可能具有多样的性质，将一个通用的数据增强方法应用于所有数据集和任务可能导致次优性能。<br>一些方法采用经验法则从昂贵的试错中选择适当的增强，但这种手动选择不如从学习的角度来说是不可取的。</p><p>数据增强的目的：数据增强有助于实现可推广、可传递和鲁棒的表示学习，通过将输入训练空间正确外推到一个更大的区域（引用了 Wilk 等人在 2018 年的工作）。<br>正实例围绕一个具有判别性的区域，其中所有数据点应与原始实例相似。</p><p>期望的数据增强特性：对于对比表示学习，期望的数据增强应同时具有高保真度和高多样性。<br>高保真度鼓励增强数据保持语义标识的不变性，例如，在分类任务中，生成的增强输入应该保持类别不变。高多样性有助于通过增加泛化能力来进行表示学习</p><p>理论分析基础：作者通过信息论在数据增强中进行了信息流的理论分析，并导出了选择理想的时间序列增强的标准。由于实际时间序列数据的不可解释性，作者假设语义标识由下游任务中的目标表示。**<u><em>因此，高保真度可以通过最大化下游标签与增强数据之间的互信息来实现。</em><strong></strong></u>**</p><p>高保真度的实现：在无监督设置下，当下游标签不可用时，给每个实例分配一个独热伪标签。<br>这些伪标签鼓励不同实例的增强之间可区分性</p><p>高多样性的实现：同时，***<u>作者通过在原始实例条件下最大化增强数据的熵，以增加数据增强的多样性。**</u>*</p><p>自适应数据增强方法 - InfoTS：基于导出的标准，作者提出了一种自适应数据增强方法，即InfoTS。图1展示了该方法的架构。InfoTS的设计旨在避免临时选择或繁琐的试错调整。</p><img src="/2024/07/15/Time-Series/1.png" class alt="1"><p>元学习器（Meta-Learner）的使用：作者在InfoTS中引入了另一个神经网络，称为元学习器，用于与对比学习一同学习数据增强的先验知识。元学习器自动选择从候选增强中生成可行正样本的最佳增强。增强后的实例与随机采样的负实例一同输入时间序列编码器，以对比学习的方式学习表示。</p><h5 id="Reparameterization-Trick：通过重新参数化技巧，元学习器可以根据提出的标准进行高效优化，并通过反向传播来学习。这使得元学习器能够在不倚赖专业知识或繁琐的下游验证的情况下，以每个数据集和每个学习任务的方式自动选择数据增强。"><a href="#Reparameterization-Trick：通过重新参数化技巧，元学习器可以根据提出的标准进行高效优化，并通过反向传播来学习。这使得元学习器能够在不倚赖专业知识或繁琐的下游验证的情况下，以每个数据集和每个学习任务的方式自动选择数据增强。" class="headerlink" title="Reparameterization Trick：通过重新参数化技巧，元学习器可以根据提出的标准进行高效优化，并通过反向传播来学习。这使得元学习器能够在不倚赖专业知识或繁琐的下游验证的情况下，以每个数据集和每个学习任务的方式自动选择数据增强。"></a>Reparameterization Trick：通过重新参数化技巧，元学习器可以根据提出的标准进行高效优化，并通过反向传播来学习。这使得元学习器能够在不倚赖专业知识或繁琐的下游验证的情况下，以每个数据集和每个学习任务的方式自动选择数据增强。</h5><h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><h5 id="Information-Aware-Criteria-for-Good-Augmentations"><a href="#Information-Aware-Criteria-for-Good-Augmentations" class="headerlink" title="Information-Aware Criteria for Good Augmentations"></a>Information-Aware Criteria for Good Augmentations</h5><p>数据增强的目标：对比学习的数据增强目标是创建具有现实合理性的实例，通过不同的转换方法保持语义。与视觉和语言领域的实例不同，时间序列数据的底层语义对人类来说并不可识别，因此很难，甚至是不可能，将人类知识纳入时间序列数据的数据增强过程中。</p><p>难以人为引导的增强：举例说明，旋转图像不会改变其内容或标签，而对一个时间序列实例进行排列可能破坏其信号模式并生成一个无意义的时间序列实例。现实生活中时间序列数据集的巨大异质性使得基于试错选择的方法变得不切实际。</p><p>从作者的角度来看，对比表示学习中的理想数据增强应该保持高保真度、高多样性，并且能够适应不同的数据集。<br>图示和示例：</p><img src="/2024/07/15/Time-Series/2.png" class alt="2"><h4 id="高保真"><a href="#高保真" class="headerlink" title="高保真"></a>高保真</h4><p>高保真度的概念：高保真度的增强应该能够维持在转换中不变的语义标识。<br>由于在实际时间序列数据中，语义的可识别性较差，因此通过目视检查增强的保真度是具有挑战性的。<br>假设和定义：作者假设时间序列实例的语义标识可以由其在下游任务中的标签表示，该标签在训练期间可能是可用或不可用的。<br>在受监督情况下，作者从保持高保真度的目标开始分析，并稍后将其扩展到无监督情况。<br>目标定义：受到信息瓶颈理论的启发，作者定义了一个目标，即保持高保真度，其形式是增强 v 与标签 y 之间的互信息（Mutual Information，MI）较大，即 MI(v; y)。增强函数 v 的定义：<br>作者将增强函数 v 视为输入 x 和随机变量 的概率函数，表示为 v &#x3D; g(x; )。</p><p>互信息的定义：<br>从互信息的定义出发，有 MI(v; y) &#x3D; H(y) - H(y|v)，其中 H(y) 是标签 y 的（Shannon）熵，H(y|v) 是在增强 v 条件下的标签 y 的熵。</p><p>目标函数：<br>由于 H(y) 与数据增强无关，因此目标等价于最小化条件熵 H(y|v)。<br>为了更高效地优化，作者采用了类似于 (Ying et al. 2019) 和 (Luo et al. 2020) 的方法，将条件熵近似为标签 y 和预测标签 ˆy 之间的交叉熵。</p><p>交叉熵的使用：<br>为了有效地近似条件熵，作者选择使用交叉熵作为衡量标签预测与真实标签之间的差异的度量。这里，ˆy 是通过将增强 v 作为输入计算得到的预测标签。</p><p>增强之后的保真性<br>性质 1 - 保持保真度：</p><p>如果增强函数 v 保持独热编码伪标签，那么增强 v 与下游任务标签 y 之间的互信息（尽管在训练中不可见）等同于原始输入 x 与标签 y 之间的互信息，即 MI(v; y) &#x3D; MI(x; y)。<br>独热编码伪标签的关键：</p><p>这个性质的关键在于保持了在无监督学习中引入的独热编码伪标签的信息。独热编码伪标签的引入是为了在对比学习中构建正样本。<br>互信息的等价性：</p><p>在这个性质下，作者指出，如果增强 v 保持了与原始输入 x 相关的独热编码伪标签，那么在对比学习任务中，v 与 y 之间的互信息与 x 与 y 之间的互信息是等价的。</p><p>性质 2 - 添加新信息：</p><p>如果保持独热编码伪标签，增强 v 相对于原始输入 x 包含了新的信息，即熵 H(v) ≥ H(x)。<br>这表示，通过在对比学习中引入独热编码伪标签，增强后的实例相对于原始实例具有更多的信息。<br>证明和详细信息：</p><p>文中提到了有关这一性质的详细证明在附录中，可以进一步查阅附录以获取更详细的解释。<br>性质的意义：</p><p>这些性质说明在无监督学习中，通过保持独热编码伪标签，确保生成的增强实例在对比学习中不会降低保真度，而且可能引入新的信息，有助于对比学习的效果。<br>优化效率问题：</p><p>在无监督学习中，数据集 X 中的标签数等于实例数，直接优化公式（2）是低效且不可扩展的。为了提高效率，作者使用批次的独热编码 yB 近似 y，将标签数 C 从数据集大小减少到批次大小。</p><h4 id="高多样性"><a href="#高多样性" class="headerlink" title="高多样性"></a>高多样性</h4><p>高多样性的重要性：文中指出，增强中存在足够的变化可以提升对比学习模型的泛化能力。在对比学习中，泛化能力指模型在未见过的数据上的性能。<br>信息论中的不确定性：信息论中，随机变量可能结果的不确定性由其熵来描述。熵越高，表示不确定性越大。<br>条件熵的概念：文中提到，为了保持增强的高多样性，作者通过最大化在给定原始输入 x 的条件下增强 v 的条件熵 H(v|x)。<br>条件熵的定义：条件熵是在给定一些其他信息的情况下，关于某个事件的不确定性。在这里，文中没有给出具体的条件熵的定义，但这通常表示为 H(v|x)。</p><h3 id="InfoMin的设计原理："><a href="#InfoMin的设计原理：" class="headerlink" title="InfoMin的设计原理："></a>InfoMin的设计原理：</h3><p>InfoMin的设计基于信息瓶颈理论，即良好的视图应该从原始输入中保留最小而足够的信息。InfoMin假设增强的视图是输入的函数，这在很大程度上限制了数据增强的变化。</p><h5 id="与InfoMin的类比："><a href="#与InfoMin的类比：" class="headerlink" title="与InfoMin的类比："></a>与InfoMin的类比：</h5><p>与信息瓶颈类似，InfoMin假设增强的视图是输入的函数，这严重限制了数据增强的变化。此外，在无监督设置中，高保真度属性被忽略。<br>对时间序列数据的适用性：</p><p>文中指出，InfoMin在图像数据集中效果良好，因为那里有人类知识的可用性。然而，在时间序列数据中，它可能无法生成合理的增强。<br>对信息增强多样性的处理方法：</p><p>InfoMin采用对抗学习来最小化互信息的下界，以增加增强的多样性。相比之下，作者的方法倾向于最小化统计依赖性，更倾向于使用上界（如L1Out），而不是下界。</p><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h4><p>编码器架构：<br>采用的编码器表示为 fθ(x)：R^T × F → R^D，其中 x 是时间序列实例，T 是序列长度，F 是特征的维度，D 是表示向量的维度。<br>组件：</p><p>编码器包含两个主要组件：<br>全连接层<br>10层的扩张卷积神经网络模块<br>时间序列结构的探索：<br>为了探索时间序列的内在结构，采用了全连接层和扩张卷积神经网络模块。<br>对比学习框架中的损失：<br>在对比学习框架中，为了训练编码器，采用了全局层面（实例级别）和局部层面（子序列级别）的损失。</p><p>全局损失</p><h4 id="全局层面的对比损失设计："><a href="#全局层面的对比损失设计：" class="headerlink" title="全局层面的对比损失设计："></a>全局层面的对比损失设计：</h4><p>全局层面的对比损失旨在捕捉时间序列数据集中实例级别的关系。<br>正负对的定义：</p><p>对于给定的时间序列实例批次（Batch）XB ⊆ X，对于每个实例 x ∈ XB，生成一个通过自适应选择的变换得到的增强实例 v。这里 (x, v) 被视为正对（positive pair），而其他 (B−1) 组合 {(x, v0 )}，其中 v0 是 x0 的增强实例，且 x0 &#x3D; x，被视为负对（negative pairs）。<br>基于InfoNCE的对比损失：</p><p>采用了基于InfoNCE（Noise-Contrastive Estimation）的全局层面对比损失设计，其灵感来自于先前的工作（Hjelm et al.，2018）。<br>批次实例级别的对比损失：</p><p>文中没有提供具体的对比损失公式，但提到了根据InfoNCE设计的批次实例级别的对比损失。</p><h4 id="局部损失"><a href="#局部损失" class="headerlink" title="局部损失"></a>局部损失</h4><p>局部层面对比损失的设计：</p><p>局部层面的对比损失旨在探索时间序列中的时序内部关系。<br>增强实例的处理：</p><p>对于时间序列实例 x 的增强实例 v，首先将其分割成一组子序列 S，每个子序列的长度为 L。<br>正对的生成：</p><p>对于子序列 s ∈ S，采用类似于 (Tonekaboni, Eytan, and Goldenberg 2021) 的方法生成一个正对（positive pair）(s, p)，其中选择了与 s 靠近的另一个子序列 p。<br>负对的生成：</p><p>采用了非相邻样本（¯Ns）来生成负对（negative pairs）。<br>局部层面对比损失公式：</p><p>文中未提供具体的局部层面对比损失公式，但表明了如何通过正对和负对的生成来定义该损失。详细的描述可以在附录中找到</p><h4 id="Meta-learner-Network"><a href="#Meta-learner-Network" class="headerlink" title="Meta-learner Network"></a>Meta-learner Network</h4><p>元学习网络的背景：</p><p>先前的时间序列对比学习方法通常采用规则或繁琐的试错方法生成数据增强，这些方法要么依赖于预先制定的人类先验知识，要么需要大量的试验和错误。作者在这部分讨论了如何通过元学习网络来基于提出的信息感知准则自适应地选择最优的增强方式。<br>元学习网络的作用：</p><p>元学习网络被用来动态选择最优的数据增强，从而避免了对特定数据集和学习任务进行设计的繁琐过程。作者将元学习网络的选择看作一种先验选择，以适应性地确定最优的增强方式。<br>候选变换和权重：</p><p>选择了一组候选变换 T，例如 jittering 和 time warping。每个候选变换 ti ∈ T 都与一个权重 pi ∈ (0, 1) 相关联，表示选择变换 ti 的概率。<br>增强实例的计算：</p><p>对于给定的实例 x，通过变换 ti 生成的增强实例 vi 的计算方式未在这里给出，但表示了作者如何使用元学习网络和候选变换集来计算最终的增强实例。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>对比学习在表示学习中的应用：<br>对比学习在各个领域中都得到了广泛的应用，取得了卓越的性能，包括图像、语言等领域（Chen et al. 2020; Xie et al. 2019; You et al. 2020）。</p><p>时间序列领域的对比学习研究：<br>近年来，一些研究工作开始将对比学习应用到时间序列领域（Oord, Li, and Vinyals 2018; Franceschi, Dieuleveut, and Jaggi 2019; Fan, Zhang, and Gao 2020; Eldele et al. 2021; Tonekaboni, Eytan, and Goldenberg 2021; Yue et al. 2021）。</p><p>时间对比学习的方法：<br>时间对比学习使用多项式逻辑回归分类器训练特征提取器，以区分时间序列中的所有段落（Hyvarinen and Morioka 2016）。<br>一些方法生成正对和负对，基于子序列进行对比学习（Franceschi, Dieuleveut, and Jaggi 2019; Tonekaboni, Eytan, and Goldenberg 2021）。<br>TNC采用了去偏的对比目标，以确保在表示空间中，局部邻域内的信号能够与非邻域内的信号区分开（Tonekaboni, Eytan, and Goldenberg 2021）。<br>SelfTime通过探索样本间和样本内关系，采用多种手工制作的增强方法进行无监督时间序列对比学习（Fan, Zhang, and Gao 2020）。<br>TS2Vec以分层方式学习每个时间戳的表示，并进行对比学习（Yue et al. 2021）。</p><p>方法的限制：<br>这些方法中的数据增强方法要么是通用的，要么是通过错误和尝试来选择的，因此在复杂的实际生活数据集中的广泛应用受到了限制。</p><p>自适应数据增强<br>数据增强的重要性：</p><p>数据增强是对比学习中的重要组成部分，有助于提高模型的性能。<br>自适应数据增强的背景：</p><p>其他领域的研究表明，最佳增强的选择取决于下游任务和数据集（Chen et al. 2020; Fan, Zhang, and Gao 2020）。<br>视觉领域的自适应数据增强方法：</p><p>在视觉领域，一些研究者已经探索了自适应选择最佳增强的方法。AutoAugment通过强化学习方法自动搜索平移策略的组合（Cubuk et al. 2019）。Faster-AA通过可微分策略网络改进了数据增强的搜索管道（Hataya et al. 2020）。DADA进一步引入了无偏梯度估计器，实现了高效的一遍优化策略（Li et al. 2020）。<br>对比学习框架中的信息瓶颈理论：</p><p>在对比学习框架中，Tian等人应用信息瓶颈理论，该理论认为最佳视图应该共享最小且足够的信息，以指导对比学习中的良好视图的选择（Tian et al. 2020）。<br>对于时间序列领域的不同方法：</p><p>由于时间序列数据的复杂性，直接应用信息瓶颈框架可能会在增强过程中保留不足够的信息。与之不同的是，作者聚焦于时间序列领域，并提出了一种端到端可微分的方法，以自动选择每个数据集的最佳增强方法。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h5 id="评估设置"><a href="#评估设置" class="headerlink" title="评估设置:"></a>评估设置:</h5><p>通过有监督的时间序列分类来评估表示的质量。<br>使用具有径向基函数内核的SVM分类器对训练集中的表示进行训练。<br>训练后的分类器用于在测试集上进行预测。</p><h5 id="数据集和基线方法"><a href="#数据集和基线方法" class="headerlink" title="数据集和基线方法:"></a>数据集和基线方法:</h5><p>使用两种类型的基准数据集：ETTh1,ETTh2,ETTm1,The Electricity dataset和包含30个多变量数据集的UEA档案。<br>进行比较的基线方法包括TS2Vec、T-Loss、TS-TCC、TST和DTW。<br>InfoTS与这些基线进行比较，并且它在表示学习方面采用了纯粹的无监督设置。</p><h5 id="性能结果"><a href="#性能结果" class="headerlink" title="性能结果:"></a>性能结果:</h5><p>在UEA数据集上的结果总结在表2中（完整结果在附录中）。<br>在由地面真相标签引导的情况下，InfoTSs显著优于其他基线。平均而言，它比最佳基线（TS2Vec）提高了3.7%的分类准确度。<br>在纯粹的无监督设置下，InfoTS实现了第二好的平均性能，展示了它保持保真度的能力。</p><h2 id="论文复现"><a href="#论文复现" class="headerlink" title="论文复现"></a>论文复现</h2><p>import datautils</p><p>from infots import InfoTS as MetaInfoTS</p><p>from utils import init_dl_program, dict2class</p><p>import numpy as np</p><p>import matplotlib.pyplot as plt</p><p>import argparse</p><p>import json</p><p>import warnings</p><p>warnings.filterwarnings(“ignore”)</p><p># 数据集名称</p><p>dataset &#x3D; ‘electricity.uni’</p><p># 读取配置文件</p><p>with open(f’.&#x2F;configures&#x2F;{dataset}.json’) as f:</p><p>​    configs &#x3D; json.load(f)</p><p>parser &#x3D; argparse.ArgumentParser()</p><p>args &#x3D; dict2class(**configs)</p><p># 初始化设备</p><p>device &#x3D; init_dl_program(args.gpu, seed&#x3D;args.seed,</p><p>​                         max_threads&#x3D;args.max_threads)</p><p># 准备数据集并打印列名</p><p>valid_dataset &#x3D; datautils.load_forecast_csv(args.dataset, univar&#x3D;True)</p><p>data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols &#x3D; valid_dataset</p><p>train_data &#x3D; data[:, train_slice]</p><p># 数据预处理</p><p>if train_data.shape[0] &#x3D;&#x3D; 1:</p><p>​    train_slice_number &#x3D; int(train_data.shape[1] &#x2F; args.max_train_length)</p><p>​    if train_slice_number &lt; args.batch_size:</p><p>​        args.batch_size &#x3D; train_slice_number</p><p>else:</p><p>​    if train_data.shape[0] &lt; args.batch_size:</p><p>​        args.batch_size &#x3D; train_data.shape[0]</p><p># 初始化模型</p><p>model &#x3D; MetaInfoTS(</p><p>​    batch_size&#x3D;args.batch_size,</p><p>​    lr&#x3D;args.lr,</p><p>​    meta_lr&#x3D;args.meta_lr,</p><p>​    output_dims&#x3D;args.repr_dims,</p><p>​    max_train_length&#x3D;args.max_train_length,</p><p>​    input_dims&#x3D;train_data.shape[-1],</p><p>​    device&#x3D;device,</p><p>​    num_cls&#x3D;args.batch_size,</p><p>​    aug_p1&#x3D;args.aug_p1,</p><p>​    eval_every_epoch&#x3D;20</p><p>)</p><p># 训练模型</p><p>res &#x3D; model.fit(</p><p>​    train_data,</p><p>​    task_type&#x3D;’forecasting’,</p><p>​    meta_beta&#x3D;args.meta_beta,</p><p>​    n_epochs&#x3D;args.epochs,</p><p>​    n_iters&#x3D;args.iters,</p><p>​    beta&#x3D;args.beta,</p><p>​    verbose&#x3D;False,</p><p>​    miverbose&#x3D;True,</p><p>​    split_number&#x3D;args.split_number,</p><p>​    supervised_meta&#x3D;False,  # for forecasting, use unsupervised setting.</p><p>​    valid_dataset&#x3D;valid_dataset,</p><p>​    train_labels&#x3D;None</p><p>)</p><p># 提取 mse 和 mae</p><p>mse, mae &#x3D; res</p><p># 确保 mse 和 mae 是数组，并进行切片操作</p><p>mse &#x3D; np.array(mse)</p><p>mae &#x3D; np.array(mae)</p><p># 检查长度并进行切片</p><p>if len(mse) &gt; 1 and len(mae) &gt; 1:</p><p>​    mse &#x3D; mse[:-1]</p><p>​    mae &#x3D; mae[:-1]</p><p>else:</p><p>​    print(“mse 和 mae 的长度不足，无法进行切片操作”)</p><p># 绘制结果</p><p>x &#x3D; 20 * np.arange(len(mse))</p><p>plt.plot(x, mse, label&#x3D;”mse@24”)</p><p>plt.plot(x, mae, label&#x3D;”mae@24”)</p><p>plt.legend()</p><p>plt.show()</p><h4 id="实验设置："><a href="#实验设置：" class="headerlink" title="实验设置："></a>实验设置：</h4><h5 id="数据准备与配置"><a href="#数据准备与配置" class="headerlink" title="数据准备与配置"></a>数据准备与配置</h5><p>1.配置文件读取</p><p>dataset &#x3D; ‘electricity.uni’<br>with open(f’.&#x2F;configures&#x2F;{dataset}.json’) as f:<br>    configs &#x3D; json.load(f)<br>parser &#x3D; argparse.ArgumentParser()<br>args &#x3D; dict2class(**configs)</p><p>从<code>configures</code>文件夹中读取指定数据集（<code>electricity.uni</code>）的配置文件，并将其解析为参数对象（<code>args</code>）。这些配置参数用于初始化设备和模型的超参数。</p><p>2.初始化设备</p><p>device &#x3D; init_dl_program(args.gpu, seed&#x3D;args.seed, max_threads&#x3D;args.max_threads)</p><p>初始化设备，包括设置GPU、随机种子和最大线程数，以确保实验的可重复性。</p><h5 id="数据集加载与预处理"><a href="#数据集加载与预处理" class="headerlink" title="数据集加载与预处理"></a>数据集加载与预处理</h5><p>3.加载数据集</p><p>valid_dataset &#x3D; datautils.load_forecast_csv(args.dataset, univar&#x3D;True)<br>data, train_slice, valid_slice, test_slice, scaler, pred_lens, n_covariate_cols &#x3D; valid_dataset<br>train_data &#x3D; data[:, train_slice]</p><p>使用<code>datautils.load_forecast_csv</code>函数加载时间序列数据集，并进行相应的切片操作。<code>train_data</code>是从完整数据中提取的训练部分。</p><p>4.数据预处理</p><p>if train_data.shape[0] &#x3D;&#x3D; 1:<br>    train_slice_number &#x3D; int(train_data.shape[1] &#x2F; args.max_train_length)<br>    if train_slice_number &lt; args.batch_size:<br>        args.batch_size &#x3D; train_slice_number<br>else:<br>    if train_data.shape[0] &lt; args.batch_size:<br>        args.batch_size &#x3D; train_data.shape[0]</p><p>对数据进行预处理，以确保训练批次大小（<code>batch_size</code>）适合数据的维度。如果训练数据只有一个样本，则根据最大训练长度调整批次大小；否则，确保批次大小不超过样本数量。</p><h5 id="模型初始化与训练"><a href="#模型初始化与训练" class="headerlink" title="模型初始化与训练"></a>模型初始化与训练</h5><p>5.模型初始化</p><p>model &#x3D; MetaInfoTS(<br>    batch_size&#x3D;args.batch_size,<br>    lr&#x3D;args.lr,<br>    meta_lr&#x3D;args.meta_lr,<br>    output_dims&#x3D;args.repr_dims,<br>    max_train_length&#x3D;args.max_train_length,<br>    input_dims&#x3D;train_data.shape[-1],<br>    device&#x3D;device,<br>    num_cls&#x3D;args.batch_size,<br>    aug_p1&#x3D;args.aug_p1,<br>    eval_every_epoch&#x3D;20<br>)</p><p>初始化InfoTS模型，设置相关的超参数，包括批次大小、学习率、元学习率、表示维度、最大训练长度、输入维度、设备、分类数量（与批次大小相同）和数据增强概率。</p><p>6.模型训练</p><p>res &#x3D; model.fit(<br>    train_data,<br>    task_type&#x3D;’forecasting’,<br>    meta_beta&#x3D;args.meta_beta,<br>    n_epochs&#x3D;args.epochs,<br>    n_iters&#x3D;args.iters,<br>    beta&#x3D;args.beta,<br>    verbose&#x3D;False,<br>    miverbose&#x3D;True,<br>    split_number&#x3D;args.split_number,<br>    supervised_meta&#x3D;False,<br>    valid_dataset&#x3D;valid_dataset,<br>    train_labels&#x3D;None<br>)</p><p>调用<code>fit</code>方法训练模型，设置任务类型为预测（<code>forecasting</code>），并传入相关超参数，包括元学习率（<code>meta_beta</code>）、训练周期（<code>epochs</code>）、迭代次数（<code>iters</code>）、对比学习中的超参数（<code>beta</code>）、是否显示详细信息（<code>verbose</code>和<code>miverbose</code>）、切分数量（<code>split_number</code>）、是否进行监督元学习（<code>supervised_meta</code>）、验证数据集和训练标签（<code>train_labels</code>）。</p><h5 id="结果处理与可视化"><a href="#结果处理与可视化" class="headerlink" title="结果处理与可视化"></a>结果处理与可视化</h5><p>7.提取结果</p><p>mse, mae &#x3D; res<br>mse &#x3D; np.array(mse)<br>mae &#x3D; np.array(mae)</p><p>从训练结果中提取均方误差（MSE）和平均绝对误差（MAE），并转换为Numpy数组。</p><p>8.处理与可视化</p><p>if len(mse) &gt; 1 and len(mae) &gt; 1:<br>    mse &#x3D; mse[:-1]<br>    mae &#x3D; mae[:-1]<br>else:<br>    print(“mse 和 mae 的长度不足，无法进行切片操作”)</p><p>x &#x3D; 20 * np.arange(len(mse))<br>plt.plot(x, mse, label&#x3D;”mse@24”)<br>plt.plot(x, mae, label&#x3D;”mae@24”)<br>plt.legend()<br>plt.show()</p><p>检查MSE和MAE的长度并进行切片操作，去掉最后一个元素（这是因为在对比学习过程中，最后一个元素可能是训练结束后的残余值）。生成x轴的值（每20个间隔），绘制MSE和MAE随训练过程的变化曲线，并显示图例。</p><h4 id="实验结果和分析"><a href="#实验结果和分析" class="headerlink" title="实验结果和分析"></a>实验结果和分析</h4><p>1.ETTh1数据集</p><img src="/2024/07/15/Time-Series/ETTh1.png" class alt="ETTh1"><p>2.ETTh2数据集</p><img src="/2024/07/15/Time-Series/ETTh2.png" class alt="ETTh2"><p>3.ETTm1数据集</p><img src="/2024/07/15/Time-Series/ETTm1.png" class alt="ETTm1"><p>4.electricity数据集</p><img src="/2024/07/15/Time-Series/electricity'.png" class alt="electricity&#39; %}{% asset_img electricity.png"><p>经过比较，我们可知结果与论文表中呈现结果基本一致。</p><p>性能。**表 2 总结了 UEA 数据集的结果。全部结果见附录。有了ground-truth标签对元学习网络的指导，InfoTS s 的表现大大优于其他基线。平均而言，它比最佳基线 TS2Vec 的分类准确率提高了 3.7%，在所有 30 个 UEA 数据集上的平均排名值为 1.967。在纯粹的无监督设置下，InfoTS 采用独热编码作为伪标签，从而保持了保真度。在表 2 中，InfoTS 的平均性能排名第二，平均排名值为 2.633。</p><p>14173</p><p>​                          表 2：30 个 UEA 数据集的多变量时间序列分类。</p><p><strong>元学习网络的评估。</strong>在这一部分中，我们通过实证分析表明了所开发的元学习器网络在学习最优增强方面的优势。结果如表 3 所示。我们将 InfoTS 与变体 “随机 “和 “全部 “进行了比较。”随机 “每次从候选变换函数中随机选择一个增强，而 “全部 “则按顺序应用变换来生成增强实例。它们的性能受到低质量候选增强的严重影响，这验证了自适应选择在我们方法中的关键作用。2) 为了展示元学习网络训练中多样性和保真度目标的影响，我们加入了 “w&#x2F;o Fidelity “和 “w&#x2F;o Variety “两个变体，它们分别取消了保真度或多样性目标。通过对 InfoTS 和这两个变体的比较，我们从经验上证实了多样性和保真度对于对比学习中的数据增强都很重要。</p><p>14655</p><p>​                           表 3：以 MSE 作为评估指标的电力消融研究。</p><h2 id="总结汇报"><a href="#总结汇报" class="headerlink" title="总结汇报"></a>总结汇报</h2><h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>近年来，许多对比学习方法取得了显著的成功，但在时间序列数据领域的探索较少。对比学习的一个关键组成部分是选择适当的数据增强，以构建可行的正样本，从而训练编码器以学习鲁棒且具有辨别力的表示。与图像和语言领域不同，时间序列数据的多样性和人类难以识别的时间结构，使得人为选择合适的增强方式变得困难。</p><h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><p>时间序列数据在实际应用中具有高维度、非结构化和复杂性，这导致了数据建模的挑战。由于缺乏人类可识别的模式，时间序列数据比图像和语言数据更难标注，这限制了需要大量标注数据的深度学习方法在时间序列数据中的应用。表示学习从原始时间序列中学习固定维度的嵌入，保持其固有特性。相比于原始时间序列数据，这些表示具有更好的可迁移性和泛化能力。</p><h4 id="现有方法的局限性"><a href="#现有方法的局限性" class="headerlink" title="现有方法的局限性"></a>现有方法的局限性</h4><p>现有的对比学习方法通常涉及特定的数据增强策略，这些策略通过不改变标签生成新颖且真实感强的训练数据，以构建输入样本的正样本替代品。这些方法的成功依赖于精心设计的经验法则，但这些法则主要针对图像和语言数据设计，对于时间序列数据并不适用。此外，现有的方法主要有两个局限性：</p><ol><li>时间序列数据通常具有不可解释的底层模式，强增强（如排列）可能破坏这些模式，使模型将负样本误认为正样本。弱增强（如抖动）生成的增强实例可能与原始输入过于相似，不足以提供足够的信息。</li><li>来自不同领域的时间序列数据可能具有不同的性质，通用的数据增强方法（如子序列提取）在所有数据集和任务上的表现可能次优。</li></ol><h4 id="提出的解决方案"><a href="#提出的解决方案" class="headerlink" title="提出的解决方案"></a>提出的解决方案</h4><p>为了应对这些挑战，作者提出了基于信息论的高保真和多样性标准来选择合适的数据增强，并在此基础上提出了一种新的对比学习方法InfoTS，该方法自适应地选择时间序列表示学习的最优增强。InfoTS由三部分组成：候选转换生成原始输入的不同增强；元学习器网络选择最优增强；编码器学习时间序列实例的表示。元学习器与对比编码器学习同时进行。</p><h4 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h4><p>信息感知标准</p><ol><li><strong>高保真</strong>：增强应保持通过变换不变的语义身份。作者假设时间序列实例的语义身份由下游任务的标签表示，目标是通过最大化增强数据与标签之间的互信息来实现高保真。</li><li><strong>高多样性</strong>：通过增加数据增强的多样性来提高表示学习的泛化能力。目标是最大化增强数据在给定原始输入下的条件熵。</li></ol><p>基于这些标准，作者提出了一种自适应数据增强方法InfoTS。具体来说，利用元学习器网络在对比学习的同时学习增强先验，自动选择最优增强生成可行的正样本实例，并与随机采样的负实例一起输入时间序列编码器进行对比学习。</p><h4 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h4><p>实验在各种数据集上进行了验证，结果表明，InfoTS在预测任务中MSE最高可减少12.0%，在分类任务中准确率相对提高最高达3.7%，表现优于现有的主流基线方法。</p><h4 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h4><ol><li>提出指导对比时间序列表示学习的数据增强选择的标准，无需预设知识。</li><li>提出一种方法，自动为不同时间序列数据集选择可行的数据增强，并可通过反向传播高效优化。</li><li>实验证明所提出标准的有效性，展示InfoTS在多种任务上的竞争性能。</li></ol><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>该研究提出了一种新的时间序列对比学习方法，通过信息感知增强自适应选择最优数据增强，从而解决了现有方法的局限性，并在多种任务上表现出色。这一方法为时间序列数据的对比学习提供了新的思路和工具，具有重要的理论和应用价值。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>联邦学习</title>
      <link href="/2024/07/08/README/"/>
      <url>/2024/07/08/README/</url>
      
        <content type="html"><![CDATA[<h1 id="Federated-Learning"><a href="#Federated-Learning" class="headerlink" title="Federated Learning"></a>Federated Learning</h1><h2 id="Part-1-Introduction"><a href="#Part-1-Introduction" class="headerlink" title="Part 1: Introduction"></a>Part 1: Introduction</h2><ul><li><a href="https://federated.withgoogle.com/">Federated Learning Comic</a></li><li><a href="http://ai.googleblog.com/2017/04/federated-learning-collaborative.html">Federated Learning: Collaborative Machine Learning without Centralized Training Data</a></li><li><a href="https://aaai.org/Conferences/AAAI-19/invited-speakers/#yang">GDPR, Data Shotrage and AI (AAAI-19)</a></li><li><a href="https://www.youtube.com/watch?v=89BGjQYA0uE">Federated Learning: Machine Learning on Decentralized Data (Google I&#x2F;O’19)</a></li><li><a href="https://www.fedai.org/static/flwp-en.pdf">Federated Learning White Paper V1.0</a></li><li><a href="https://blog.fastforwardlabs.com/2018/11/14/federated-learning.html">Federated learning: distributed machine learning with data locality and privacy</a></li></ul><h2 id="Part-2-Survey"><a href="#Part-2-Survey" class="headerlink" title="Part 2: Survey"></a>Part 2: Survey</h2><ul><li><a href="https://arxiv.org/abs/1908.07873">Federated Learning: Challenges, Methods, and Future Directions</a></li><li><a href="https://arxiv.org/abs/1907.09693">Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection</a></li><li><a href="https://arxiv.org/abs/1909.11875">Federated Learning in Mobile Edge Networks: A Comprehensive Survey</a></li><li><a href="https://arxiv.org/abs/1908.06847">Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges</a></li><li><a href="https://arxiv.org/pdf/1907.08349.pdf">Convergence of Edge Computing and Deep Learning: A Comprehensive Survey</a></li><li><a href="https://arxiv.org/pdf/1912.04977.pdf">Advances and Open Problems in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1902.04885.pdf">Federated Machine Learning: Concept and Applications</a></li><li><a href="https://arxiv.org/pdf/2003.02133.pdf">Threats to Federated Learning: A Survey</a></li><li><a href="https://arxiv.org/pdf/2003.08673.pdf">Survey of Personalization Techniques for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2006.06217.pdf">SECure: A Social and Environmental Certificate for AI Systems</a></li><li><a href="https://arxiv.org/pdf/2006.03594.pdf">From Federated Learning to Fog Learning: Towards Large-Scale Distributed Machine Learning in Heterogeneous Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/2006.02931.pdf">Federated Learning for 6G Communications: Challenges, Methods, and Future Directions</a></li><li><a href="https://arxiv.org/pdf/2004.11794.pdf">A Review of Privacy Preserving Federated Learning for Private IoT Analytics</a></li><li><a href="https://arxiv.org/pdf/2002.11545.pdf">Towards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective</a></li><li><a href="https://arxiv.org/pdf/2002.10610.pdf">Federated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art</a></li><li><a href="https://arxiv.org/pdf/1912.04859.pdf">Privacy-Preserving Blockchain Based Federated Learning with Differential Data Sharing</a></li><li><a href="https://arxiv.org/pdf/1912.01554.pdf">An Introduction to Communication Efficient Edge Machine Learning</a></li><li><a href="https://arxiv.org/pdf/1911.06270.pdf">Federated Learning for Healthcare Informatics</a></li><li><a href="https://arxiv.org/pdf/1910.06799.pdf">Federated Learning for Coalition Operations</a></li><li><a href="https://arxiv.org/pdf/1812.03288.pdf">No Peek: A Survey of private distributed deep learning</a></li><li><a href="http://arxiv.org/pdf/2002.09668.pdf">Communication-Efficient Edge AI: Algorithms and Systems</a></li></ul><h2 id="Part-3-Benchmarks"><a href="#Part-3-Benchmarks" class="headerlink" title="Part 3: Benchmarks"></a>Part 3: Benchmarks</h2><ul><li><a href="https://arxiv.org/abs/1812.01097">LEAF: A Benchmark for Federated Settings</a>(<a href="https://github.com/TalwalkarLab/leaf">https://github.com/TalwalkarLab/leaf</a>) [Recommend]</li><li><a href="https://www.researchgate.net/profile/Gregor_Ulm/publication/329106719_A_Performance_Evaluation_of_Federated_Learning_Algorithms/links/5c0fabcfa6fdcc494febf907/A-Performance-Evaluation-of-Federated-Learning-Algorithms.pdf">A Performance Evaluation of Federated Learning Algorithms</a></li><li><a href="https://arxiv.org/abs/1908.01924">Edge AIBench: Towards Comprehensive End-to-end Edge Computing Benchmarking</a></li></ul><h2 id="Part-4-Converge"><a href="#Part-4-Converge" class="headerlink" title="Part 4: Converge"></a>Part 4: Converge</h2><h3 id="4-1-Model-Aggregation"><a href="#4-1-Model-Aggregation" class="headerlink" title="4.1 Model Aggregation"></a>4.1 Model Aggregation</h3><ul><li><a href="https://arxiv.org/abs/1902.11175">One-Shot Federated Learning</a></li><li><a href="https://arxiv.org/abs/1910.08234">Federated Learning with Unbiased Gradient Aggregation and Controllable Meta Updating</a> (NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1905.12022">Bayesian Nonparametric Federated Learning of Neural Networks</a> (ICML 2019)</li><li><a href="https://openreview.net/forum?id=dgtpE6gKjHn">FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning</a> (ICLR 2021)</li><li><a href="https://arxiv.org/abs/1902.00146">Agnostic Federated Learning</a> (ICML 2019)</li><li><a href="https://openreview.net/forum?id=BkluqlSFDS">Federated Learning with Matched Averaging</a> (ICLR 2020)</li><li><a href="https://arxiv.org/abs/1907.01132">Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications</a></li></ul><h3 id="4-2-Convergence-Research"><a href="#4-2-Convergence-Research" class="headerlink" title="4.2 Convergence Research"></a>4.2 Convergence Research</h3><ul><li><a href="https://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication">A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication</a> （NIPS 2018）</li><li><a href="https://openreview.net/forum?id=jDdzh5ul-d">Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning</a> (ICLR 2021)</li><li><a href="https://arxiv.org/pdf/2007.07682.pdf">FetchSGD: Communication-Efficient Federated Learning with Sketching</a></li><li><a href="https://arxiv.org/abs/2105.05001">FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning Convergence Analysis</a> (ICML 2021)</li><li><a href="http://proceedings.mlr.press/v130/shi21c.html">Federated Multi-armed Bandits with Personalization</a> (AISTATS 2021)</li><li><a href="http://proceedings.mlr.press/v130/haddadpour21a.html">Federated Learning with Compression: Unified Analysis and Sharp Guarantees</a> (AISTATS 2021)</li><li><a href="http://proceedings.mlr.press/v130/charles21a.html">Convergence and Accuracy Trade-Offs in Federated Learning and Meta-Learning</a> (AISTATS 2021)</li><li><a href>Towards Flexible Device Participation in Federated Learning</a> (AISTATS 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467309">Fed2: Feature-Aligned Federated Learning</a> (KDD 2021)</li><li><a href="https://arxiv.org/pdf/1812.06127">Federated Optimization for Heterogeneous Networks</a></li><li><a href="https://arxiv.org/abs/1907.02189">On the Convergence of FedAvg on Non-IID Data</a> <a href="https://openreview.net/forum?id=HJxNAnVtDS">[OpenReview]</a></li><li><a href="https://arxiv.org/abs/1910.09126">Communication Efficient Decentralized Training with Multiple Local Updates</a></li><li><a href="https://arxiv.org/abs/1805.09767">Local SGD Converges Fast and Communicates Little</a></li><li><a href="https://arxiv.org/abs/1910.00643">SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum</a></li><li><a href="https://arxiv.org/abs/1807.06629">Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning</a> (AAAI 2018）</li><li><a href="https://arxiv.org/abs/1905.03817">On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization</a> (ICML 2019）</li><li><a href="https://arxiv.org/abs/1811.11479">Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data</a></li><li><a href="https://arxiv.org/abs/1905.12648">Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data</a> （NIPS 2019 Workshop)</li></ul><h3 id="4-3-Statistical-Heterogeneity"><a href="#4-3-Statistical-Heterogeneity" class="headerlink" title="4.3 Statistical Heterogeneity"></a>4.3 Statistical Heterogeneity</h3><ul><li><a href="https://arxiv.org/pdf/2005.11418.pdf">FedPD: A Federated Learning Framework with Optimal Rates andAdaptivity to Non-IID Data</a></li><li><a href="https://openreview.net/forum?id=6YEQUn0QICG">FedBN: Federated Learning on Non-IID Features via Local Batch Normalization</a> (ICLR 2021)</li><li><a href="https://openreview.net/forum?id=Ogga20D2HO-">FedMix: Approximation of Mixup under Mean Augmented Federated Learning</a> (ICLR 2021)</li><li><a href="https://openreview.net/forum?id=TNkPBBYFkXg">HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients</a> (ICLR 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467254">FedRS: Federated Learning with Restricted Softmax for Label Distribution Non-IID Data</a> (KDD 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3459637.3482345">FedMatch: Federated Learning Over Heterogeneous Question Answering Data</a> (CIKM 2021)</li><li><a href="https://arxiv.org/pdf/1905.09684.pdf">Decentralized Learning of Generative Adversarial Networks from Non-iid Data</a></li><li><a href="https://arxiv.org/pdf/2008.06217.pdf">Towards Class Imbalance in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1811.11479v1.pdf">Communication-Efficient On-Device Machine Learning:Federated Distillation and Augmentationunder Non-IID Private Data</a></li><li><a href="https://arxiv.org/pdf/2007.07481.pdf">Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization</a></li><li><a href="https://arxiv.org/abs/1911.02054">Federated Adversarial Domain Adaptation</a></li><li><a href="https://arxiv.org/pdf/2004.10342.pdf">Federated Learning with Only Positive Labels</a></li><li><a href="https://arxiv.org/abs/1806.00582">Federated Learning with Non-IID Data</a> </li><li><a href="https://arxiv.org/abs/1910.00189">The Non-IID Data Quagmire of Decentralized Machine Learning</a></li><li><a href="https://arxiv.org/pdf/1903.02891">Robust and Communication-Efficient Federated Learning from Non-IID Data</a> (IEEE transactions on neural networks and learning systems)</li><li><a href="https://arxiv.org/abs/1910.03581">FedMD: Heterogenous Federated Learning via Model Distillation</a> (NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1909.04715">First Analysis of Local GD on Heterogeneous Data</a></li><li><a href="https://arxiv.org/abs/1910.06378">SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning</a></li><li><a href="https://arxiv.org/abs/1909.12488">Improving Federated Learning Personalization via Model Agnostic Meta Learning</a> (NIPS 2019 Workshop)</li><li><a href="https://openreview.net/forum?id=ehJqJQk9cw">Personalized Federated Learning with First Order Model Optimization</a> (ICLR 2021)</li><li><a href="https://arxiv.org/pdf/1811.12629">LoAdaBoost: Loss-Based AdaBoost Federated Machine Learning on Medical Data</a></li><li><a href="https://openreview.net/forum?id=SJeOAJStwB">On Federated Learning of Deep Networks from Non-IID Data: Parameter Divergence and the Effects of Hyperparametric Methods</a></li><li><a href="https://arxiv.org/abs/1910.07796">Overcoming Forgetting in Federated Learning on Non-IID Data</a> （NIPS 2019 Workshop)</li><li><a href="#workshop">FedMAX: Activation Entropy Maximization Targeting Effective Non-IID Federated Learning</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/pdf/2003.00295.pdf">Adaptive Federated Optimization.</a>(ICLR 2021 (Under Review))</li><li><a href="https://arxiv.org/pdf/1707.01155.pdf">Stochastic, Distributed and Federated Optimization for Machine Learning. FL PhD Thesis. By Jakub</a></li><li><a href="https://arxiv.org/pdf/1706.07880.pdf">Collaborative Deep Learning in Fixed Topology Networks</a></li><li><a href="https://arxiv.org/pdf/2006.09637.pdf">FedCD: Improving Performance in non-IID Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.10937.pdf">Life Long Learning: FedFMC: Sequential Efficient Federated Learning on Non-iid Data.</a></li><li><a href="https://arxiv.org/pdf/2006.08907.pdf">Robust Federated Learning: The Case of Affine Distribution Shifts.</a></li><li><a href="https://arxiv.org/abs/2102.07078">Exploiting Shared Representations for Personalized Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2103.04628">Personalized Federated Learning using Hypernetworks</a> (ICML 2021)</li><li><a href="https://onikle.com/articles/359482">Ditto: Fair and Robust Federated Learning Through Personalization</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2105.10056">Data-Free Knowledge Distillation for Heterogeneous Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2102.03198">Bias-Variance Reduced Local SGD for Less Heterogeneous Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2103.00697">Heterogeneity for the Win: One-Shot Federated Clustering</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2105.05883">Clustered Sampling: Low-Variance and Improved Representativity for Clients Selection in Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2102.04635">Federated Deep AUC Maximization for Hetergeneous Data with a Constant Communication Complexity</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2104.08776">Federated Learning of User Verification Models Without Sharing Embeddings</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2103.03228">One for One, or All for All: Equilibria and Optimality of Collaboration in Federated Learning</a> (ICML 2021)</li><li><a href="https://arxiv.org/pdf/2006.07242.pdf">Ensemble Distillation for Robust Model Fusion in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.05148.pdf">XOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.04088.pdf">An Efficient Framework for Clustered Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2005.12657.pdf">Continual Local Training for Better Initialization of Federated Models.</a></li><li><a href="https://arxiv.org/pdf/2005.11418.pdf">FedPD: A Federated Learning Framework with Optimal Rates and Adaptivity to Non-IID Data.</a></li><li><a href="https://arxiv.org/pdf/2005.10848.pdf">Global Multiclass Classification from Heterogeneous Local Models.</a></li><li><a href="https://arxiv.org/pdf/2005.01026.pdf">Multi-Center Federated Learning.</a></li><li><a href="https://openreview.net/forum?id=ce6CFXBh30h">Federated Semi-Supervised Learning with Inter-Client Consistency &amp; Disjoint Learning</a> (ICLR 2021)</li><li><a href="https://arxiv.org/pdf/2004.03657.pdf">(*) FedMAX: Mitigating Activation Divergence for Accurate and Communication-Efficient Federated Learning. CMU ECE.</a></li><li><a href="https://arxiv.org/pdf/2003.13461.pdf">(*) Adaptive Personalized Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2003.12795.pdf">Semi-Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.11223.pdf">Device Heterogeneity in Federated Learning: A Superquantile Approach.</a></li><li><a href="https://arxiv.org/pdf/2002.10671.pdf">Personalized Federated Learning for Intelligent IoT Applications: A Cloud-Edge based Framework</a></li><li><a href="https://arxiv.org/pdf/2002.10619.pdf">Three Approaches for Personalization with Applications to Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.07948.pdf">Personalized Federated Learning: A Meta-Learning Approach</a></li><li><a href="https://arxiv.org/pdf/2002.05038.pdf">Towards Federated Learning: Robustness Analytics to Data Heterogeneity</a></li><li><a href="https://arxiv.org/pdf/2002.04758.pdf">Salvaging Federated Learning by Local Adaptation</a></li><li><a href="https://arxiv.org/pdf/2001.11359.pdf">FOCUS: Dealing with Label Quality Disparity in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2001.08300.pdf">Overcoming Noisy and Irrelevant Data in Federated Learning.</a>(ICPR 2020)</li><li><a href="https://arxiv.org/pdf/2001.03229.pdf">Real-Time Edge Intelligence in the Making: A Collaborative Learning Framework via Federated Meta-Learning.</a></li><li><a href="https://arxiv.org/pdf/2001.01523.pdf">(*) Think Locally, Act Globally: Federated Learning with Local and Global Representations. NeurIPS 2019 Workshop on Federated Learning distinguished student paper award</a></li><li><a href="https://arxiv.org/pdf/1912.00818.pdf">Federated Learning with Personalization Layers</a></li><li><a href="https://arxiv.org/pdf/1910.10252.pdf">Federated Evaluation of On-device Personalization</a></li><li><a href="https://arxiv.org/pdf/1909.08525.pdf">Measure Contribution of Participants in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1909.06335.pdf">(*) Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification</a></li><li><a href="https://arxiv.org/pdf/1907.06426.pdf">Multi-hop Federated Private Data Augmentation with Sample Compression</a></li><li><a href="https://arxiv.org/pdf/1906.01736.pdf">Distributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms</a></li><li><a href="https://arxiv.org/pdf/1902.08999.pdf">High Dimensional Restrictive Federated Model Selection with multi-objective Bayesian Optimization over shifted distributions</a></li><li><a href="https://arxiv.org/pdf/1912.13075.pdf">Robust Federated Learning Through Representation Matching and Adaptive Hyper-parameters</a></li><li><a href="https://arxiv.org/pdf/2005.12326.pdf">Towards Efficient Scheduling of Federated Mobile Devices under Computational and Statistical Heterogeneity</a></li><li><a href="https://arxiv.org/pdf/2007.04806.pdf">Client Adaptation improves Federated Learning with Simulated Non-IID Clients</a></li></ul><h3 id="4-4-Adaptive-Aggregation"><a href="#4-4-Adaptive-Aggregation" class="headerlink" title="4.4 Adaptive Aggregation"></a>4.4 Adaptive Aggregation</h3><ul><li><a href="https://link.springer.com.remotexs.ntu.edu.sg/chapter/10.1007/978-3-030-14880-5_2">Asynchronous Federated Learning for Geospatial Applications</a> (ECML PKDD Workshop 2018） </li><li><a href="https://arxiv.org/abs/1903.03934">Asynchronous Federated Optimization</a></li><li><a href="https://arxiv.org/abs/1804.05271">Adaptive Federated Learning in Resource Constrained Edge Computing Systems</a> (IEEE Journal on Selected Areas in Communications, 2019）</li><li><a href="https://arxiv.org/abs/2102.06387">The Distributed Discrete Gaussian Mechanism for Federated Learning with Secure Aggregation</a> (ICML 2021)</li></ul><h2 id="Part-5-Security"><a href="#Part-5-Security" class="headerlink" title="Part 5: Security"></a>Part 5: Security</h2><h3 id="5-1-Adversarial-Attacks"><a href="#5-1-Adversarial-Attacks" class="headerlink" title="5.1 Adversarial Attacks"></a>5.1 Adversarial Attacks</h3><ul><li><a href="https://arxiv.org/abs/1911.07963">Can You Really Backdoor Federated Learning? </a>(NeruIPS 2019)</li><li><a href="https://dais-ita.org/sites/default/files/main_secml_model_poison.pdf">Model Poisoning Attacks in Federated Learning</a> (NIPS workshop 2018）</li><li><a href="https://arxiv.org/pdf/2004.04676.pdf">An Overview of Federated Deep Learning Privacy Attacks and Defensive Strategies.</a></li><li><a href="https://arxiv.org/pdf/1807.00459.pdf">How To Backdoor Federated Learning.</a>(AISTATS 2020)</li><li><a href="https://arxiv.org/pdf/1702.07464.pdf">Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning.</a>(ACM CCS 2017)</li><li><a href="https://arxiv.org/pdf/1803.01498.pdf">Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates</a></li><li><a href="https://papers.nips.cc/paper/9617-deep-leakage-from-gradients.pdf">Deep Leakage from Gradients.</a>(NIPS 2019)</li><li><a href="https://arxiv.org/pdf/1812.00910.pdf">Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1812.00535.pdf">Beyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning.</a>(INFOCOM 2019)</li><li><a href="https://arxiv.org/pdf/1811.12470.pdf">Analyzing Federated Learning through an Adversarial Lens.</a>(ICML 2019）</li><li><a href="https://arxiv.org/pdf/1808.04866.pdf">Mitigating Sybils in Federated Learning Poisoning.</a>(RAID 2020)</li><li><a href="https://arxiv.org/abs/1811.03761">RSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets.</a>(AAAI 2019)</li><li><a href="https://arxiv.org/pdf/2004.10397.pdf">A Framework for Evaluating Gradient Leakage Attacks in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1911.11815.pdf">Local Model Poisoning Attacks to Byzantine-Robust Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.07026.pdf">Backdoor Attacks on Federated Meta-Learning</a></li><li><a href="https://arxiv.org/pdf/2004.04986.pdf">Towards Realistic Byzantine-Robust Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2004.10020.pdf">Data Poisoning Attacks on Federated Machine Learning.</a></li><li><a href="https://arxiv.org/pdf/2004.12571.pdf">Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.13041.pdf">Byzantine-Resilient High-Dimensional SGD with Local Iterations on Heterogeneous Data.</a></li><li><a href="https://arxiv.org/pdf/2006.11489.pdf">FedMGDA+: Federated Learning meets Multi-objective Optimization.</a></li><li><a href="http://proceedings.mlr.press/v130/fraboni21a.html">Free-rider Attacks on Model Aggregation in Federated Learning</a> (AISTATS 2021)</li><li><a href="https://arxiv.org/pdf/2006.15632.pdf">FDA3 : Federated Defense Against Adversarial Attacks for Cloud-Based IIoT Applications.</a></li><li><a href="https://arxiv.org/pdf/2003.07630.pdf">Privacy-preserving Weighted Federated Learning within Oracle-Aided MPC Framework.</a></li><li><a href="https://arxiv.org/pdf/2003.00937.pdf">BASGD: Buffered Asynchronous SGD for Byzantine Learning.</a></li><li><a href="https://arxiv.org/pdf/2002.10940.pdf">Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees.</a></li><li><a href="https://arxiv.org/pdf/2002.00211.pdf">Learning to Detect Malicious Clients for Robust Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1912.13445.pdf">Robust Aggregation for Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1912.12370.pdf">Towards Deep Federated Defenses Against Malware in Cloud Ecosystems.</a></li><li><a href="https://arxiv.org/pdf/1912.11464.pdf">Attack-Resistant Federated Learning with Residual-based Reweighting.</a></li><li><a href="https://arxiv.org/pdf/1911.12560.pdf">Free-riders in Federated Learning: Attacks and Defenses.</a></li><li><a href="https://arxiv.org/pdf/1911.00251.pdf">Robust Federated Learning with Noisy Communication.</a></li><li><a href="https://arxiv.org/pdf/1910.09933.pdf">Abnormal Client Behavior Detection in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1910.06044.pdf">Eavesdrop the Composition Proportion of Training Labels in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1909.05125.pdf">Byzantine-Robust Federated Machine Learning through Adaptive Model Averaging.</a></li><li><a href="https://arxiv.org/pdf/1908.08340.pdf">An End-to-End Encrypted Neural Network for Gradient Updates Transmission in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/1906.00887.pdf">Secure Distributed On-Device Learning Networks With Byzantine Adversaries.</a></li><li><a href="https://arxiv.org/pdf/1905.02941.pdf">Robust Federated Training via Collaborative Machine Teaching using Trusted Instances.</a></li><li><a href="https://arxiv.org/pdf/1811.09712.pdf">Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting.</a></li><li><a href="https://arxiv.org/pdf/2003.14053.pdf">Inverting Gradients - How easy is it to break privacy in federated learning?</a></li></ul><h3 id="5-2-Data-Privacy-and-Confidentiality"><a href="#5-2-Data-Privacy-and-Confidentiality" class="headerlink" title="5.2 Data Privacy and Confidentiality"></a>5.2 Data Privacy and Confidentiality</h3><ul><li><a href="https://arxiv.org/abs/1805.05838">Gradient-Leaks: Understanding and Controlling Deanonymization in Federated Learning</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/pdf/1910.05467.pdf">Quantification of the Leakage in Federated Learning</a></li></ul><h2 id="Part-6-Communication-Efficiency"><a href="#Part-6-Communication-Efficiency" class="headerlink" title="Part 6: Communication Efficiency"></a>Part 6: Communication Efficiency</h2><ul><li><a href="https://arxiv.org/abs/1602.05629">Communication-Efficient Learning of Deep Networks from Decentralized Data</a>](<a href="https://github.com/roxanneluo/Federated-Learning">https://github.com/roxanneluo/Federated-Learning</a>) [Google] <strong>[Must Read]</strong></li><li><a href="https://ieeexplore.ieee.org/document/8698609">Two-Stream Federated Learning: Reduce the Communication Costs</a> (2018 IEEE VCIP)</li><li><a href="https://openreview.net/forum?id=B7v4QMR6Z9w">Federated Learning Based on Dynamic Regularization</a> (ICLR 2021)</li><li><a href="https://openreview.net/forum?id=GFsU8a0sGB">Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms</a> (ICLR 2021)</li><li><a href="https://openreview.net/forum?id=LkFG3lB13U5">Adaptive Federated Optimization</a> (ICLR 2021)</li><li><a href="https://arxiv.org/abs/1905.13727">PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization</a> （NIPS 2019）</li><li><a href="https://arxiv.org/abs/1712.01887">Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training</a> (ICLR 2018)</li><li><a href="https://arxiv.org/abs/1909.05350">The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication</a></li><li><a href="https://arxiv.org/abs/1912.11187">A Communication Efficient Collaborative Learning Framework for Distributed Features</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1909.12641">Active Federated Learning</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1909.05844">Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1909.04716">Gradient Descent with Compressed Iterates</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1805.09965">LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning</a></li><li><a href="https://arxiv.org/pdf/2006.12583.pdf">Exact Support Recovery in Federated Regression with One-shot Communication</a></li><li><a href="https://arxiv.org/pdf/2006.11401.pdf">DEED: A General Quantization Scheme for Communication Efficiency in Bits</a></li><li><a href="https://arxiv.org/pdf/2006.08848.pdf">Personalized Federated Learning with Moreau Envelopes</a></li><li><a href="https://arxiv.org/pdf/2006.06954.pdf">Towards Flexible Device Participation in Federated Learning for Non-IID Data.</a></li><li><a href="https://arxiv.org/pdf/2006.03474.pdf">A Primal-Dual SGD Algorithm for Distributed Nonconvex Optimization</a></li><li><a href="https://arxiv.org/pdf/2005.05238.pdf">FedSplit: An algorithmic framework for fast federated optimization</a></li><li><a href="https://arxiv.org/pdf/2005.00224.pdf">Distributed Stochastic Non-Convex Optimization: Momentum-Based Variance Reduction</a></li><li><a href="https://arxiv.org/pdf/2007.00878.pdf">On the Outsized Importance of Learning Rates in Local Update Methods.</a></li><li><a href="https://arxiv.org/pdf/2007.01154.pdf">Federated Learning with Compression: Unified Analysis and Sharp Guarantees.</a></li><li><a href="https://arxiv.org/pdf/2004.01442.pdf">From Local SGD to Local Fixed-Point Methods for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2003.12880.pdf">Federated Residual Learning.</a></li><li><a href="https://arxiv.org/pdf/2002.11364.pdf">Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization.</a>[ICML 2020]</li><li><a href="https://arxiv.org/abs/1804.08333">Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge</a> (FedCS)</li><li><a href="https://arxiv.org/abs/1905.07210">Hybrid-FL for Wireless Networks: Cooperative Learning Mechanism Using Non-IID Data</a></li><li><a href="https://arxiv.org/pdf/2002.11360.pdf">LASG: Lazily Aggregated Stochastic Gradients for Communication-Efficient Distributed Learning</a></li><li><a href="https://arxiv.org/pdf/2002.08958.pdf">Uncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor</a></li><li><a href="https://arxiv.org/pdf/2002.08782.pdf">Dynamic Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.07454.pdf">Distributed Optimization over Block-Cyclic Data</a></li><li><a href="https://arxiv.org/abs/2011.08474">Federated Composite Optimization</a> (ICML 2021)</li><li><a href="https://arxiv.org/pdf/2002.07399.pdf">Distributed Non-Convex Optimization with Sublinear Speedup under Intermittent Client Availability</a></li><li><a href="https://arxiv.org/pdf/2002.05516.pdf">Federated Learning of a Mixture of Global and Local Models</a></li><li><a href="https://arxiv.org/pdf/2002.02090.pdf">Faster On-Device Training Using New Federated Momentum Algorithm</a></li><li><a href="https://arxiv.org/pdf/2001.01920.pdf">FedDANE: A Federated Newton-Type Method</a></li><li><a href="https://arxiv.org/pdf/1912.09925.pdf">Distributed Fixed Point Methods with Compressed Iterates</a></li><li><a href="https://arxiv.org/pdf/1912.08546.pdf">Primal-dual methods for large-scale and distributed convex optimization and data analytics</a></li><li><a href="https://arxiv.org/pdf/1912.06036.pdf">Parallel Restarted SPIDER - Communication Efficient Distributed Nonconvex Optimization with Optimal Computation Complexity</a></li><li><a href="https://arxiv.org/pdf/1912.05571.pdf">Representation of Federated Learning via Worst-Case Robust Optimization Theory</a></li><li><a href="https://arxiv.org/pdf/1910.14425.pdf">On the Convergence of Local Descent Methods in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1910.06378.pdf">SCAFFOLD: Stochastic Controlled Averaging for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1910.03197.pdf">Accelerating Federated Learning via Momentum Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/1906.06629.pdf">Robust Federated Learning in a Heterogeneous Environment</a></li><li><a href="https://arxiv.org/pdf/1906.08320.pdf">Scalable and Differentially Private Distributed Aggregation in the Shuffled Model</a></li><li><a href="https://arxiv.org/pdf/1905.03871.pdf">Differentially Private Learning with Adaptive Clipping</a></li><li><a href="https://arxiv.org/pdf/1904.10120.pdf">Semi-Cyclic Stochastic Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/1812.06127.pdf">Federated Optimization in Heterogeneous Networks</a></li><li><a href="https://arxiv.org/pdf/1811.11206.pdf">Partitioned Variational Inference: A unified framework encompassing federated and continual learning</a></li><li><a href="https://arxiv.org/pdf/1809.03832.pdf">Learning Rate Adaptation for Federated and Differentially Private Learning</a></li><li><a href="https://arxiv.org/pdf/2006.09992.pdf">Communication-Efficient Robust Federated Learning Over Heterogeneous Datasets</a></li><li><a href="https://arxiv.org/pdf/1808.07217.pdf">Don’t Use Large Mini-Batches, Use Local SGD</a></li><li><a href="https://arxiv.org/pdf/2002.09539.pdf">Overlap Local-SGD: An Algorithmic Approach to Hide Communication Delays in Distributed SGD</a></li><li><a href="https://arxiv.org/pdf/2006.02582.pdf">Local SGD With a Communication Overhead Depending Only on the Number of Workers</a></li><li><a href="https://arxiv.org/pdf/2006.08950.pdf">Federated Accelerated Stochastic Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/1909.04746.pdf">Tighter Theory for Local SGD on Identical and Heterogeneous Data</a></li><li><a href="https://arxiv.org/pdf/2006.06377.pdf">STL-SGD: Speeding Up Local SGD with Stagewise Communication Period</a></li><li><a href="https://arxiv.org/pdf/1808.07576.pdf">Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms</a></li><li><a href="http://arxiv.org/pdf/2006.07490.pdf">Understanding Unintended Memorization in Federated Learning</a></li><li><a href="https://www.usenix.org/conference/hotedge18/presentation/tao">eSGD: Communication Efficient Distributed Deep Learning on the Edge</a> (USENIX 2018 Workshop)</li><li><a href="http://home.cse.ust.hk/~lwangbm/CMFL.pdf">CMFL: Mitigating Communication Overhead for Federated Learning</a></li></ul><h3 id="6-1-Compression"><a href="#6-1-Compression" class="headerlink" title="6.1 Compression"></a>6.1 Compression</h3><ul><li><a href="https://arxiv.org/abs/1812.07210">Expanding the Reach of Federated Learning by Reducing Client Resource Requirements</a></li><li><a href="https://arxiv.org/abs/1610.05492">Federated Learning: Strategies for Improving Communication Efficiency</a> （NIPS2016 Workshop) [Google]</li><li><a href="https://arxiv.org/abs/1905.10988">Natural Compression for Distributed Deep Learning</a></li><li><a href="https://arxiv.org/abs/1909.13014">FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization</a></li><li><a href="https://arxiv.org/abs/1806.04090">ATOMO: Communication-efficient Learning via Atomic Sparsification</a>(NIPS 2018）</li><li><a href="https://arxiv.org/abs/1911.07971">vqSGD: Vector Quantized Stochastic Gradient Descent</a></li><li><a href="https://arxiv.org/abs/1610.02132">QSGD: Communication-efficient SGD via gradient quantization and encoding</a> （NIPS 2017)</li><li><a href="https://arxiv.org/abs/1610.02527">Federated Optimization: Distributed Machine Learning for On-Device Intelligence</a> [Google]</li><li><a href="https://arxiv.org/abs/1611.00429">Distributed Mean Estimation with Limited Communication</a> (ICML 2017)</li><li><a href="https://arxiv.org/abs/1611.07555">Randomized Distributed Mean Estimation: Accuracy vs Communication</a></li><li><a href="https://arxiv.org/abs/1901.09847">Error Feedback Fixes SignSGD and other Gradient Compression Schemes</a> (ICML 2019）</li><li><a href="http://proceedings.mlr.press/v70/zhang17e.html">ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning</a> (ICML 2017)</li></ul><h2 id="Part-7-Personalized-Federated-Learning"><a href="#Part-7-Personalized-Federated-Learning" class="headerlink" title="Part 7: Personalized Federated Learning"></a>Part 7: Personalized Federated Learning</h2><h3 id="7-1-Meta-Learning"><a href="#7-1-Meta-Learning" class="headerlink" title="7.1 Meta Learning"></a>7.1 Meta Learning</h3><ul><li><a href="https://arxiv.org/abs/1802.07876">Federated Meta-Learning with Fast Convergence and Efficient Communication</a></li><li><a href="https://www.semanticscholar.org/paper/Federated-Meta-Learning-for-Recommendation-Chen-Dong/8e21d353ba283bee8fd18285558e5e8df39d46e8#paper-header">Federated Meta-Learning for Recommendation</a></li><li><a href="https://arxiv.org/abs/1906.02717">Adaptive Gradient-Based Meta-Learning Methods</a></li></ul><h3 id="7-2-Multi-task-Learning"><a href="#7-2-Multi-task-Learning" class="headerlink" title="7.2 Multi-task Learning"></a>7.2 Multi-task Learning</h3><ul><li><a href="https://arxiv.org/abs/1705.10467">MOCHA: Federated Multi-Task Learning</a> (NIPS 2017)</li><li><a href="https://arxiv.org/abs/1906.06268">Variational Federated Multi-Task Learning</a></li><li><a href="https://mlsys.org/Conferences/2019/doc/2018/30.pdf">Federated Kernelized Multi-Task Learning</a></li><li><a href="https://arxiv.org/abs/1910.01991">Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/pdf/2006.13460.pdf">Local Stochastic Approximation: A Unified View of Federated Learning and Distributed Multi-Task Reinforcement Learning Algorithms</a></li></ul><h3 id="7-3-Hierarchical-FL"><a href="#7-3-Hierarchical-FL" class="headerlink" title="7.3 Hierarchical FL"></a>7.3 Hierarchical FL</h3><ul><li><a href="https://arxiv.org/pdf/1905.06641.pdf">Client-Edge-Cloud Hierarchical Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.01647.pdf">(FL startup: Tongdun, HangZhou, China) Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework.</a></li><li><a href="https://arxiv.org/pdf/2002.11343.pdf">HFEL: Joint Edge Association and Resource Allocation for Cost-Efficient Hierarchical Federated Edge Learning</a></li><li><a href="https://arxiv.org/pdf/1909.02362.pdf">Hierarchical Federated Learning Across Heterogeneous Cellular Networks</a></li><li><a href="https://arxiv.org/pdf/2004.11361.pdf">Enhancing Privacy via Hierarchical Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2004.11791.pdf">Federated learning with hierarchical clustering of local updates to improve training on non-IID data.</a></li><li><a href="https://arxiv.org/pdf/1906.00638.pdf">Federated Hierarchical Hybrid Networks for Clickbait Detection</a></li></ul><h3 id="7-4-Transfer-Learning"><a href="#7-4-Transfer-Learning" class="headerlink" title="7.4 Transfer Learning"></a>7.4 Transfer Learning</h3><ul><li><a href="https://arxiv.org/pdf/1812.03337.pdf">Secure Federated Transfer Learning. IEEE Intelligent Systems 2018.</a></li><li><a href="https://arxiv.org/pdf/1910.13271.pdf">Secure and Efficient Federated Transfer Learning</a></li><li><a href="https://arxiv.org/pdf/1907.02745.pdf">Wireless Federated Distillation for Distributed Edge Learning with Heterogeneous Data</a></li><li><a href="https://arxiv.org/pdf/2005.06105.pdf">Proxy Experience Replay: Federated Distillation for Distributed Reinforcement Learning.</a></li><li><a href="https://arxiv.org/pdf/2002.01337.pdf">Cooperative Learning via Federated Distillation over Fading Channels</a></li><li><a href="https://arxiv.org/pdf/1912.11279.pdf">(*) Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer</a></li><li><a href="https://arxiv.org/pdf/1907.06536.pdf">Federated Reinforcement Distillation with Proxy Experience Memory</a></li><li><a href="https://openreview.net/forum?id=xWr8qQCJU3m">Federated Continual Learning with Weighted Inter-client Transfer</a> (ICML 2021)</li></ul><h2 id="Part-8-Decentralization-Incentive-Mechanism"><a href="#Part-8-Decentralization-Incentive-Mechanism" class="headerlink" title="Part 8 Decentralization &amp; Incentive Mechanism"></a>Part 8 Decentralization &amp; Incentive Mechanism</h2><h3 id="8-1-Decentralized"><a href="#8-1-Decentralized" class="headerlink" title="8.1 Decentralized"></a>8.1 Decentralized</h3><ul><li><a href="https://arxiv.org/abs/1803.06443">Communication Compression for Decentralized Training</a> （NIPS 2018）</li><li><a href="https://arxiv.org/abs/1907.07346">𝙳𝚎𝚎𝚙𝚂𝚚𝚞𝚎𝚎𝚣𝚎: Decentralization Meets Error-Compensated Compression</a></li><li><a href="https://arxiv.org/pdf/1910.04956.pdf">Central Server Free Federated Learning over Single-sided Trust Social Networks</a></li><li><a href="https://arxiv.org/pdf/1705.09056.pdf">Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/2005.00797.pdf">Multi-consensus Decentralized Accelerated Gradient Descent</a></li><li><a href="https://arxiv.org/pdf/1905.10466.pdf">Decentralized Bayesian Learning over Graphs.</a></li><li><a href="https://arxiv.org/pdf/1905.06731.pdf">BrainTorrent: A Peer-to-Peer Environment for Decentralized Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1811.09904.pdf">Biscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning</a></li><li><a href="https://arxiv.org/pdf/1905.09435.pdf">Matcha: Speeding Up Decentralized SGD via Matching Decomposition Sampling</a></li></ul><h3 id="8-2-Incentive-Mechanism"><a href="#8-2-Incentive-Mechanism" class="headerlink" title="8.2 Incentive Mechanism"></a>8.2 Incentive Mechanism</h3><ul><li><a href="https://ieeexplore.ieee.org/document/8832210">Incentive Mechanism for Reliable Federated Learning: A Joint Optimization Approach to Combining Reputation and Contract Theory</a></li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3470814">Towards Fair Federated Learning</a> (KDD 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467281">Federated Adversarial Debiasing for Fair and Transferable Representations</a> (KDD 2021)</li><li><a href="https://arxiv.org/abs/1908.03092">Motivating Workers in Federated Learning: A Stackelberg Game Perspective</a></li><li><a href="https://arxiv.org/abs/1905.07479">Incentive Design for Efficient Federated Learning in Mobile Networks: A Contract Theory Approach</a></li><li><a href="https://arxiv.org/pdf/1905.10497v1.pdf">Fair Resource Allocation in Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2002.09699.pdf">FMore: An Incentive Scheme of Multi-dimensional Auction for Federated Learning in MEC.</a>(ICDCS 2020)</li><li><a href="https://arxiv.org/pdf/1912.06370.pdf">Toward an Automated Auction Framework for Wireless Federated Learning Services Market</a></li><li><a href="https://arxiv.org/pdf/1911.05642.pdf">Federated Learning for Edge Networks: Resource Optimization and Incentive Mechanism</a></li><li><a href="https://www.u-aizu.ac.jp/~pengli/files/fl_incentive_iot.pdf">A Learning-based Incentive Mechanism forFederated Learning</a></li><li><a href="https://arxiv.org/pdf/1911.01046.pdf">A Crowdsourcing Framework for On-Device Federated Learning</a></li><li><a href="https://arxiv.org/abs/1908.11598">Rewarding High-Quality Data via Influence Functions</a></li><li><a href="https://arxiv.org/abs/1811.12082">Joint Service Pricing and Cooperative Relay Communication for Federated Learning</a></li><li><a href="https://arxiv.org/abs/1909.08525">Measure Contribution of Participants in Federated Learning</a></li><li><a href="https://eprint.iacr.org/2018/679.pdf">DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-based Incentive</a></li></ul><h2 id="Part-9-Vertical-Federated-Learning"><a href="#Part-9-Vertical-Federated-Learning" class="headerlink" title="Part 9: Vertical Federated Learning"></a>Part 9: Vertical Federated Learning</h2><ul><li><a href="https://arxiv.org/abs/1912.00513">A Quasi-Newton Method Based Vertical Federated Learning Framework for Logistic Regression</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/pdf/1901.08755.pdf">SecureBoost: A Lossless Federated Learning Framework</a></li><li><a href="https://arxiv.org/pdf/1911.09824.pdf">Parallel Distributed Logistic Regression for Vertical Federated Learning without Third-Party Coordinator</a></li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467169">AsySQN: Faster Vertical Federated Learning Algorithms with Better Computation Resource Utilization</a> (KDD 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3459637.3482361">Large-scale Secure XGB for Vertical Federated Learning</a> (CIKM 2021)</li><li><a href="https://arxiv.org/pdf/1711.10677.pdf">Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption</a></li><li><a href="https://arxiv.org/pdf/1803.04035.pdf">Entity Resolution and Federated Learning get a Federated Resolution.</a></li><li><a href="https://arxiv.org/pdf/2001.11154.pdf">Multi-Participant Multi-Class Vertical Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1912.11187.pdf">A Communication-Efficient Collaborative Learning Framework for Distributed Features</a></li><li><a href="https://arxiv.org/pdf/2004.07427.pdf">Asymmetrical Vertical Federated Learning</a></li><li><a href="https://arxiv.org/abs/2007.06081">VAFL: a Method of Vertical Asynchronous Federated Learning</a> (ICML workshop on FL, 2020)</li><li><a href="https://arxiv.org/abs/2004.12088v2">SplitFed: When Federated Learning Meets Split Learning</a></li><li><a href="https://arxiv.org/abs/1910.13212">Privacy Enhanced Multimodal Neural Representations for Emotion Recognition</a></li><li><a href="https://arxiv.org/abs/1709.06161">PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training</a></li><li><a href="https://arxiv.org/abs/1911.01682">One Pixel Image and RF Signal Based Split Learning for mmWave Received Power Prediction</a></li><li><a href="https://arxiv.org/abs/1812.06415">Stochastic Distributed Optimization for Machine Learning from Decentralized Features</a></li></ul><h3 id="Part-10-Wireless-Communication-and-Cloud-Computing"><a href="#Part-10-Wireless-Communication-and-Cloud-Computing" class="headerlink" title="Part 10: Wireless Communication and Cloud Computing"></a>Part 10: Wireless Communication and Cloud Computing</h3><ul><li><a href="https://arxiv.org/pdf/2006.09801.pdf">Mix2FLD: Downlink Federated Learning After Uplink Federated Distillation With Two-Way Mixup</a></li><li><a href="https://arxiv.org/pdf/2006.02499.pdf">Wireless Communications for Collaborative Federated Learning in the Internet of Things</a></li><li><a href="https://arxiv.org/pdf/2007.00641.pdf">Democratizing the Edge: A Pervasive Edge Computing Framework</a></li><li><a href="https://arxiv.org/pdf/2006.03262.pdf">UVeQFed: Universal Vector Quantization for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2005.09969.pdf">Federated Deep Learning Framework For Hybrid Beamforming in mm-Wave Massive MIMO</a></li><li><a href="https://arxiv.org/pdf/2005.07776.pdf">Efficient Federated Learning over Multiple Access Channel with Differential Privacy Constraints</a></li><li><a href="https://arxiv.org/pdf/2005.05752.pdf">A Secure Federated Learning Framework for 5G Networks</a></li><li><a href="https://arxiv.org/pdf/2005.05265.pdf">Federated Learning and Wireless Communications</a></li><li><a href="https://arxiv.org/pdf/2005.03977.pdf">Lightwave Power Transfer for Federated Learning-based Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/2004.13563.pdf">Towards Ubiquitous AI in 6G with Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2004.09168.pdf">Optimizing Over-the-Air Computation in IRS-Aided C-RAN Systems</a></li><li><a href="https://arxiv.org/pdf/2004.08488.pdf">Network-Aware Optimization of Distributed Learning for Fog Computing</a></li><li><a href="https://arxiv.org/pdf/2004.07351.pdf">On the Design of Communication Efficient Federated Learning over Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/2004.05843.pdf">Federated Machine Learning for Intelligent IoT via Reconfigurable Intelligent Surface</a></li><li><a href="https://arxiv.org/pdf/2004.04314.pdf">Client Selection and Bandwidth Allocation in Wireless Federated Learning Networks: A Long-Term Perspective</a></li><li><a href="https://arxiv.org/pdf/2004.04104.pdf">Resource Management for Blockchain-enabled Federated Learning: A Deep Reinforcement Learning Approach</a></li><li><a href="https://arxiv.org/pdf/2004.00773.pdf">A Blockchain-based Decentralized Federated Learning Framework with Committee Consensus</a></li><li><a href="https://arxiv.org/pdf/2004.00490.pdf">Scheduling for Cellular Federated Edge Learning with Importance and Channel.</a></li><li><a href="https://arxiv.org/pdf/2003.12705.pdf">Differentially Private Federated Learning for Resource-Constrained Internet of Things.</a></li><li><a href="https://arxiv.org/pdf/2003.09375.pdf">Federated Learning for Task and Resource Allocation in Wireless High Altitude Balloon Networks.</a></li><li><a href="https://arxiv.org/pdf/2003.08059.pdf">Gradient Estimation for Federated Learning over Massive MIMO Communication Systems</a></li><li><a href="https://arxiv.org/pdf/2003.01344.pdf">Adaptive Federated Learning With Gradient Compression in Uplink NOMA</a></li><li><a href="https://arxiv.org/pdf/2003.00229.pdf">Performance Analysis and Optimization in Privacy-Preserving Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2003.00199.pdf">Energy-Efficient Federated Edge Learning with Joint Communication and Computation Design</a></li><li><a href="https://arxiv.org/pdf/2002.12873.pdf">Federated Over-the-Air Subspace Learning and Tracking from Incomplete Data</a></li><li><a href="https://arxiv.org/pdf/2002.12507.pdf">Decentralized Federated Learning via SGD over Wireless D2D Networks</a></li><li><a href="https://arxiv.org/pdf/2002.08196.pdf">Federated Learning in the Sky: Joint Power Allocation and Scheduling with UAV Swarms</a></li><li><a href="https://arxiv.org/pdf/2002.05151.pdf">Wireless Federated Learning with Local Differential Privacy</a></li><li><a href="https://arxiv.org/pdf/2002.01337.pdf">Federated Learning under Channel Uncertainty: Joint Client Scheduling and Resource Allocation.</a></li><li><a href="https://arxiv.org/pdf/2001.11567.pdf">Learning from Peers at the Wireless Edge</a></li><li><a href="https://arxiv.org/pdf/2001.10402.pdf">Convergence of Update Aware Device Scheduling for Federated Learning at the Wireless Edge</a></li><li><a href="https://arxiv.org/pdf/2001.08737.pdf">Communication Efficient Federated Learning over Multiple Access Channels</a></li><li><a href="https://arxiv.org/pdf/2001.07845.pdf">Convergence Time Optimization for Federated Learning over Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/2001.05713.pdf">One-Bit Over-the-Air Aggregation for Communication-Efficient Federated Edge Learning: Design and Convergence Analysis</a></li><li><a href="https://arxiv.org/pdf/1912.13163.pdf">Federated Learning with Cooperating Devices: A Consensus Approach for Massive IoT Networks.</a>(IEEE Internet of Things Journal. 2020)</li><li><a href="https://arxiv.org/pdf/1912.07902.pdf">Asynchronous Federated Learning with Differential Privacy for Edge Intelligence</a></li><li><a href="https://arxiv.org/pdf/1912.06273.pdf">Federated learning with multichannel ALOHA</a></li><li><a href="https://arxiv.org/pdf/1912.00131.pdf">Federated Learning with Autotuned Communication-Efficient Secure Aggregation</a></li><li><a href="https://arxiv.org/pdf/1911.07615.pdf">Bandwidth Slicing to Boost Federated Learning in Edge Computing</a></li><li><a href="https://arxiv.org/pdf/1911.02417.pdf">Energy Efficient Federated Learning Over Wireless Communication Networks</a></li><li><a href="https://arxiv.org/pdf/1911.00856.pdf">Device Scheduling with Fast Convergence for Wireless Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1911.00188.pdf">Energy-Aware Analog Aggregation for Federated Learning with Redundant Data</a></li><li><a href="https://arxiv.org/pdf/1910.14648.pdf">Age-Based Scheduling Policy for Federated Learning in Mobile Edge Networks</a></li><li><a href="https://arxiv.org/pdf/1910.13067.pdf">Federated Learning over Wireless Networks: Convergence Analysis and Resource Allocation</a></li><li><a href="http://networking.khu.ac.kr/layouts/net/publications/data/2019)Federated%20Learning%20over%20Wireless%20Network.pdf">Federated Learning over Wireless Networks: Optimization Model Design and Analysis</a></li><li><a href="https://arxiv.org/pdf/1910.09172.pdf">Resource Allocation in Mobility-Aware Federated Learning Networks: A Deep Reinforcement Learning Approach</a></li><li><a href="https://arxiv.org/pdf/1910.06837.pdf">Reliable Federated Learning for Mobile Networks</a></li><li><a href="https://arxiv.org/pdf/1909.12567.pdf">Cell-Free Massive MIMO for Wireless Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1909.07972.pdf">A Joint Learning and Communications Framework for Federated Learning over Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/1909.06512.pdf">On Safeguarding Privacy and Security in the Framework of Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1908.06287.pdf">Scheduling Policies for Federated Learning in Wireless Networks</a></li><li><a href="https://arxiv.org/pdf/1908.05891.pdf">Federated Learning with Additional Mechanisms on Clients to Reduce Communication Costs</a></li><li><a href="https://arxiv.org/pdf/1907.06040.pdf">Energy-Efficient Radio Resource Allocation for Federated Edge Learning</a></li><li><a href="https://arxiv.org/pdf/1906.10893.pdf">Mobile Edge Computing, Blockchain and Reputation-based Crowdsourcing IoT Federated Learning: A Secure, Decentralized and Privacy-preserving System</a></li><li><a href="https://arxiv.org/pdf/1906.10718.pdf">Active Learning Solution on Distributed Edge Computing</a></li><li><a href="https://arxiv.org/pdf/1905.04519.pdf">Fast Uplink Grant for NOMA: a Federated Learning based Approach</a></li><li><a href="https://arxiv.org/pdf/1901.00844.pdf">Machine Learning at the Wireless Edge: Distributed Stochastic Gradient Descent Over-the-Air</a></li><li><a href="https://arxiv.org/pdf/1812.11494.pdf">Broadband Analog Aggregation for Low-Latency Federated Edge Learning</a></li><li><a href="https://arxiv.org/pdf/1812.01202.pdf">Federated Echo State Learning for Minimizing Breaks in Presence in Wireless Virtual Reality Networks</a></li><li><a href="https://arxiv.org/pdf/1811.12082.pdf">Joint Service Pricing and Cooperative Relay Communication for Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1809.07857.pdf">In-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1905.01656.pdf">Asynchronous Task Allocation for Federated and Parallelized Mobile Edge Learning</a></li><li><a href="https://arxiv.org/abs/1812.03633">Ask to upload some data from client to server Efficient Training Management for Mobile Crowd-Machine Learning: A Deep Reinforcement Learning Approach</a></li><li><a href="https://arxiv.org/abs/1812.11494">Low-latency Broadband Analog Aggregation For Federated Edge Learning</a></li><li><a href="https://arxiv.org/pdf/1907.09769.pdf">Federated Learning over Wireless Fading Channels</a></li><li><a href="https://arxiv.org/abs/1812.11750">Federated Learning via Over-the-Air Computation</a></li></ul><h2 id="Part-11-Federated-with-Deep-learning"><a href="#Part-11-Federated-with-Deep-learning" class="headerlink" title="Part 11: Federated with Deep learning"></a>Part 11: Federated with Deep learning</h2><h3 id="11-1-Neural-Architecture-Search-NAS"><a href="#11-1-Neural-Architecture-Search-NAS" class="headerlink" title="11.1 Neural Architecture Search(NAS)"></a>11.1 Neural Architecture Search(NAS)</h3><ul><li><a href="https://arxiv.org/pdf/2004.08546.pdf">FedNAS: Federated Deep Learning via Neural Architecture Search.</a>(CVPR 2020)</li><li><a href="https://arxiv.org/pdf/2003.02793.pdf">Real-time Federated Evolutionary Neural Architecture Search.</a></li><li><a href="https://arxiv.org/pdf/2002.06352.pdf">Federated Neural Architecture Search.</a></li><li><a href="https://arxiv.org/pdf/2006.10559.pdf">Differentially-private Federated Neural Architecture Search.</a></li></ul><h3 id="11-2-Graph-Neural-Network-GNN"><a href="#11-2-Graph-Neural-Network-GNN" class="headerlink" title="11.2 Graph Neural Network(GNN)"></a>11.2 Graph Neural Network(GNN)</h3><ul><li><a href="https://ieeexplore.ieee.org/document/9005983">SGNN: A Graph Neural Network Based Federated Learning Approach by Hiding Structure</a> (Big Data)</li><li><a href="https://arxiv.org/abs/2008.11989">GraphFederator: Federated Visual Analysis for Multi-party Graphs.</a></li><li><a href="https://arxiv.org/abs/2010.12882">FedE: Embedding Knowledge Graphs in Federated Setting</a></li><li><a href="https://arxiv.org/abs/2011.03248">ASFGNN: Automated Separated-Federated Graph Neural Network</a></li><li><a href="https://arxiv.org/abs/2012.04187">GraphFL: A Federated Learning Framework for Semi-Supervised Node Classification on Graphs</a></li><li><a href="https://arxiv.org/abs/1901.11173">Peer-to-peer Federated Learning on Graphs</a></li><li><a href="https://arxiv.org/abs/1909.12946">Towards Federated Graph Learning for Collaborative Financial Crimes Detection</a></li><li><a href="https://arxiv.org/abs/2005.00455v3">Secure Deep Graph Generation with Link Differential Privacy</a> (IJCAI 2021)</li><li><a href="https://arxiv.org/pdf/2006.05535.pdf">Locally Private Graph Neural Networks</a> (CCS 2021)</li><li><a href="https://arxiv.org/pdf/2006.05535v1.pdf">When Differential Privacy Meets Graph Neural Networks</a></li><li><a href="https://arxiv.org/abs/2109.08907">Releasing Graph Neural Networks with Differential Privacy</a></li><li><a href="https://arxiv.org/abs/2005.11903">Vertically Federated Graph Neural Network for Privacy-Preserving Node Classification</a></li><li><a href="https://arxiv.org/abs/2102.04925">FedGNN: Federated Graph Neural Network for Privacy-Preserving Recommendation</a> (ICML 2021)</li><li><a href="https://federated-learning.org/fl-ijcai-2021/FTL-IJCAI21_paper_20.pdf">Decentralized Federated Graph Neural Networks</a> (IJCAI 2021)</li><li><a href="https://arxiv.org/abs/2106.13423">Federated Graph Classification over Non-IID Graphs</a> (NeurIPS 2021)</li><li><a href="https://arxiv.org/abs/2106.02743">SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks</a> (ICML 2021)</li><li><a href="https://arxiv.org/abs/2104.07145">FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks</a> (ICLR 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467371">Cross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling</a> (KDD 2021)</li></ul><h2 id="Part-12-FL-system-Library-Courses"><a href="#Part-12-FL-system-Library-Courses" class="headerlink" title="Part 12: FL system &amp; Library &amp; Courses"></a>Part 12: FL system &amp; Library &amp; Courses</h2><h3 id="12-1-System"><a href="#12-1-System" class="headerlink" title="12.1 System"></a>12.1 System</h3><ul><li><a href="https://arxiv.org/abs/1902.01046">Towards Federated Learning at Scale: System Design</a> <strong>[Must Read]</strong></li><li><a href="https://www.cs.cmu.edu/~muli/file/mu-thesis.pdf">Scaling Distributed Machine Learning with System and Algorithm Co-design</a></li><li><a href="https://ieeexplore.ieee.org/document/8784064">Demonstration of Federated Learning in a Resource-Constrained Networked Environment</a></li><li><a href="https://arxiv.org/abs/1812.02903">Applied Federated Learning: Improving Google Keyboard Query Suggestions</a></li><li><a href="https://arxiv.org/abs/2007.00914">Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy</a></li><li><a href="https://arxiv.org/pdf/2007.13518.pdf">FedML: A Research Library and Benchmark for Federated Machine Learning</a></li><li><a href="https://arxiv.org/pdf/2006.07273.pdf">FLeet: Online Federated Learning via Staleness Awareness and Performance Prediction.</a></li><li><a href="https://arxiv.org/pdf/2006.06983.pdf">Heterogeneity-Aware Federated Learning</a></li><li><a href="https://arxiv.org/pdf/2006.04150.pdf">Decentralised Learning from Independent Multi-Domain Labels for Person Re-Identification</a></li><li><a href="https://arxiv.org/pdf/2005.06850.pdf">[startup] Industrial Federated Learning – Requirements and System Design</a></li><li><a href="https://arxiv.org/pdf/2001.09249.pdf">(*) TiFL: A Tier-based Federated Learning System.</a>(HPDC 2020)</li><li><a href="https://arxiv.org/pdf/2001.04756.pdf">Adaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach</a>(ICDCS 2020)</li><li><a href="https://arxiv.org/pdf/1912.12795.pdf">Quantifying the Performance of Federated Transfer Learning</a></li><li><a href="https://arxiv.org/pdf/1912.01684.pdf">ELFISH: Resource-Aware Federated Learning on Heterogeneous Edge Devices</a></li><li><a href="https://arxiv.org/pdf/1911.04559.pdf">Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices</a></li><li><a href="https://arxiv.org/pdf/1910.11567.pdf">Substra: a framework for privacy-preserving, traceable and collaborative Machine Learning</a></li><li><a href="https://arxiv.org/pdf/1909.07452.pdf">BAFFLE : Blockchain Based Aggregator Free Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1808.08143.pdf">Functional Federated Learning in Erlang (ffl-erl)</a></li><li><a href="https://arxiv.org/pdf/2003.09876.pdf">HierTrain: Fast Hierarchical Edge AI Learning With Hybrid Parallelism in Mobile-Edge-Cloud Computing</a></li><li><a href="https://petuum.com/wp-content/uploads/2019/01/Orpheus.pdf">Orpheus: Efficient Distributed Machine Learning via System and Algorithm Co-design</a></li><li><a href="https://arxiv.org/abs/1810.11112">Scalable Distributed DNN Training using TensorFlow and CUDA-Aware MPI: Characterization, Designs, and Performance Evaluation</a></li><li><a href="https://arxiv.org/abs/1707.09414">Optimized Broadcast for Deep Learning Workloads on Dense-GPU InfiniBand Clusters: MPI or NCCL?</a></li><li><a href="https://arxiv.org/abs/1902.06855">Optimizing Network Performance for Distributed DNN Training on GPU Clusters: ImageNet&#x2F;AlexNet Training in 1.5 Minutes</a></li></ul><h3 id="12-2-Courses"><a href="#12-2-Courses" class="headerlink" title="12.2 Courses"></a>12.2 Courses</h3><ul><li><p><a href="https://www.udacity.com/course/applied-cryptography--cs387">Applied Cryptography</a></p></li><li><p><a href="https://medium.com/georgian-impact-blog/a-brief-introduction-to-differential-privacy-eacf8722283b">A Brief Introduction to Differential Privacy</a></p></li><li><p><a href="http://doi.acm.org/10.1145/2976749.2978318">Deep Learning with Differential Privacy.</a></p></li><li><p><a href="http://iamtrask.github.io/2017/03/17/safe-ai/">Building Safe A.I.</a></p><ul><li>A Tutorial for Encrypted Deep Learning</li><li>Use Homomorphic Encryption (HE)</li></ul></li><li><p><a href="https://mortendahl.github.io/2017/04/17/private-deep-learning-with-mpc/">Private Deep Learning with MPC</a></p><ul><li>A Simple Tutorial from Scratch</li><li>Use Multiparty Compuation (MPC)</li></ul></li><li><p><a href="https://mortendahl.github.io/2017/09/19/private-image-analysis-with-mpc/">Private Image Analysis with MPC</a></p><ul><li>Training CNNs on Sensitive Data</li><li>Use SPDZ as MPC protocol</li></ul></li></ul><h3 id="13-2-Secret-Sharing"><a href="#13-2-Secret-Sharing" class="headerlink" title="13.2 Secret Sharing"></a>13.2 Secret Sharing</h3><ul><li><a href="https://www.youtube.com/watch?v=kkMps3X_tEE">Simple Introduction to Sharmir’s Secret Sharing and Lagrange Interpolation</a></li><li><a href="https://mortendahl.github.io/2017/06/04/secret-sharing-part1/">Secret Sharing, Part 1</a>: Shamir’s Secret Sharing &amp; Packed Variant</li><li><a href="https://mortendahl.github.io/2017/06/24/secret-sharing-part2/">Secret Sharing, Part 2</a>: Improve efficiency</li><li><a href="https://mortendahl.github.io/2017/08/13/secret-sharing-part3/">Secret Sharing, Part 3</a></li></ul><h2 id="Part-13-Secure-Multi-party-Computation-MPC"><a href="#Part-13-Secure-Multi-party-Computation-MPC" class="headerlink" title="Part 13: Secure Multi-party Computation(MPC)"></a>Part 13: Secure Multi-party Computation(MPC)</h2><h3 id="13-1-Differential-Privacy"><a href="#13-1-Differential-Privacy" class="headerlink" title="13.1 Differential Privacy"></a>13.1 Differential Privacy</h3><ul><li><a href="https://arxiv.org/abs/1710.06963">Learning Differentially Private Recurrent Language Models</a></li><li><a href="https://arxiv.org/abs/1911.10071">Federated Learning with Bayesian Differential Privacy</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1912.06733">Private Federated Learning with Domain Adaptation</a> （NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1805.10559">cpSGD: Communication-efficient and differentially-private distributed SGD</a></li><li><a href="https://arxiv.org/pdf/1611.04482.pdf">Practical Secure Aggregation for Federated Learning on User-Held Data.</a>（NIPS 2016 Workshop)</li><li><a href="https://arxiv.org/pdf/1712.07557.pdf">Differentially Private Federated Learning: A Client Level Perspective.</a>（NIPS 2017 Workshop)</li><li><a href="https://arxiv.org/pdf/1805.04049.pdf">Exploiting Unintended Feature Leakage in Collaborative Learning.</a>(S&amp;P 2019)</li><li><a href="https://arxiv.org/pdf/1812.03224.pdf">A Hybrid Approach to Privacy-Preserving Federated Learning.</a> (AISec 2019)</li><li><a href="https://arxiv.org/pdf/1811.04017.pdf">A generic framework for privacy preserving deep learning.</a> (PPML 2018)</li><li><a href="https://arxiv.org/pdf/1910.08385.pdf">Federated Generative Privacy.</a>（IJCAI 2019 FL Workshop)</li><li><a href="https://arxiv.org/pdf/1911.01812.pdf">Enhancing the Privacy of Federated Learning with Sketching.</a></li><li><a href="https://arxiv.org/pdf/1912.05897.pdf">https://aisec.cc/</a></li><li><a href="http://proceedings.mlr.press/v130/zheng21a.html">Federated f-Differential Privacy</a> (AISTATS 2021)</li><li><a href="http://proceedings.mlr.press/v130/girgis21a.html">Shuffled Model of Differential Privacy in Federated Learning</a> (AISTATS 2021)</li><li><a href="https://dl.acm.org/doi/10.1145/3459637.3482252">Differentially Private Federated Knowledge Graphs Embedding</a> (CIKM 2021)</li><li><a href="https://arxiv.org/pdf/2002.09096.pdf">Anonymizing Data for Privacy-Preserving Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2002.09843.pdf">Practical and Bilateral Privacy-preserving Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2003.06612.pdf">Decentralized Policy-Based Private Analytics.</a></li><li><a href="https://arxiv.org/pdf/2003.10637.pdf">FedSel: Federated SGD under Local Differential Privacy with Top-k Dimension Selection.</a> (DASFAA 2020)</li><li><a href="https://arxiv.org/pdf/2003.10933.pdf">Learn to Forget: User-Level Memorization Elimination in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.03637.pdf">LDP-Fed: Federated Learning with Local Differential Privacy.</a>（EdgeSys 2020)</li><li><a href="https://arxiv.org/pdf/2004.02264.pdf">PrivFL: Practical Privacy-preserving Federated Regressions on High-dimensional Data over Mobile Networks.</a></li><li><a href="https://arxiv.org/pdf/2004.08856.pdf">Local Differential Privacy based Federated Learning for Internet of Things.</a></li><li><a href="https://arxiv.org/pdf/2004.06337.pdf">Differentially Private AirComp Federated Learning with Power Adaptation Harnessing Receiver Noise.</a></li><li><a href="https://arxiv.org/pdf/2004.06567.pdf">Decentralized Differentially Private Segmentation with PATE.</a>（MICCAI 2020 Under Review)</li><li><a href="https://arxiv.org/pdf/2004.12108.pdf">Privacy Preserving Distributed Machine Learning with Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2005.00218.pdf">Exploring Private Federated Learning with Laplacian Smoothing.</a></li><li><a href="https://arxiv.org/pdf/2005.02503.pdf">Information-Theoretic Bounds on the Generalization Error and Privacy Leakage in Federated Learning.</a></li><li><a href="https://arxiv.org/pdf/2005.04563.pdf">Efficient Privacy Preserving Edge Computing Framework for Image Classification.</a></li><li><a href="https://arxiv.org/pdf/2006.02456.pdf">A Distributed Trust Framework for Privacy-Preserving Machine Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.04747.pdf">Secure Byzantine-Robust Machine Learning.</a></li><li><a href="https://arxiv.org/pdf/2006.04593.pdf">ARIANN: Low-Interaction Privacy-Preserving Deep Learning via Function Secret Sharing.</a></li><li><a href="https://arxiv.org/pdf/2006.05459.pdf">Privacy For Free: Wireless Federated Learning Via Uncoded Transmission With Adaptive Power Control.</a></li><li><a href="https://arxiv.org/pdf/2006.07218.pdf">(*) Distributed Differentially Private Averaging with Improved Utility and Robustness to Malicious Parties.</a></li><li><a href="https://arxiv.org/pdf/2006.08848.pdf">GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators.</a></li><li><a href="https://arxiv.org/pdf/1911.00222.pdf">Federated Learning with Differential Privacy:Algorithms and Performance Analysis</a></li></ul><h3 id="13-2-Secret-Sharing-1"><a href="#13-2-Secret-Sharing-1" class="headerlink" title="13.2 Secret Sharing"></a>13.2 Secret Sharing</h3><ul><li><a href="https://www.youtube.com/watch?v=kkMps3X_tEE">Simple Introduction to Sharmir’s Secret Sharing and Lagrange Interpolation</a></li><li><a href="https://mortendahl.github.io/2017/06/04/secret-sharing-part1/">Secret Sharing, Part 1</a>: Shamir’s Secret Sharing &amp; Packed Variant</li><li><a href="https://mortendahl.github.io/2017/06/24/secret-sharing-part2/">Secret Sharing, Part 2</a>: Improve efficiency</li><li><a href="https://mortendahl.github.io/2017/08/13/secret-sharing-part3/">Secret Sharing, Part 3</a></li></ul><h2 id="Part-14-Applications"><a href="#Part-14-Applications" class="headerlink" title="Part 14: Applications"></a>Part 14: Applications</h2><ul><li><a href="https://arxiv.org/abs/1907.13113">Federated Learning Approach for Mobile Packet Classification</a></li><li><a href="https://arxiv.org/abs/1911.11807">Federated Learning for Ranking Browser History Suggestions</a> (NIPS 2019 Workshop)</li></ul><h3 id="14-1-Healthcare"><a href="#14-1-Healthcare" class="headerlink" title="14.1 Healthcare"></a>14.1 Healthcare</h3><ul><li><a href="https://arxiv.org/abs/1909.05784">HHHFL: Hierarchical Heterogeneous Horizontal Federated Learning for Electroencephalography</a> (NIPS 2019 Workshop)</li><li><a href="https://arxiv.org/abs/1912.01792">Learn Electronic Health Records by Fully Decentralized Federated Learning</a> (NIPS 2019 Workshop)</li><li><a href="https://dl.acm.org/doi/10.1145/3447548.3467185">FLOP: Federated Learning on Medical Datasets using Partial Networks</a> (KDD 2021)</li><li><a href="https://arxiv.org/ftp/arxiv/papers/1903/1903.09296.pdf">Patient Clustering Improves Efficiency of Federated Machine Learning to predict mortality and hospital stay time using distributed Electronic Medical Records</a> <a href="https://venturebeat.com/2019/03/25/federated-learning-technique-predicts-hospital-stay-and-patient-mortality/">[News]</a></li><li><a href="https://www.ncbi.nlm.nih.gov/pubmed/29500022">Federated learning of predictive models from federated Electronic Health Records.</a></li><li><a href="https://arxiv.org/pdf/1907.09173.pdf">FedHealth: A Federated Transfer Learning Framework for Wearable Healthcare</a></li><li><a href="https://arxiv.org/pdf/1810.04304.pdf">Multi-Institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation</a></li><li><a href="https://blogs.nvidia.com/blog/2019/12/01/clara-federated-learning/">NVIDIA Clara Federated Learning to Deliver AI to Hospitals While Protecting Patient Data</a></li><li><a href="https://blogs.nvidia.com/blog/2019/10/13/what-is-federated-learning/">What is Federated Learning</a></li><li><a href="https://arxiv.org/pdf/1812.00564">Split learning for health: Distributed deep learning without sharing raw patient data</a></li><li><a href="https://www.aclweb.org/anthology/W19-5030.pdf">Two-stage Federated Phenotyping and Patient Representation Learning</a> (ACL 2019)</li><li><a href="https://dl.acm.org/doi/10.1145/3097983.3098118">Federated Tensor Factorization for Computational Phenotyping</a> (SIGKDD 2017)</li><li><a href="https://arxiv.org/abs/1907.09173">FedHealth- A Federated Transfer Learning Framework for Wearable Healthcare</a> (ICJAI 2019 workshop)</li><li><a href="https://arxiv.org/abs/1810.04304">Multi-Institutional Deep Learning Modeling Without Sharing Patient Data- A Feasibility Study on Brain Tumor Segmentation</a> (MICCAI’18 Workshop)</li><li><a href="https://aaai.org/ojs/index.php/AAAI/article/view/6121">Federated Patient Hashing</a> (AAAI 2020)</li></ul><h3 id="14-2-Natual-Language-Processing"><a href="#14-2-Natual-Language-Processing" class="headerlink" title="14.2 Natual Language Processing"></a>14.2 Natual Language Processing</h3><p>Google</p><ul><li><a href="https://arxiv.org/abs/1811.03604">Federated Learning for Mobile Keyboard Prediction</a></li><li><a href="https://arxiv.org/abs/1812.02903">Applied Federated Learning: Improving Google Keyboard Query Suggestions</a></li><li><a href="https://arxiv.org/abs/1903.10635">Federated Learning Of Out-Of-Vocabulary Words</a></li><li><a href="https://arxiv.org/abs/1906.04329">Federated Learning for Emoji Prediction in a Mobile Keyboard</a></li></ul><p>Snips</p><ul><li><a href="https://arxiv.org/pdf/1810.05512.pdf">Federated Learning for Wake Keyword Spotting</a> <a href="https://medium.com/snips-ai/federated-learning-for-wake-word-detection-c8b8c5cdd2c5">[Blog]</a> <a href="https://github.com/snipsco/keyword-spotting-research-datasets">[Github]</a></li></ul><h3 id="14-3-Computer-Vision"><a href="#14-3-Computer-Vision" class="headerlink" title="14.3 Computer Vision"></a>14.3 Computer Vision</h3><ul><li><a href="https://arxiv.org/abs/2008.11560">Performance Optimization for Federated Person Re-identification via Benchmark Analysis</a> (ACMMM 2020) <a href="https://github.com/cap-ntu/FedReID">[Github]</a></li><li><a href="https://arxiv.org/abs/1910.11089">Real-World Image Datasets for Federated Learning</a></li><li><a href="https://arxiv.org/abs/2001.06202">FedVision- An Online Visual Object Detection Platform Powered by Federated Learning</a> (IAAI20)</li><li><a href="http://web.pkusz.edu.cn/adsp/files/2019/11/AAAI-FenglinL.1027.pdf">Federated Learning for Vision-and-Language Grounding Problems</a> (AAAI20)</li></ul><h3 id="14-4-Recommendation"><a href="#14-4-Recommendation" class="headerlink" title="14.4 Recommendation"></a>14.4 Recommendation</h3><ul><li><a href="https://arxiv.org/abs/1901.09888">Federated Collaborative Filtering for Privacy-Preserving Personalized Recommendation System</a></li><li><a href="https://arxiv.org/abs/1802.07876">Federated Meta-Learning with Fast Convergence and Efficient Communication</a></li><li><a href="https://arxiv.org/abs/1906.05108">Secure Federated Matrix Factorization</a></li><li><a href="https://www.cs.cmu.edu/~muli/file/difacto.pdf">DiFacto: Distributed Factorization Machines</a></li></ul><h3 id="14-5-Industrial"><a href="#14-5-Industrial" class="headerlink" title="14.5 Industrial"></a>14.5 Industrial</h3><ul><li><a href="https://github.com/matthiaslau/Turbofan-Federated-Learning-POC">Turbofan POC: Predictive Maintenance of Turbofan Engines using Federated Learning</a></li><li><a href="https://turbofan.fastforwardlabs.com/">Turbofan Tycoon Simulation by Cloudera&#x2F;FastForwardLabs</a></li><li><a href="https://florian.github.io/federated-learning/">Firefox Search Bar</a></li></ul><h2 id="Part-15-Organizations-and-Companies"><a href="#Part-15-Organizations-and-Companies" class="headerlink" title="Part 15: Organizations and Companies"></a>Part 15: Organizations and Companies</h2><h3 id="15-1-国内篇"><a href="#15-1-国内篇" class="headerlink" title="15.1 国内篇"></a>15.1 国内篇</h3><h5 id="微众银行开源-FATE-框架"><a href="#微众银行开源-FATE-框架" class="headerlink" title="微众银行开源 FATE 框架."></a>微众银行开源 <a href="https://github.com/FederatedAI/FATE">FATE</a> 框架.</h5><p>Qiang Yang, Tianjian Chen, Yang Liu, Yongxin Tong.</p><ul><li><a href="https://dl.acm.org/doi/abs/10.1145/3298981">《Federated machine learning: Concept and applications》</a></li><li><a href="https://ieeexplore.ieee.org/abstract/document/9440789">《Secureboost: A lossless federated learning framework》</a></li></ul><h5 id="字节跳动开源-FedLearner-框架"><a href="#字节跳动开源-FedLearner-框架" class="headerlink" title="字节跳动开源 FedLearner 框架."></a>字节跳动开源 <a href="https://github.com/bytedance/fedlearner">FedLearner</a> 框架.</h5><p>Jiankai Sun, Weihao Gao, Hongyi Zhang, Junyuan Xie.<a href="https://arxiv.org/pdf/2102.08504.pdf">《Label Leakage and Protection in Two-party Split learning》</a></p><h5 id="华控清交-PrivPy-多方计算平台"><a href="#华控清交-PrivPy-多方计算平台" class="headerlink" title="华控清交 PrivPy 多方计算平台"></a>华控清交 PrivPy 多方计算平台</h5><p>Yi Li, Wei Xu.<a href="https://dl.acm.org/doi/pdf/10.1145/3292500.3330920">《PrivPy: General and Scalable Privacy-Preserving Data Mining》</a></p><h5 id="同盾科技-同盾志邦知识联邦平台"><a href="#同盾科技-同盾志邦知识联邦平台" class="headerlink" title="同盾科技 同盾志邦知识联邦平台"></a>同盾科技 同盾志邦知识联邦平台</h5><p>Hongyu Li, Dan Meng, Hong Wang, Xiaolin Li.</p><ul><li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9194566">《Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework》</a></li><li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9408024">《FedMONN: Meta Operation Neural Network for Secure Federated Aggregation》</a></li></ul><h5 id="百度-MesaTEE-安全计算平台"><a href="#百度-MesaTEE-安全计算平台" class="headerlink" title="百度 MesaTEE 安全计算平台"></a>百度 <a href="https://anquan.baidu.com/product/mesatee">MesaTEE</a> 安全计算平台</h5><p>Tongxin Li, Yu Ding, Yulong Zhang, Tao Wei.<a href="https://www.ieee-security.org/TC/SP2019/posters/hotcrp_sp19posters-final11.pdf">《gbdt-rs: Fast and Trustworthy Gradient Boosting Decision Tree》</a></p><h5 id="矩阵元-Rosetta-隐私开源框架"><a href="#矩阵元-Rosetta-隐私开源框架" class="headerlink" title="矩阵元 Rosetta 隐私开源框架"></a>矩阵元 <a href="https://github.com/LatticeX-Foundation/Rosetta">Rosetta</a> 隐私开源框架</h5><h5 id="百度-PaddlePaddle-开源联邦学习框架"><a href="#百度-PaddlePaddle-开源联邦学习框架" class="headerlink" title="百度 PaddlePaddle 开源联邦学习框架"></a>百度 <a href="https://github.com/PaddlePaddle/PaddleFL">PaddlePaddle</a> 开源联邦学习框架</h5><h5 id="蚂蚁区块链科技-蚂蚁链摩斯安全计算平台"><a href="#蚂蚁区块链科技-蚂蚁链摩斯安全计算平台" class="headerlink" title="蚂蚁区块链科技 蚂蚁链摩斯安全计算平台"></a>蚂蚁区块链科技 <a href="https://antchain.antgroup.com/products/morse">蚂蚁链摩斯安全计算平台</a></h5><h5 id="阿里云-DataTrust-隐私增强计算平台"><a href="#阿里云-DataTrust-隐私增强计算平台" class="headerlink" title="阿里云 DataTrust 隐私增强计算平台"></a>阿里云 <a href="https://dp.alibaba.com/index">DataTrust</a> 隐私增强计算平台</h5><h5 id="百度百度点石联邦学习平台"><a href="#百度百度点石联邦学习平台" class="headerlink" title="百度百度点石联邦学习平台"></a>百度百度点石联邦学习平台</h5><h5 id="富数科技-阿凡达安全计算平台"><a href="#富数科技-阿凡达安全计算平台" class="headerlink" title="富数科技 阿凡达安全计算平台"></a>富数科技 阿凡达安全计算平台</h5><h5 id="香港理工大学"><a href="#香港理工大学" class="headerlink" title="香港理工大学"></a>香港理工大学</h5><p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/7021">《FedVision: An Online Visual Object Detection Platform Powered by Federated Learning》</a></p><p><a href="https://www.usenix.org/system/files/atc20-zhang-chengliang.pdf">《BatchCrypt: Efficient Homomorphic Encryption for Cross-Silo Federated Learning》</a></p><p><a href="https://arxiv.org/pdf/1910.09933.pdf">《Abnormal Client Behavior Detection in Federated Learning》</a></p><h5 id="北京航空航天大学"><a href="#北京航空航天大学" class="headerlink" title="北京航空航天大学"></a>北京航空航天大学</h5><p><a href="https://dl.acm.org/doi/abs/10.1145/3298981">《Federated machine learning: Concept and applications》</a></p><p><a href="https://arxiv.org/pdf/2101.11715.pdf">《Failure Prediction in Production Line Based on Federated Learning: An Empirical Study》</a></p><h3 id="15-2-国际篇"><a href="#15-2-国际篇" class="headerlink" title="15.2 国际篇"></a>15.2 国际篇</h3><p>Google 提出 Federated Learning.<br>H. Brendan McMahan. Daniel Ramage. Jakub Konečný. Kallista A. Bonawitz. Hubert Eichner.</p><p><a href="https://arxiv.org/abs/1602.05629">《Communication-efficient learning of deep networks from decentralized data》</a></p><p><a href="https://arxiv.org/abs/1610.05492">《Federated Learning: Strategies for Improving Communication Efficiency》</a></p><p><a href="https://arxiv.org/pdf/1912.04977.pdf">《Advances and Open Problems in Federated Learning》</a></p><p><a href="https://arxiv.org/abs/1902.01046">《Towards Federated Learning at Scale: System Design》</a></p><p><a href="https://arxiv.org/pdf/1905.03871.pdf">《Differentially Private Learning with Adaptive Clipping》</a></p><p>……（更多联邦学习相关文章请自行搜索 Google Scholar）</p><h4 id="Cornell-University"><a href="#Cornell-University" class="headerlink" title="Cornell University."></a>Cornell University.</h4><p>Antonio Marcedone.</p><p><a href="https://arxiv.org/pdf/1611.04482.pdf">《Practical Secure Aggregation for Federated Learning on User-Held Data》</a></p><p><a href="https://academic.microsoft.com/paper/2949130532/citedby/search?q=Practical%20Secure%20Aggregation%20for%20Privacy%20Preserving%20Machine%20Learning.&qe=RId%253D2949130532&f=&orderBy=0">《Practical Secure Aggregation for Privacy-Preserving Machine Learning》</a></p><p>Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, Vitaly Shmatikov.</p><p><a href="https://arxiv.org/pdf/1807.00459.pdf">《How To Backdoor Federated Learning》</a></p><p><a href="https://proceedings.neurips.cc/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html">《Differential privacy has disparate impact on model accuracy》</a></p><p>Ziteng Sun.</p><p><a href="https://arxiv.org/abs/1911.07963">《Can you really backdoor federated learning?》</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/07/07/hello-world/"/>
      <url>/2024/07/07/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
